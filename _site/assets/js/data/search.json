[
  
  {
    "title": "Auto-Encoding Variational Bayes",
    "url": "/computer-vision/2025/01/29/vae.html",
    "categories": "Computer-Vision",
    "tags": "Python, CV, AI",
    "date": "2025-01-29 00:00:00 +0900",
    





    
    "snippet": "Auto EncoderVariational Auto-Encoding을 이해하기 위해 기본적인 Auto-Encoding을 알아야 한다.Auto Encoder(AE)는 데이터를 압축하고 복원하는 단순한 모델이다. Linear layer을 통해 데이터 크기를 줄이고 복원한다. Auto Encoder 구성은 다음과 같다.  Encoder: 데이터를 압축하는 ...",
    "content": "Auto EncoderVariational Auto-Encoding을 이해하기 위해 기본적인 Auto-Encoding을 알아야 한다.Auto Encoder(AE)는 데이터를 압축하고 복원하는 단순한 모델이다. Linear layer을 통해 데이터 크기를 줄이고 복원한다. Auto Encoder 구성은 다음과 같다.  Encoder: 데이터를 압축하는 신경망 (파란 부분)  latent variable: 데이터가 압축된 벡터  Decoder: 데이터를 복원하는 신경망 (초록 부분)다른 표현으로 Encoder를 Recognition model, Decoder를 Reconstruction model이라고 부른다.class Autoencoder(nn.Module):    def __init__(self):        super(Autoencoder, self).__init__()        self.encoder = nn.Sequential(            nn.Linear(in_dim, hidden_dim),            nn.ReLU(),            nn.Linear(hidden_dim, latent_dim),            nn.ReLU(),        )        self.decoder = nn.Sequential(            nn.Linear(latent_dim, hidden_dim),            nn.ReLU(),            nn.Linear(hidden_dim, in_dim),            nn.Tanh(),        )이를 활용하면 이미지 노이즈를 제거할 수 있다. 전체 코드: Github.입력을 노이즈 있는 이미지, 정답을 노이즈 없는 이미지로 두고 학습하면 노이즈를 제거하는 모델이 학습된다. 같은 맥락에서 워터마크를 제거하는 모델도 학습할 수 있다.Variational AE 개요Variational Auto Encoder(VAE)는 “Auto-Encoding Variational Bayes“에서 소개된 모델로, latent variable을 확률 분포에서 샘플링한다.Encoder가 latent variable을 출력하는 대신, 평균($\\mu$)과 표준편차($\\sigma$)를 출력한다. 평균과 표준편차를 이용해 Gaussian 분포를 생성하고 latent variable을 샘플링한다. 즉, Gaussian 분포 $N(\\mu ,\\sigma^2)$에 대해 Encoder는 $\\mu$와 $\\sigma$를 생성하도록 학습한다. 샘플링한 latent $z$는 Decoder 입력이 된다. 조금 더 깊이 들어가보자.확률 분포를 생성하고 샘플링하는 과정을 수식으로 표현해보자.  $p_{\\theta}(x)$: 풀려는 문제. 올바른 $x$를 생성해낼 확률.  $p_{\\theta}(x|z)$: Decoder. latent $z$로부터 $x$가 나올 확률.  $p_{\\theta}(z|x)$: Encoder. 입력 $x$로부터 latent $z$가 나올 확률.          $q_{\\phi}(z|x)$: $p_{\\theta}(z|x)$의 근삿값.      먼저, Encoder는 입력 $x$가 주어졌을 때 $z$를 출력한다. 그런데 우리는 $x$에 대응하는 $z$를 알지 못한다. 따라서 $p_{\\theta}(z|x)$를 구할 수 없다. 대신 Encoder를 학습시켜 $p_{\\theta}$에 근사하는 $q_{\\phi}$를 구한다.다시 말해, Encoder를 학습하는 과정은 파라미터 $\\phi$를 학습시켜 $q_{\\phi}$가 $p_{\\theta}$에 가까워지도록 한다.Decoder는 $z$가 주어졌을 때 $x$를 출력한다. 따라서 $p_{\\theta}(x|z)$로 표현할 수 있다.  참고로 $p(x)$는 Encoder + Decoder를 나타내는 식이 아니다. 다만, 정의한 문제 $p(x)=p(z)p(x|z)$를 풀기 위해 추론에 Encoder, Decoder 구조를 활용하는 것일 뿐이다.Stochastic Gradient Variational BayesLoss function을 유도해보자.$\\log p_{\\theta}(\\mathbf{x})$는 log-likelihood로 올바른 $x$를 생성할 가능성을 나타낸다. 우리는 이 가능성을 최대로 만들어 올바른 $x$를 생성하려 한다.  아래는 Evidence Lower Bound: ELBO에 대한 식으로, 유도 과정을 생략하고 결과만 작성했다.KL-divergence를 $\\log p_{\\theta}(x)$에 대해 정리하면 다음과 같다.\\[\\log p_{\\theta}(\\mathbf{x}^{(i)}) = D_{KL} \\left( q_{\\phi}(\\mathbf{z} | \\mathbf{x}^{(i)}) \\parallel p_{\\theta}(\\mathbf{z} | \\mathbf{x}^{(i)}) \\right) + \\mathcal{L}(\\theta, \\phi; \\mathbf{x}^{(i)})\\]KL-divergence 부분은 항상 양수이기 때문에 다음과 같은 부등식이 성립한다.\\[\\log p_{\\theta}(\\mathbf{x}^{(i)}) \\geq \\mathcal{L}(\\theta, \\phi; \\mathbf{x}^{(i)})\\]따라서 $\\log p_{\\theta}(\\mathbf{x})$를 최대화하기 위해 $\\mathcal{L}(\\theta, \\phi; \\mathbf{x})$을 최대화해야 하고, 다시 말해 $- \\mathcal{L}(\\theta, \\phi; \\mathbf{x})$를 최소화해야 한다.이 식을 다시 작성하면 다음과 같다.\\[- \\mathcal{L}(\\theta, \\phi; \\mathbf{x}^{(i)}) = D_{KL} \\left( q_{\\phi}(\\mathbf{z} | \\mathbf{x}^{(i)}) \\parallel p_{\\theta}(\\mathbf{z}) \\right) - \\mathbb{E}_{q_{\\phi}(\\mathbf{z} | \\mathbf{x}^{(i)})} \\left[ \\log p_{\\theta}(\\mathbf{x}^{(i)} | \\mathbf{z}) \\right]\\]여기서 우변은 Regularization + Reconstruction로 구성되어 있다.  Regularization Loss: Encoder가 주어진 $x$에 대해 $z$를 잘 생성하는지  Reconstruction Loss: Decoder가 주어진 $z$에 대해 $x$를 잘 생성하는지정리하면, VAE의 Loss function은 Lower bound로부터 파생된다. Loss는 Encoder와 Decoder에 대한 Loss를 더한 값이다. 자세한 과정은 논문 2.2와 2.3에 기록되어 있다.Reparameterization trick앞서 설명했듯 VAE에서 latent $z$는 Gaussian 분포에서 샘플링한다.평균을 $\\mu$, 표준편차를 $\\sigma$라 할 때,\\[z^{(i,l)}\\sim q_{\\phi}(z|x^{(i)})\\]\\[z^{(i,l)} = \\mu^{(i)} + \\sigma^{(i)} \\odot \\epsilon^{(l)}\\]$\\epsilon\\sim N(0,1)$는 랜덤한 작은 값이다.epsilon = randn_like(std)z = mu + std * epsilonLoss Function 정의위에서 설명했던 Loss는 일반화된 모습이었다. 구현을 위해서는 구체적인 식을 정의해야 한다.\\[p_{\\theta}(z)\\sim N(z;0,I)\\]\\[\\log q_{\\phi}(z|x^{(i)})=\\log N(z;\\mu^{(i)},\\sigma^{2(i)}I)\\]먼저, $p_{\\theta}(z)$는 centered isotropic Gaussian을 따르며, $\\log q_{\\phi}(z|x)$도 Gaussian을 따른다고 가정한다.\\[- \\mathcal{L}(\\theta, \\phi; \\mathbf{x}^{(i)}) \\simeq - \\frac{1}{2} \\sum_{j=1}^{J} \\left( 1 + \\log \\left( (\\sigma_{j}^{(i)})^2 \\right) - (\\mu_{j}^{(i)})^2 - (\\sigma_{j}^{(i)})^2 \\right) - \\frac{1}{L} \\sum_{l=1}^{L} \\log p_{\\theta} (\\mathbf{x}^{(i)} | \\mathbf{z}^{(i,l)})\\]이 식은 Gaussian 분포에 대해 Regularization Loss를 구체적으로 정의했다. 두번째 항인 Reconstruction Loss는 negative log-likelihood다. 따라서, Binary Cross Entropy로 정의할 수 있다.def loss(x, x_reconstructed, mu, std):    # Regularization Loss    kl_div = -0.5 * sum(1 + log(std.pow(2)) - mu.pow(2) - std.pow(2))    # Reconstruction Loss    recon_loss = binary_cross_entropy(x_reconstructed, x)    return kl_div + recon_lossPytorch 구현전체 구현은 Github: VAE에서 확인할 수 있다.class VAE(nn.Module):    def __init__(self, input_dim, hidden_dim, latent_dim):        super(VAE, self).__init__()        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)    def forward(self, x):        mu, logvar = self.encoder(x)        # Reparameterization trick        std = torch.exp(0.5 * logvar)        epsilon = torch.randn_like(std)        z = mu + std * epsilon        x_recon = self.decoder(z)        return x_recon, mu, logvar구현에는 표준편차 $\\sigma$ 대신 $\\log \\sigma^2$인 logvar를 반환하도록 한다.  $\\sigma$는 일반적으로 매우 작은 값으로 계산된다. 따라서 학습 과정에서 최적화가 잘 되지 않는 문제가 있다. 하지만 분산을 log 공간에 매핑시키면 값을 더 큰 범위로 변환할 수 있다. $\\sigma$가 일반적으로 [0, 1] 범위를 가진다고 하면, $\\log \\sigma^2$는 [log(1), -inf] 범위를 가진다. 따라서 학습 과정에서 잘 최적화되는 모습을 보인다. - 출처.참고로 $\\log \\sigma^2$가 음수 범위를 가지기 때문에 logvar를 출력하는 layer는 activation으로 ReLU를 사용하면 안 된다.def loss(x, x_recon, mu, logvar):    recon_loss = nn.functional.binary_cross_entropy(x_recon, x)    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())    return recon_loss + kl_div이렇게 하면 Loss function도 logvar에 대해 재정의할 수 있다.MNIST 데이터셋을 이용해 학습하면 입력 이미지와 유사한 출력 만들어 낸다.시각화2차원 latent space를 시각화했다. 코드: Github2차원 latent space, 즉 2개의 Gaussian 분포를 생성하도록 Encoder를 학습시켰다. $z$를 [-3, 3] 범위에 대해 Decoder에 입력했다. $p(z)$가 Standard Normal Distribution을 따른다고 가정했기 때문에 [-3, 3] 범위로 latent space 대부분을 시각화할 수 있다.시각화한 이미지를 통해 샘플링된 $z$와 출력 $x$의 관계를 확인할 수 있다."
  },
  
  {
    "title": "Histograms of Oriented Gradients for Human Detection",
    "url": "/paper-review/2025/01/27/hog.html",
    "categories": "Paper-Review",
    "tags": "CV, Python, C++",
    "date": "2025-01-27 00:00:00 +0900",
    





    
    "snippet": "Histograms of Oriented Gradients for Human Detection(vision.stanford) 논문을 바탕으로 HOG descriptor 작동 원리에 대해 분석한다. 논문을 완전히 번역하는 것이 아닌 내용을 분석하고 정리한 글이다. 따라서 실제 논문 목차와 다르며, 필자의 설명이 추가되었다.본 글에서 gradient는 이...",
    "content": "Histograms of Oriented Gradients for Human Detection(vision.stanford) 논문을 바탕으로 HOG descriptor 작동 원리에 대해 분석한다. 논문을 완전히 번역하는 것이 아닌 내용을 분석하고 정리한 글이다. 따라서 실제 논문 목차와 다르며, 필자의 설명이 추가되었다.본 글에서 gradient는 이미지의 x 또는 y 방향에 대한 미분값을 말하며, 자세한 내용은 블로그: edge-detection에서 정의했다. SVM도 블로그: svm에서 다뤘었다.초록본 연구는 Linear SVM을 이용한 사람 검출 모델을 개발했다. 이미지 gradient를 이용해 경계(edge)를 탐지하는 Histograms of Oriented Gradient(HOG) descriptor는 사람 검출에 매우 좋은 성능을 보였다.  fine-scale gradients: 픽셀 간 gradient 크기 계산  fine orientation binning: 방향 정보를 히스토그램 bin으로 사용  relatively coarse spatial binning: 인접한 픽셀을 “Cell” 단위로 묶어서 계산  high-quality local contrast normalization: “Block” 단위 정규화주요 특징은 위와 같으며, 자세한 내용은 본문에서 소개한다.요약객체 부분 특징은 gradient 크기(magnitude)와 방향(orientation)으로 나타낼 수 있다. 본 연구는 Cell이라는 단위로 공간을 나누어 계산한다. 각 cell에 대해 gradient 히스토그램을 생성한다. 그리고 Block이라는 더 큰 단위로 묶어 정규화를 진행한다. 이렇게 정규화된 block을 HOG descriptor라고 한다.참고로 cell은 픽셀을 n x n으로 묶은 단위이며, block은 cell을 m x m으로 묶은 단위다. 위 예시는 8 x 8 픽셀의 cell과 2 x 2 cell을 묶은 block이다.HOG는 경계와 gradient 구조를 잘 파악한다. 또 약간의 이미지 변환(왜곡, 회전 등)에도 강하다. 사람 탐지 문제에서는 넓은 범위의 정규화가 도움이 된다. 이는 사람이 서있는 모습은 유지한 상태로 팔다리를 앞뒤로 움직이기 때문으로 보인다. 다시 말해, 큰 형태는 유지한 채 작은 변화가 발생하기 때문에 넓은 공간에 대한 일반화는 모델 성능에 영향을 준다.구현 및 성능각 단계에 대해 설명하고 모델 성능에 끼치는 영향을 분석한다. 본론에 앞서 기본(default) 모델은 다음과 같이 정의한다.  RGB 색상 공간에 대해 gamma 보정 없음  [-1 0 1] 필터를 보정(smoothing) 없이 사용  voting 전, $\\sigma = 8$의 가우시안 필터를 cell 단위로 적용  히스토그램이 방향 정보 0° ~ 180°에 대해 9개 bin을 가지도록 구성  block은 16 x 16으로 4개의 8 x 8 cell로 구성  block에 대해 L2-Hys(Lowe-style clipped L2norm) 정규화  정규화 시, block은 8 픽셀의 stride를 가짐 (4-fold coverage)  64 x 128 detection window  Linear SVM감마/색상 정규화Power law (gamma) equation을 이용해 이미지 정규화를 시도했다.  컬러 이미지를 보정했을 때 약간의 성능 향상을 보였다. 이후 단계에서 정규화를 따로 진행하기 때문에 큰 효과가 없는 것으로 보인다.  회색조 이미지는 1.5% 성능이 감소했다.추가로 square root gamma compression은 1%의 성능 향상을 보였지만, log compression은 너무 강한 나머지 2% 성능 하락을 보였다.Gradient 계산가우시안 필터를 이용한 smoothing과 다양한 마스크(cubic-corrected, sobel, diagonal 등)를 실험했다. 가우시안 필터를 사용하지 않고($\\sigma=0$), [-1 0 1] 마스크를 적용했을 때 가장 좋은 성능을 보였다.Smoothing은 성능에 치명적이다. $\\sigma$를 0에서 2로 늘렸을 때, recall rate가 89%에서 80%으로 감소했다.큰 마스크를 사용했을 때 성능이 감소했다. [-1, 1] 마스크도 1.5% 성능이 감소했는데, x와 y 방향에 대해 중심이 같지 않기 때문에 발생한 것으로 추측된다. 다시 말해, 계산하는 픽셀에 대해 대칭인 마스크가 아니기 때문에 gradient(변화)를 잘 반영하지 못한 것으로 보인다.컬러 이미지는 각 채널에 대해 gradient를 구한다. 각 픽셀에 대해 3개 채널의 gradient 중 norm(크기)이 가장 큰 벡터를 최종 gradient로 채택한다. 이는 각 채널(색상) 중 가장 강한 특징을 gradient(변화율)로 사용하기 위해서다.방향 binning히스토그램에서 각 막대의 구간을 bin이라고 하며, 데이터 분포에 맞게 bin을 나누는 과정을 binning이라고 표현한다. 나누어진 bin에 대해 데이터를 축적하는 과정은 voting이라고 한다.본 모델은 각 gradient 방향을 bin(x축)으로 설정하고, gradient 크기를 막대(y축)에 축적한다. 이러한 히스토그램을 cell마다 생성한다.bin은 0° ~ 180°(unsigned) 또는 0° ~ 360°(signed)에 대해 균일하게 나눈다. 예를 들어, unsigned 방향에 대해 9개 bin을 지정한다면 [0°, 20°, 40° … 160°]가 된다. 만약 현재 픽셀의 방향 정보가 105라면 가장 가까운 100 구간으로 분류할 수 있다. 하지만 이는 aliasing을 만든다. 따라서 더 정교한 분류를 위해 bilinear interpolation을 사용한다.4개의 픽셀에 대해 히스토그램을 생성하는 예시다. 초록 픽셀(48, 110)을 살펴보자. 110은 100과 120 사이의 값이다. 100으로부터 10만큼 떨어져있고, 120으로부터 10만큼 떨어져있다. 따라서 거리를 기반으로 가중치를 주어 100과 120에 gradient를 나누어 줄 수 있다.  100° 구간: $48\\times \\cfrac{|100-110|}{20}$  120° 구간: $48\\times \\cfrac{|120-110|}{20}$따라서 100과 120에 각각 24를 나누어주는 방식으로 히스토그램을 완성한다. 다른 셀도 같은 방식으로 gradient 크기를 축적한다.gradient 크기는 기본(L2-norm), square, square root 등 다양한 방식으로 정의할 수 있지만 기본 L2-norm이 가장 좋은 결과를 보였다.bin 개수를 늘리는 것은 9개까지 유의미한 성능 향상을 보였다. 9개 이상은 큰 차이를 발견하지 못했다. 이는 bin을 unsigned 방향에 대해 나누었을 때 이야기다. signed 방향으로 나누는 것은 오히려 성능을 떨어뜨린다. 사람 탐지에서는 옷 색상, 배경 등 폭넓은 정보를 다루기 때문에 signed 정보가 의미없을 수 있다. (참고로 다른 객체에 대해서는 signed가 좋은 모습을 보일 수 있다.)정규화정규화는 성능에 큰 영향을 준다. cell을 block 단위로 묶어 정규화를 진행한다. 정규화에서 stride를 사용해 cell이 겹치도록 할 경우, 성능이 크게 올라간다. 예를 들어, 16 x 16 블록을 8 픽셀 씩 겹치도록 정규화를 수행할 경우 한 cell은 4번의 정규화에 사용된다. 이를 4-fold coverage라고 표현한다. 아래 그림을 보면 쉽게 이해할 수 있다.빨간 테두리는 현재 단계에서 정규화가 진행되고 있는 block 크기의 구역을 나타낸다. 초록 색으로 표현한 cell은 총 4번의 정규화에 영향을 준다. 다른 cell도 중복으로 총 4번의 정규화에 사용된다.정규화 방법은 총 4 종류를 실험했다. 정규화하지 않은 벡터를 $v$라 할 때,L2-norm: $\\cfrac{v}{\\sqrt{| v |_2^2+\\epsilon^2}}$L2-Hys: $max(L2(v), 0.2)$L1-norm: $\\cfrac{v}{| v |_1+\\epsilon}$L1-sqrt: $\\cfrac{v}{\\sqrt{| v |_1+\\epsilon}}$L2-Hys, L2-norm, L1-sqrt는 비슷한 성능을 보였고, L1-norm은 성능이 5% 감소했으며, 정규화를 수행하지 않으면 성능이 27% 감소했다.Block 단위의 정규화 대신 Centre-surround 정규화도 시도해봤다. 방향에 대한 합계(히스토그램)에 가우시안 필터를 통해 정규화하는 방식이다. $\\sigma= 1$ cell width로 수행했을 때 2% 성능 하락이 있었다. 이 방법은 각 셀 안에서 필터를 적용하는 방식으로 block 간 겹치는 현상이 없기 때문이다. 이를 통해 다른 공간에 있는 상대적인 정보를 반영하는 것이 더 중요하다는 것을 알 수 있다.R-HOG와 C-HOGBlock 모양을 정의하는 방법에 따라 R-HOG와 C-HOG로 나뉜다.R-HOG: Radial HOG는 정사각형의 n x n 크기를 하나의 셀로 정의한다. 사람 탐지 문제에서 6 x 6 픽셀의 cell과 3 x 3개 cell로 이루어진 block이 가장 좋은 성능을 보였다. 학습에 사용한 이미지에서 사람의 신체(손, 다리 등)가 약 6 ~ 8 픽셀 정도였기 때문이다. 2 x 2나 3 x 3 블록은 효과가 좋았으나, 너무 크거나 작은 블록은 특징을 과하게 또는 작게 반영해 성능이 좋지 않았다.Gradient에 대해 가우시안 필터($\\sigma=0.5$ * block width)를 적용한 뒤 vote하면 1% 성능 향상을 보인다. 참고로 이미지 픽셀에 대해 smoothing을 적용하는 것이 아니라 계산한 gradient 크기에 대해 필터를 적용하는 것이다. 따라서 객체 경계를 흐릿하게 만드는 일반적인 smoothing filter와 다르다.다양한 크기의 cell과 block을 사용하는 방식은 미미한 성능 향상을 보였지만 descriptor 크기를 크게 증가시킨다.vertical(2x1) block과 horizontal(1x2) block보다는 둘을 같이 사용하는 편이 낫지만, 여전히 2 x 2와 3 x 3 block이 더 좋다.C-HOG: Circular HOG는 원 형태의 block으로 중심이 여러 개의 angular sector로 구분되어 있다. 총 4개의 파라미터를 가진다.  angular bin 개수  radial bin 개수  중심 bin 반지름  expansion factor최소 2개의 radial bin과 4개의 angular bin을 사용해야 좋은 성능을 보인다. radial bin을 늘리는 것은 큰 차이를 만들지 못하고, angular bin을 늘리는 것은 오히려 성능을 낮춘다.중심 반지름에 대해 4 픽셀이 가장 좋은 성능을 보였다.Detection window계산된 descriptor는 SVM에 입력되기 전 detection window로 조각조각 나누어진다.64 x 128 크기의 window는 16 픽셀의 여백(margin)을 포함한다. 여백을 16에서 8로 변경하면 4%의 성능이 감소한다. window 크기를 유지하고 내부 사람을 키울 때도 성능이 감소한다. 필자가 이미지를 이용해 테스트 해보니 사람 주변에 충분한 여백이 없다면 사람을 찾지 못한다.분류기본으로 $C=0.01$인 soft linear SVM을 사용한다. Gaussian 커널을 사용한 SVM의 성능이 3% 정도 더 좋지만 실행 시간(runtime)이 크게 늘어난다.결과 비교MIT와 INRIA 데이터셋에 대해 아래 모델과 비교를 진행했다.  Generalized Haar Wavelets  PCA-SIFT  Shape Contexts대체적으로 타 모델에 비해 우수한 성적을 보였다. MIT 데이터셋에 대해 완벽에 가까운 성능을 보였다. INRIA 데이터셋에 대해서도 False positive per window가 유의미하게 감소했다.R-HOG와 C-HOG는 비슷한 성능을 보였지만 C-HOG가 약간 더 좋았다. R2-HOG(primitive bar detector가 추가된 R-HOG)는 2% 정도 성능 향상을 보였다. Binary edge voting(EC-HOG)은 C-HOG에 비해 5% 정도 성능이 감소했다. Gradient 방향을 생략하고 계산하면 성능이 33% 하락한다.코드로 정리하기자세한 구현 코드는 Github: hog에서 확인할 수 있다.CELL_SIZE = 8  # Cell: 8 x 8 pixelBLOCK_SIZE = 2  # Block: 16 x 16 pixelBLOCK_STRIDE = 1  # 4-fold coverageSTD = 8  # Block_width * 0.5N_BINS = 9UNSIGNED = 180# 이미지 준비image = cv2.imread(\"human.jpg\", cv2.IMREAD_GRAYSCALE)# Gradient 크기 및 방향magnitude, orientation = gradients(image)# Gaussian 필터 적용filtered_magnitude = gaussian_filter(magnitude, CELL_SIZE, BLOCK_SIZE, STD)# Histogram 생성hist = vote_histogram(filtered_magnitude, orientation, CELL_SIZE, N_BINS, UNSIGNED)# Block 정규화norm_hist = normalize(hist, BLOCK_SIZE, BLOCK_STRIDE)기본 HOG descriptor를 코드로 정리했다. 각 단계의 결과를 시각화하면 다음과 같다.학습된 OpenCV의 HOG 모델을 이용해 추론하면 원하는 결과를 잘 찾는다. 자세한 코드는 Github: detection.cpp에 있다.HOGDescriptor hog;hog.setSVMDetector(HOGDescriptor::getDefaultPeopleDetector());vector&lt;Rect&gt; detected;hog.detectMultiScale(img, detected, 0, Size(8, 8), Size(16, 16));시각화skimage는 scikit-learn image로 HOG 특징을 쉽게 시각화할 수 있는 함수를 제공한다.features, hog_image = hog(    image,    orientations=9,    pixels_per_cell=(8, 8),    cells_per_block=(2, 2),    visualize=True,)# 시각화를 위한 Normalizehog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))"
  },
  
  {
    "title": "FastAPI 기반 딥러닝 모델 API 구축하기",
    "url": "/playground/2025/01/17/ml-api.html",
    "categories": "Playground",
    "tags": "Python, MLOps",
    "date": "2025-01-17 00:00:00 +0900",
    





    
    "snippet": "항상 공부를 하면서 궁금한 점이 있었다. 내가 만드는 기술이 사용자에게 닿기까지 어떤 과정이 있을까? 머신러닝 모델을 공부하면서도 같은 의문이 들었다. 그래서 이미지 파일을 받아 딥러닝 모델로 예측하는 API를 만들어 보았다.Github: deep-learning-codes/ML-OpsModel 학습모델과 데이터셋을 고르는 기준은 단순하다. 로컬에서 ...",
    "content": "항상 공부를 하면서 궁금한 점이 있었다. 내가 만드는 기술이 사용자에게 닿기까지 어떤 과정이 있을까? 머신러닝 모델을 공부하면서도 같은 의문이 들었다. 그래서 이미지 파일을 받아 딥러닝 모델로 예측하는 API를 만들어 보았다.Github: deep-learning-codes/ML-OpsModel 학습모델과 데이터셋을 고르는 기준은 단순하다. 로컬에서 가볍게 돌릴 수 있어야 한다. 지금은 모델이 중요한 게 아니라 그럴싸한 API를 만드는 것이 목표이기 때문에 성능보다 속도를 우선시했다. 데이터셋은 가벼운 Fashion MNIST를 사용했다. 28 x 28의 작은 크기 덕분에 빠르게 학습할 수 있다.참고로 Fashion MNIST는 부츠, 운동화, 티셔츠, 가방 등 의류 이미지로 구성된 데이터셋이다.데이터 정규화Pytorch에서 제공하는 사전학습 모델 중 가장 가벼운 MobileNet_v2를 사용했다.먼저 의문이 든 부분은 정규화 방식이었다. 본 모델은 흑백 이미지를 사용하기 때문에 각 채널에 같은 평균과 표준편차를 주는 게 맞다고 생각했다. 그런데 사전학습된 원본 모델은 각 채널에 다른 평균과 표준편차를 사용한다. 학습된 모델 파라미터를 활용하기 위해서는 원본 모델이 사용한 정규화 방식을 그대로 사용해야 할 것도 같다. 구글링을 해보니 이 부분에 대해서 의견이 다양했다. 그래서 같은 조건[batch: 64, learning rate: 0.005] + Early stopping을 적용해 정규화 결과를 비교해 보았다.  원본 모델의 정규화 방식: accuracy 91.75%  같은 값을 모든 채널에 적용: accuracy 89.15%유의미한 결과라고 확신할 수 없지만 원본 모델의 정규화가 더 좋은 성능을 보였다.MobileNet_v2 학습데이터는 Pytorch 문서에 따라 사전 학습 데이터와 동일한 정규화를 진행한다.transform = transforms.Compose([    transforms.Grayscale(num_output_channels=3),    transforms.Resize(224),    transforms.ToTensor(),    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])학습: final.ipynb  Training set: 54000 (90%)  Validation set: 6000 (10%)  Test set: 10000  Batch: 32  Learning rate: (1차) 0.003, (2차) 0.001  Epoch: (1차) 10, (2차) 7  Optimizer: Adam##### 1차 #####[1] Train: 0.38468 | Validation: 0.29368[2] Train: 0.27225 | Validation: 0.24403...[10] Train: 0.13610 | Validation: 0.17212Accuracy 93.01##### 2차 #####[1] Train: 0.09615 | Validation: 0.07122...[6] Train: 0.05107 | Validation: 0.06074[7] Train: 0.04638 | Validation: 0.06033[8] Train: 0.04151 | Validation: 0.06148[9] Train: 0.03675 | Validation: 0.06153EarlyStopping: [Epoch: 7]Accuracy: 94.36옷장에서 사진을 몇 장 찍어 테스트 했다.대부분 잘 예측했다. 비록 부츠를 운동화라고 농담도 하지만 API 만드는 연습을 하기에는 그럴싸한 모델이라고 판단했다.FastAPIAPI를 생성하기 위해 기존에 작성했던 Flask 코드를 바탕으로 코드를 완성했다. 그런데 Flask에 대해 찾아보다보니 틈틈히 FastAPI가 보였다. FastAPI 소개 영상에는 FastAPI를 찬양하는 댓글이 많았고, 궁금해서 이번 기회에 사용해 보았다. 결론만 말하면 마음에 들었다. 이유는 다음과 같다.  데이터 검증이 쉽다. 타입 힌트를 이용해 입력 타입을 강제할 수 있다.  자동 생성된 /docs를 통해 POST 요청을 쉽게 보낼 수 있다.  속도가 빠르다. 필자가 체감할 수준은 아니지만 여러 지표가 그렇게 말하고 있다.  쉽다. 벡엔드를 잘 모르는 필자도 쉽게 짤 수 있었다.API 구현목표는 사용자로부터 이미지를 입력받아 모델 추론 결과를 돌려주는 API이다. 크게 HTML form을 통해 POST 요청을 입력받는 방법과 직접 POST 요청을 보내는 방법이 있다.구현: app/main.py@app.get(\"/\", response_class=HTMLResponse)async def main(request: Request):    return templates.TemplateResponse(\"index.html\", {\"request\": request})@app.post(\"/result/\", response_class=HTMLResponse)async def show_prediction(request: Request, file: UploadFile = File()):    # 생략...    return templates.TemplateResponse(        \"result.html\",        {            \"request\": request,            \"img_src\": img_url,            \"label\": label,            \"prob\": f\"{prob * 100:.2f}%\",        },    )@app.post(\"/predict/\")async def get_prediction(file: UploadFile = File()):    # 생략...    return {\"label\": label, \"prob\": prob}  /index에서 이미지를 입력받아 /result에서 결과를 보여준다.  /predict로 POST 요청을 보내 예측 레이블과 확률을 JSON 형식으로 받는다.HTML form으로 테스트템플릿 코드: templatesPOST 요청을 받으면 이미지가 맞는지 확인한다. 검증된 이미지는 전처리를 거쳐 /static/img에 저장한다. 동시에 모델을 거쳐 예측 결과를 받아낸다. 위 사진을 보면 예상한대로 잘 작동하는 것처럼 보인다. 이렇게 템플릿을 활용하면 코드를 잘 모르는 사람도 이미지를 넣어보고 테스트할 수 있는 환경이 만들어진다.Request로 테스트이번에는 Python을 통해 요청을 날려봤다.import requestsport = \"8000\"url = f\"http://127.0.0.1:{port}/predict/\"images = [f\"./static/sample/{name}.png\" for name in (\"Sneaker\", \"Trouser\")]for img in images:    with open(img, \"rb\") as image_file:        # { Field-name: File-name, File-object, File-type }        files = {\"file\": (img, image_file, \"image/png\")}        response = requests.post(url, files=files)        resp_json = response.json()    print(\"Status:\", response.status_code)    print(\"Response:\", resp_json)    assert response.status_code == 200Status: 200Response: {'label': 'Sneaker', 'prob': 0.8817731738090515}Status: 200Response: {'label': 'Trouser', 'prob': 0.9963659048080444}원하는 결과를 잘 받아왔다. 코드로 주고 받는 방식은 결과를 받아와 추가적인 작업을 진행할 수 있다. 결과를 바탕으로 데이터 분석 등을 수행한다면 템플릿보다 유용한 방법이다. 사용자에게 이미지를 예쁘게 보여줄 필요도 없기 때문에 서버에 이미지를 저장하는 작업을 하지 않아도 된다. (데이터 수집이 필요하다면 추가하면 된다.)2종류의 API 모두 계획대로 작동한다.Docker마지막으로 완성한 API를 실행할 Docker 환경을 구축했다.Dockerfile 원본: DockerfileFROM python:3.10-slim# 생략...RUN pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cpuRUN pip install torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cpuEXPOSE 8000CMD [\"uvicorn\", \"main:app\", \"--host=0.0.0.0\", \"--port=8000\"]처음에는 평범하게 torch와 torchvision을 설치했다. 그랬더니 도커 이미지 크기가 10G를 넘어갔다. 그런데 어차피 모델을 CPU에서 돌릴 거라면 CUDA 관련 라이브러리는 설치할 필요가 없다. 그래서 whl/cpu를 통해 CPU 버전을 설치했더니 용량이 1.8G로 눈에 띄게 줄었다.컨테이너 외부에서 접속할 수 있도록 host는 0.0.0.0으로 열어주었고, port는 도커 EXPOSE와 동일하게 설정했다.docker build -t app:0.1 .docker run -p 8080:8000 --name test app:0.1빌드하고 실행해보면 위에서 봤던 것과 같이 POST 요청을 잘 처리한다.모델 학습부터 사용자에게 전달하는 과정을 살펴보았다.*여담으로 Github에 가면 버려진 파일이 있다. 원래는 모델 학습에서 보여줬던 실험을 로컬에서 MLFlow를 사용해 돌릴 계획이었다. 그런데 base 모델을 학습해보니 생각보다 시간이 오려 걸렸고, 결국 Colab의 도움을 받았다."
  },
  
  {
    "title": "SVD를 이용한 이미지 압축",
    "url": "/computer-vision/2025/01/08/svd.html",
    "categories": "Computer-Vision",
    "tags": "Python, CV, AI",
    "date": "2025-01-08 00:00:00 +0900",
    





    
    "snippet": "SVD: Singular Vector Decomposition에 대해 다룬다. 각 수식이 어떤 의미를 가지고, 이미지 압축에 어떻게 사용되는지 설명한다. 본 글을 이해하기 위해 아래 개념을 숙지하고 있어야 한다.Vector: 크기와 방향을 가지는 양으로, 2차원 공간의 벡터는 $\\vec{v}=\\begin{bmatrix}u_1 &amp; u_2\\end{...",
    "content": "SVD: Singular Vector Decomposition에 대해 다룬다. 각 수식이 어떤 의미를 가지고, 이미지 압축에 어떻게 사용되는지 설명한다. 본 글을 이해하기 위해 아래 개념을 숙지하고 있어야 한다.Vector: 크기와 방향을 가지는 양으로, 2차원 공간의 벡터는 $\\vec{v}=\\begin{bmatrix}u_1 &amp; u_2\\end{bmatrix}$와 같이 표현한다. 본문에서는 편의상 $v$ 형태로 표기한다.Inversed matrix: $A$에 대한 역행렬로 $A^{-1}$로 표기하며, $A^{-1}A=I$라는 특징을 가진다.Orthogonal matrix: 모든 column 벡터가 직교하는 행렬로, $AA^T=A^TA=I$라는 특징을 가진다. 동시에 $A^T=A^{-1}$이다.Diagonal matrix: 주대각 성분을 제외한 모든 값이 0이며, $diag(u_1,u_2 …)$로 표현한다.선형 변환: $s\\cdot \\vec{v}$를 통해 벡터의 크기와 방향을 왜곡할 수 있다.Eigenvector의 특징eigenvector는 고윳값으로 불리며, 선형 변환이 발생해도 방향을 유지하는 벡터를 말한다. eigenvector를 검색하면 다음과 같은 식이 나온다.\\[Av=\\lambda v\\]식만 봐서는 모르겠으니, 한 단계씩 해석해 보자. $Av$는 벡터 $v$에 행렬 $A$를 곱해 선형 변환을 했다. 이 과정에서 대부분의 벡터는 왜곡된다.\\[Av=\\begin{bmatrix}2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix}v\\]하지만 같은 방향을 유지하는 벡터도 존재한다. 이 벡터를 eigenvector라고 부른다. 방향은 유지하고 있지만 크기는 바뀌었다. 따라서 변형된 벡터를 $\\lambda v$로 표현할 수 있다. $\\lambda$는 크기를 조절하는 scaling factor 역할을 한다. eigenvector의 크기를 결정하는 $\\lambda$를 eigenvalue라고 한다.&gt;&gt;&gt; eigenvalues, eigenvectors = np.linalg.eig(transformation_matrix)&gt;&gt;&gt; eigenvectors[[-0.85065081 -0.52573111] [ 0.52573111 -0.85065081]] &gt;&gt;&gt; eigenvalues[1.38196601 3.61803399]다시 처음으로 돌아와 $Av=\\lambda v$는 벡터 $v$에 $A$를 통해 선형 변환을 해도 여전히 $v$인 (0이 아닌) 벡터를 eigenvector라고 한다. 이때 eigenvector에 곱해진 scaling factor를 eigenvalue라고 한다.eigen-decompositioneigenvector와 eigenvalue를 알면, 변환 행렬 $A$를 찾을 수 있다.\\[A=V\\Sigma V^{-1}\\]$V$는 각 열이 eigenvector인 행렬이다. $\\Sigma =diag(…eigenvalue)$로 eigenvalue를 담고 있는 diagonal matrix이다.다시 말해, 행렬 A는 eigenvector와 eigenvalue로 분해(decompose)할 수 있고, 이 값들을 통해 재구성할 수 있다. 이 개념이 이미지를 압축하고 재구성하는 과정에도 적용된다. 하지만 eigendecomposition은 n x n의 square matrix에만 적용 가능하다. 따라서 eigenvector 대신 singular vector가 등장한다.SVDorthogonal matrix $U$와 $V$에 대해 아래 식이 성립한다.\\[A=U\\Sigma V^T\\]$A$는 m x n 크기의 행렬이며, $\\Sigma$는 diagonal matrix이다. 식을 정리해 보면 다음과 같다.\\[AV=U\\Sigma\\]이번에는 orthogonal matrix $V$와 선형 변환해도 여전히 orthogonal 한 $U$를 찾는 문제다.식을 조금 더 정리해보면,\\[AA^T=U\\Sigma V^T (V\\Sigma^T U^T)\\]\\[AA^T=U(\\Sigma^T\\Sigma)U^T\\]글 초반에 소개했던 행렬의 특성을 이용해 정리한 식이다. 위 식을 시각화하면 다음과 같다.눈치챘다시피 $U$는 eigenvector와 동일하다. $\\Sigma^T\\Sigma$는 diagonal matrix로 eigenvalue와 같다.식을 $V$에 대해 정리하면,\\[A^TA=V(\\Sigma^T\\Sigma)V^T\\]따라서 $V$도 eigenvector와 같은 성질을 가진다.용어 정리$U$와 $V$는 singular vector로 eigenvector와 같은 의미를 가진다. $\\Sigma$는 singular value로 eigenvalue와 동일하다.\\[A=U\\Sigma V^T\\]  $U$: Left Singular Vector  $V$: Right Singular Vector  $\\Sigma$: Singular Value정리하면, $A$는 singular vector $U$와 $V$로 분해되며, $\\Sigma$는 scaling 정도를 나타내는 singular value이다.A = np.array([[1, 2], [3, 4], [5, 6]])# Perform SVD (A = U * Σ * V^T)U, sigma, Vt = np.linalg.svd(A)Truncated SVD서로 다른 자연수 m과 n에 대해, m x n 행렬에 SVD를 수행하면 버려지는 singular vector가 존재한다. 3 x 2 행렬을 살펴보자.left singular vector인 $U$는 색칠된 3 x 2 행렬의 값만 연산에 사용한다. 따라서 3 x 3이 아닌 3 x 2 행렬만 저장하면 된다. singular value인 $\\Sigma$ 도 [0 0]을 저장하고 있는 행은 버려도 된다.따라서 m &gt; n일 때는 left singular vector가 m x n이 되고, m &lt; n일 때는 right singular vector가 m x n이 된다. singular value는 min(m, n) 크기의 square matrix가 된다.이미지 분해이미지 가로 길이가 $w$, 세로 길이가 $h$일 때, 2차원 이미지는 $h\\times w$ 행렬로 표현할 수 있다. 이미지 행렬을 $M$라고 할 때, 다음과 같이 분해할 수 있다.\\[M=U\\Sigma V^T\\]\\[U=[u_1, u_2 ... u_h]\\]\\[V=[v_1, v_2 ... v_w]\\]\\[\\Sigma=diag(\\sigma_1, \\sigma_2, ... \\sigma_n)\\]SVD에 재밌는 특징이 있는데 singular value가 큰 값부터 내림차순으로 나열되어 있다는 점이다. $\\sigma$ 중 $\\sigma_1$이 가장 큰 값을 갖는다. 즉, 첫 번째 값부터 순서대로 중요한 정보를 담고 있다.  “중요한” 정보란 variance를 크게 높이는 값을 말한다. variance는 데이터가 얼마나 넓게 또는 복잡하게 퍼져있는가를 나타낸다. eigenvalue와 singular value는 scaling factor로 벡터를 얼마나 크게 늘릴지 결정하는 요소다. 그렇기 때문에 큰 value는 vector를 넓게 퍼질 수 있도록 하고, 데이터 variance도 증가시킨다. 따라서 singular value가 큰 vector는 더 중요한 정보를 담고 있다고 표현할 수 있다. 자세한 내용은 아래 PCA에서 다룬다.이미지 행렬 $M$은 $\\sum_{n=1} \\sigma_n u_n v_n^T$으로 표현할 수 있다. 그런데 만약 정보를 전부 사용하지 않고, 중요한 정보 몇 가지만 사용하면 어떨까?가로 500, 세로 600의 600 x 500 행렬에 대해 실험을 해보았다.당연히 벡터를 많이 사용할수록 이미지가 선명해진다.singular value를 시각화해보면 n = 184에서 이미 singular value 총합의 80%를 넘어간다. 184 쌍의 singular vector만으로도 이미지 80%를 복원할 수 있다.만약 600 x 500 행렬을 모두 사용하면 총 300,000개의 정보가 필요하다. 하지만, n = 200이라면 총 220,200개의 정보만 있으면 된다.SVD는 np.linalg.svd를 통해 계산한다. full_matrices 옵션은 불필요한 벡터를 저장할지 결정한다.\"\"\"이미지 분해 및 재구성\"\"\"import numpy as npfrom PIL import Imageimport osimage_path = \"object4.jpg\"output_dir = \"svd_images\"image = Image.open(image_path).convert(\"L\")image = np.array(image, dtype=np.float64)# Singular Vector Decomposition (SVD)# Image: (600, 500), S: (500,), Vt: (500, 500)# U: (600, 500) when full_matrices=False# U: (600, 600) when full_matrices=TrueU, S, Vt = np.linalg.svd(image, full_matrices=False)# 이미지 재구성for n in range(1, len(S) + 1):    singular_values = np.zeros((U.shape[1], Vt.shape[0]))    np.fill_diagonal(singular_values, S[:n])    reconstructed = np.dot(        U[:, :n],        np.dot(singular_values[:n, :n], Vt[:n, :]),    )    output_image = np.clip(reconstructed, 0, 255).astype(np.uint8)    # 단계별 이미지 저장    if n % 10 == 0:        output_path = os.path.join(output_dir, f\"{n}.png\")        Image.fromarray(output_image).save(output_path)\"\"\"Singular value 시각화\"\"\"from PIL import Imageimport numpy as npimport matplotlib.pyplot as pltimage = Image.open(\"object4.jpg\").convert(\"L\")image = np.array(image, dtype=np.float64)U, S, Vt = np.linalg.svd(image, full_matrices=False)cumulative_sum = np.cumsum(S)total_sum = np.sum(S)threshold_percentage_1 = 0.5threshold_percentage_2 = 0.8threshold_1 = total_sum * threshold_percentage_1threshold_2 = total_sum * threshold_percentage_2threshold_index_1 = np.argmax(cumulative_sum &gt;= threshold_1)threshold_index_2 = np.argmax(cumulative_sum &gt;= threshold_2)plt.figure(figsize=(14, 6))plt.plot(range(len(S)), S, label=\"Values\")plt.axvline(    x=threshold_index_1,    color=\"lightcoral\",    linestyle=\"--\",    label=f\"{threshold_percentage_1 * 100}% Threshold (Index: {threshold_index_1})\",)plt.axvline(    x=threshold_index_2,    color=\"red\",    linestyle=\"--\",    label=f\"{threshold_percentage_2 * 100}% Threshold (Index: {threshold_index_2})\",)plt.xlabel(\"Index\")plt.ylabel(\"Singular value\")plt.ylim(0, S[0] + 1)plt.legend()plt.grid(True)plt.show()PCA: 주성분 분석PCA: Principle Component Analysis는 데이터의 주요한 특징을 찾아 차원을 축소하는 기법이다. 정확히 공분산 행렬에 대해 eigen-decompotion 또는 SVD를 수행한다. 본 글은 SVD를 기준으로 설명하며, scikit-learn도 SVD를 기반으로 구현되어 있다.공분산(covariance)은 고차원 행렬에 대한 분산이다. $n\\times d$ 크기의 데이터 행렬을 $X$, 데이터 평균을 $\\mu$라고 할 때, 공분산 행렬 $\\Sigma$는 다음과 같다.\\[\\Sigma=\\cfrac{1}{n-1}​(X-\\mu)^T(X-\\mu)\\]공분산 행렬을 구하기 전 원점을 중심으로 $X$를 이동시킨다. 그리고 공분산 행렬에 대해 SVD를 실행한다.공분산 행렬에 대한 Singular vector를 시각화한 그래프다. 데이터의 중심축을 따라 vector가 만들어진 것을 확인할 수 있다. Singular vector가 만드는 축을 Principle Component라고 부른다. 그림에서 빨간 색으로 표현된 Component 1이 가장 큰 singular value를 가진다. 동시에 데이터 정보를 가장 잘 표현한 축이다. 따라서 3차원 데이터를 Component 1에 대해 매핑하면 차원 축소가 일어난다.데이터의 주요한 분포를 유지한 채 차원만 축소시켰다.import numpy as npfrom sklearn.decomposition import PCAdata = # load datasetpca = PCA(n_components=1)pca.fit(data)singular_vectors = pca.components_singular_values = pca.singular_values_cov_matrix = np.cov(data.T)참고자료  3Blue1Brown: 고유벡터와 고유값  MIT OpenCourseWare: SVD"
  },
  
  {
    "title": "Edge detection",
    "url": "/computer-vision/2025/01/06/edge-detection.html",
    "categories": "Computer-Vision",
    "tags": "CV, C++",
    "date": "2025-01-06 00:00:00 +0900",
    





    
    "snippet": "엣지(edge) 검출은 객체의 경계를 찾는 방법으로 객체 판별 전처리 과정으로 사용한다. 본 글은 대표적인 엣지(이하 경계) 검출에 필요한 수학적 배경과 알고리즘에 대해 설명한다.미분과 변화량경계 검출의 핵심은 변화를 찾는 것이다. 객체와 배경은 밝기 차이가 있을 것이라고 가정한다. 밝기 변화가 일정 수준을 넘어가면 경계로 예측한다. 이미지가 복잡하면...",
    "content": "엣지(edge) 검출은 객체의 경계를 찾는 방법으로 객체 판별 전처리 과정으로 사용한다. 본 글은 대표적인 엣지(이하 경계) 검출에 필요한 수학적 배경과 알고리즘에 대해 설명한다.미분과 변화량경계 검출의 핵심은 변화를 찾는 것이다. 객체와 배경은 밝기 차이가 있을 것이라고 가정한다. 밝기 변화가 일정 수준을 넘어가면 경계로 예측한다. 이미지가 복잡하면 잘못 검출될 가능성도 있지만 합리적인 아이디어라고 볼 수 있다.그렇다면 변화를 정의해야 한다. 수학에서 변화율은 미분으로 정의한다. 연속 함수 $f(x)$에 대해 미분은 아래와 같다.\\[f'(x) = \\cfrac{df}{dx}=\\lim_{\\bigtriangleup x \\to 0}\\cfrac{f(x+\\bigtriangleup x)-f(x)}{\\bigtriangleup x}\\]$\\bigtriangleup x$는 변화량이다. 미분값은 변화량이 0에 가까워질 때 함수 값의 차이를 뜻한다. 쉽게 말해, 특정 시점에서 함수 값의 변화로 볼 수 있다. 위 파란 그래프는 함수 $f(x)$, 아래 빨간 그래프는 $f(x)$를 미분한 $f’(x)$다. 변화가 멈춘 순간에 미분값은 0이 된다. 급격한 변화가 발생하면 미분값이 0에서 멀어진다.이산 함수 미분위에서 살펴본 미분법은 함수가 연속적일 때 적용가능하다. 이미지는 독립된 픽셀로 이루어져 있다. 따라서 이산 값에 대한 미분을 다시 정의한다.\\[f'(x) = \\cfrac{df}{dx}\\approx \\cfrac{f(x+\\bigtriangleup h)-f(x)}{\\bigtriangleup h}\\]여기서 변화량 $\\bigtriangleup h$는 픽셀 간의 거리를 뜻한다.그리고 이미지는 2차원 좌표 $(x,y)$를 가진다. 따라서, x 방향과 y 방향에 대한 미분을 모두 정의해야 한다.\\[f'_x(x,y) = \\cfrac{df}{dx}\\approx \\cfrac{f(x+\\bigtriangleup h,y)-f(x,y)}{\\bigtriangleup h}\\]\\[f'_y(x,y) = \\cfrac{df}{dy}\\approx \\cfrac{f(x,y+\\bigtriangleup h)-f(x,y)}{\\bigtriangleup h}\\]이를 시각화해보면 이해가 쉽다. 인접한 픽셀과의 차를 구하는 식이다.\\[f'_x\\approx \\cfrac{f(x+1,y)-f(x,y)}{1}=59 - 30\\]\\[f'_y\\approx \\cfrac{f(x,y+1)-f(x,y)}{1}=87 - 30\\]중앙 차분중앙 차분은 인접한 두 픽셀의 미분 값을 구하는 방식이다.\\[f'_x\\approx \\cfrac{f(x+1,y)-f(x-1,y)}{2}\\]\\[f'_y\\approx \\cfrac{f(x,y+1)-f(x,y-1)}{2}\\]정의대로라면 픽셀 간 거리인 $h$가 2이므로, 2로 나누어야 한다. 하지만 우리가 필요한 건 상대적인 크기다. 물체와 배경의 밝기가 상대적으로 얼마나 다른가이다. 따라서 2로 나누는 과정을 생략하고 약식으로 계산한다.\\[f'_x\\approx f(x+1,y)-f(x-1,y)=59-17\\]\\[f'_y\\approx f(x,y+1)-f(x,y-1)=87-40\\]거창한 내용 같지만 결국은 인접한 두 픽셀의 차를 구하는 식이 된다.행렬 연산행렬 연산을 이용하면 효율적으로 연산할 수 있다. x 방향 미분 식을 다시 살펴보자.\\[f'_x\\approx f(x+1,y)\\cdot 1 + f(x,y)\\cdot 0 - f(x-1,y)\\cdot 1\\]\\[f'_x\\approx\\begin{bmatrix} f(x-1,y) &amp; f(x,y) &amp; f(x+1,y) \\end{bmatrix}\\begin{bmatrix}-1 \\\\ 0\\\\ 1 \\end{bmatrix}\\]y 방향도 같은 방법으로 행렬을 만들 수 있다.정리하면, $f(x,y)$와 인접한 픽셀의 변화량을 통해 현재 위치가 경계인지 판별할 수 있다. 이때 효율적인 연산을 위해 행렬을 이용한다.Gradient 정의미분은 gradient를 설명하기 위한 빌드업이었다. Gradient란 x 방향과 y 방향의 미분값을 나타내는 벡터이다.\\[\\bigtriangledown f=\\begin{bmatrix} f_x \\\\ f_y \\end{bmatrix}=f_x i + f_y j\\]$i,j$는 각 방향에 대한 단위 벡터를 뜻한다. 벡터의 크기는 $\\parallel \\bigtriangledown f\\parallel $, 벡터의 방향은 $\\theta$로 표현한다.\\[\\parallel \\bigtriangledown f\\parallel =\\sqrt{f_x^2+f_y^2}\\]\\[\\theta =tan^{-1}(\\cfrac{f_y}{f_x})\\]이미지 일부를 확대한 뒤 2차원 공간에 gradient 벡터를 나타냈다. 경계로 판단되는 부분은 벡터의 크기가 매우 크다. 벡터의 방향은 변화가 발생하는 방향을 나타낸다. 다시 말해, 벡터에 수직인 방향이 경계라고 볼 수 있다. 확실히 경계가 아니라고 판단되는 곳은 크기와 방향 모두 0을 가진다.다양한 마스크앞서 행렬 연산을 이용한다고 했다. 이 행렬을 마스크(mask), 필터(filter) 또는 커널(kernel) 등으로 부른다. 본 글에서는 “마스크”로 통일하겠다. 앞서 [-1 0 +1] 형태의 단순한 마스크를 소개했다. 그 외에 더 정교한 경계 검출을 위해 여러 마스크가 개발되었다.SobelSobel 마스크는 가장 대표적인 마스크다. 인접한 두 픽셀뿐만 아니라 근접한 픽셀까지 고려한다.앞서 벡터의 크기를 통해 경계가 맞는지 확인한다고 했다. 하지만 의미없는 노이즈도 섞여 있을 수 있다. 따라서 벡터가 특정 범위를 넘어서면 경계로 판별한다. 이때 기준이 되는 값을 threshold 또는 임계값이라고 한다. threshold는 상황에 맞게 직접 설정해주어야 한다.Mat dx, dy;Sobel(img, dx, CV_32FC1, 1, 0);Sobel(img, dy, CV_32FC1, 0, 1);Mat mag_float, mag;magnitude(dx, dy, mag_float);mag_float.convertTo(mag, CV_8UC1);int threshold = 150;Mat edge = mag &gt; threshold;imshow(\"edge\", edge);ScharrScharr 마스크는 인접한 픽셀에 더 큰 가중치를 준다. 따라서 Sobel보다 변화에 더 민감하다.theshold를 높게 설정했음에도 신발 얼룩까지 포함하는 모습을 보인다. 얼룩도 밝기 변화가 있는 영역이기 때문이다.Scharr(img, dx, CV_32FC1, 1, 0);Scharr(img, dy, CV_32FC1, 0, 1);magnitude(dx, dy, mag_float);mag_float.convertTo(mag, CV_8UC1);int threshold = 250;Mat edge = mag &gt; threshold;imshow(\"edge\", edge);Canny edge detectorCanny 검출기는 단순한 마스크보다 더 정확한(tight) 테두리를 검출하기 위해 개발되었다.  Gaussian Filter  Gradient  NMS: non-maximum  suppression  Double thresholding  Hysteresis edge trackingGaussian FilterGaussian Filter는 가우시안 정규분포를 활용해 노이즈를 제거하는 과정이다. 노이즈는 주변과 다른 형태를 띠는 값이기 때문에 미분을 수행했을 때 큰 값으로 나타날 수 있다. 따라서 노이즈의 영향을 줄이기 위해 필터를 사용한다. 평균이 0, 표준편차가 $\\sigma$라고 할 때, 2차원 가우시안 분포는 아래와 같다.\\[G_{\\sigma_x\\sigma_y}(x,y)=\\cfrac{1}{2\\pi\\sigma_x\\sigma_y}e^{-(\\cfrac{x^2}{2\\sigma^2_x}+\\cfrac{y^2}{2\\sigma^2_y})}\\]가우시안 필터를 사용하면 중앙에 비교적 큰 값이 곱해지고, 주변은 작은 값이 곱해진다. 주변 상황을 약하게 반영하는 과정에서 비교적 완만한 값이 만들어진다. 따라서 부드러운 이미지를 만드는 블러 효과로 사용한다.평균이 0이고 표준편차가 $\\sigma$일 때, $[-4\\sigma ,4\\sigma]$ 사이에 99.99%의 값이 들어가 있기 때문에 마스크 크기는 $8\\sigma +1$이나 그보다 작은 크기를 사용한다.동일한 조건에서 5 x 5 가우시안 필터를 적용했을 때와 적용하지 않았을 때 검출된 경계의 모습이다. 신발 발등의 불규칙한 얼룩이 제거되었다.GradientSobel 마스크를 활용해 gradient를 계산한다. 하지만 앞서 소개한 L2 norm을 이용한 크기 계산은 과정이 복잡하다. 따라서 간단한 L1 norm을 사용해 단순하게 연산하다.\\[\\parallel \\bigtriangledown f\\parallel \\approx |f_x|+|f_y|\\]추가로 gradient 방향도 함께 계산한다. 계산된 방향은 4가지 방향[0, 45, 90, 135]으로 단순화할 수 있다. 각 픽셀이 사각형의 형태로 붙어 있기 때문이다.NMS: Non-maximum suppressionSobel을 거친 gradient는 비슷한 지역에서 여러 경계를 만들기도 한다. 이 현상 때문에 일부 경계가 두껍게 나타난다.NMS: non-maximum suppression은 경계로 판단되는 픽셀 중 가장 확실한 픽셀만 선택한다. gradient 방향으로 인접한 두 픽셀을 비교한다. 그리고 가운데 픽셀이 가장 클 경우 경계로 사용하고, 그렇지 않을 경우 0으로 처리한다.이 과정을 통해 겹쳐있는 경계 영역 중 정확한 경계를 가려낸다.동일한 조건에서 NMS를 실행했을 때와 실행하지 않았을 때의 모습이다. 겹쳐있던 선이 제거되었다.Double thresholdingDouble thresholding은 임계값 2개를 이용해 경계를 판별한다. 높은 임계값을 $T_{high}$, 낮은 임계값을 $T_{low}$라고 하자.  $\\parallel \\bigtriangledown f\\parallel  \\ge T_{high}$: 확실한 경계로 판별  $ \\parallel \\bigtriangledown f\\parallel  \\le T_{low}$: 경계가 아님  $else$: edge tracking 진행두 임계값 사이에 있는 픽셀은 추가 검사를 진행한다.Hysteresis edge trackingHysteresis edge tracking은 확실한 경계를 넓혀가는 방식으로 경계를 추가한다.확실하게 경계로 판별된 픽셀에 대해 주변 픽셀을 검사한다. 만약 주변 픽셀 중 $T_{high}$보다는 작지만, $T_{low}$보다 큰 값이 있다면 경계로 판별한다. 다시 말해, $T_{high}$와 $T_{low}$ 사이 값 중 $T_{high}$와 연결된 픽셀은 경계로 인정한다. 반면에 $T_{low}$와 연결된 사이 값은 경계로 인정하지 않는다. tracking을 통해 연결된 테두리를 추가로 찾을 수 있다.정리Canny 알고리즘의 각 단계가 어떤 과정으로 진행되고, 적용했을 때와 적용하지 않을 때의 결과 차이를 알아보았다. 전체 과정을 정리하면 아래와 같다.각 단계를 거친 이미지 행렬이다. OpenCV는 Canny 함수를 통해 이 복잡한 과정을 한 번에 처리할 수 있다.Canny(img, dst, 100, 200);만약 구현 과정이 궁금하다면 Github(denev6/deep-learning-codes)를 참고하면 된다."
  },
  
  {
    "title": "이미지 변환 행렬과 OpenCV",
    "url": "/computer-vision/2025/01/03/transformation.html",
    "categories": "Computer-Vision",
    "tags": "CV, C++",
    "date": "2025-01-03 00:00:00 +0900",
    





    
    "snippet": "이미지 행렬의 이동, 확대, 축소 등 기하학적 변환에 대해 다룬다. C++로 작성한 OpenCV 코드를 사용한다. 원본 이미지 좌표는 $(x, y)$로, 변환된 이미지 좌표는 $(x’,y’)$로 표현한다. 간결한 코드를 위해 네임스페이스를 생략하며, 이미지를 읽는 과정도 생략한다. 코드에서 img는 원본 이미지, dst는 변환된 이미지이다.원본 이미지...",
    "content": "이미지 행렬의 이동, 확대, 축소 등 기하학적 변환에 대해 다룬다. C++로 작성한 OpenCV 코드를 사용한다. 원본 이미지 좌표는 $(x, y)$로, 변환된 이미지 좌표는 $(x’,y’)$로 표현한다. 간결한 코드를 위해 네임스페이스를 생략하며, 이미지를 읽는 과정도 생략한다. 코드에서 img는 원본 이미지, dst는 변환된 이미지이다.원본 이미지의 모습이다.OpenCV는 warpAffine과 perspectiveTransform 메서드를 지원한다.  warpAffine: 어파인 변환 행렬을 이용  perspectiveTransform: 투시 변환 행렬을 이용이동 변환이동(translation) 변환은 이미지 좌표를 x, y 방향으로 이동(shift)한다. 평행 이동은 간단한 덧셈으로 구현 가능하다.\\[x' = x + \\bigtriangleup x\\]\\[y' = y + \\bigtriangleup y\\]반복문을 돌며 값을 하나씩 더하면 연산 비용이 매우 크다. 따라서 행렬 연산으로 처리한다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; \\bigtriangleup x \\\\ 0 &amp; 1 &amp; \\bigtriangleup y \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]변화 값을 더해주기 위해 [x, y]가 아닌 [x, y, 1]을 사용한다. 동차(homogeneous) 좌표계라는 개념으로 머신러닝에서 흔하게 사용하는 테크닉이다. 본론으로 돌아와 코드는 아래와 같다.double d_x = 100;double d_y = 150;Mat affine_matrix = Mat_&lt;double&gt;(\t{ 2, 3 }, { 1, 0, d_x, 0, 1, d_y });warpAffine(img, dst, affine_matrix, Size());전단 변환전단(shear) 변환은 직사각형을 평행사변형으로 비트는 변환이다. 위 이미지는 x(가로) 방향으로 비튼 모습이다. 아래쪽으로 갈수록, 다시 말해 y 좌표가 증가할수록 변화가 커진다. 즉, x 좌표의 변화는 y에 비례한다.\\[x' = x + m_x y\\]\\[y' = y\\]여기서 $m_x$은 변화 정도를 나타낸다. $m_x$가 클수록 x 방향으로 강하게 비튼다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; m_x &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]위 행렬은 x 방향으로 비트는 형태라면, y(세로) 방향으로 비트는 경우를 생각해 보자.\\[x' = x\\]\\[y' = y + m_y x\\]같은 맥락에서 y 좌표의 변화는 x에 비례한다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ m_y &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]// shear_xdouble m_x = 0.5;Mat affine_matrix = Mat_&lt;double&gt;(    { 2, 3 }, { 1, m_x, 0, 0, 1, 0 });int x = img.cols;int y = img.rows;Size dst_size = Size(cvRound(x + y * m_x), y);warpAffine(img, dst, affine_matrix, dst_size);// shear_ydouble m_y = 0.5;Mat affine_matrix = Mat_&lt;double&gt;(    { 2, 3 }, { 1, 0, 0, m_y, 1, 0 });int x = img.cols;int y = img.rows;Size dst_size = Size(x, cvRound(y + x * m_y));warpAffine(img, dst, affine_matrix, dst_size);크기 변환크기(scale) 변환은 이미지를 확대하거나 축소하는 변환이다. x, y에 확대/축소할 비율을 곱하면 크기가 변한다.\\[x' = s_x \\cdot x\\]\\[y' = s_y \\cdot y\\]\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} s_x &amp; 0 &amp; 0 \\\\ 0 &amp; s_y &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]double s_x = 0.7;double s_y = 0.9;Mat affine_matrix = Mat_&lt;double&gt;(\t{ 2, 3 }, { s_x, 0, 0, 0, s_y, 0 });warpAffine(img, dst, affine_matrix, img.size(), INTER_LINEAR);또는 resize를 통해 쉽게 처리할 수 있다.resize(img, dst, Size(), s_x, s_y);여기서 의문이 생긴다. 행렬 크기가 달라진다. 따라서 이미지 픽셀의 개수가 달라진다.예를 들어, 2 x 2 이미지를 4 x 6 이미지로 늘리려 한다. 기존 이미지는 4개의 픽셀(정보)만 가지고 있지만, 확대한 이미지는 24개의 픽셀을 가진다. 이때 발생한 공백을 채우는 방법이 보간법(interpolation)이다.양선형 보간법대표적으로 양선형(bilinear) 보간법이 있다. OpenCV에서 INTER_LINEAR이라는 플래그로 표현되며, 기본(default) 설정이다. 양선형 보간법은 주어진 픽셀 간 거리를 바탕으로 가중 평균을 계산해 값을 구한다.예를 들어 2 x 2 이미지를 4 x 3으로 확대해 보자.노란색으로 표시한 $P_{2, 1}$ 값은 다음과 같이 계산한다.\\[P_{2, 1}=\\cfrac{P_{1, 1} \\cdot 2 + P_{4, 1} \\cdot 1}{2 + 1} \\approx 23\\]거리를 기반으로 가중치를 계산하고 평균을 구한다. 이미지 픽셀은 정수형이기 때문에 근삿값으로 처리한다.여기까지가 일반적으로 알려진 양선형 보간법이다. 하지만 OpenCV를 실행해 보면 예상과 다르다.cv2.resize(mat, (4, 3), interpolation=cv2.INTER_LINEAR)\"\"\"Input:[[10, 50] [30, 90]]Output:[[10 20 40 50] [20 33 58 70] [30 45 75 90]]\"\"\"좌표를 할당하는 과정에서 차이가 발생하는 것으로 보인다. (출처: stackoverflow)가로 행에 4개의 픽셀이 할당되어야 한다. 따라서 같은 거리로 값을 배치하다 보니 $P_{2,1}’$는 $(0.25, 0)$에 위치하게 된다. 이 가정을 바탕으로 $P_{2,1}’$을 계산해 보자.$P_{2,1}’$와 $P_{1,1}\\leftarrow (0, 0)$ 사이의 거리는 0.25이다. $P_{2,1}’$와 $P_{2,1}\\leftarrow (1, 0)$ 사이의 거리는 0.75이다. 따라서 가중 평균을 구하면,\\[P_{2,1}'=\\cfrac{P_{1,1} \\cdot 0.75 + P_{2,1} \\cdot 0.25}{0.25 + 0.75}=20\\]중요한 내용은 아니지만, 결과에 작은 차이가 발생할 수 있다.다양한 보간법양선형 보간법 외에 여러 보간법을 지원한다. OpenCV에서 사용 가능한 플래그는 다음과 같다.  INTER_NEAREST: nearest neighbor. 상대적으로 빠르지만 품질이 떨어진다.  INTER_CUBIC: bicubic. 상대적으로 느리지만 품질이 좋다.  INTER_AREA: resampling. 이미지 축소에 유리하다.회전 변환회전(rotation) 변환은 이미지를 시계 또는 반시계 방향으로 회전하는 변환이다. 먼저 시계 방향(clockwise) 회전에 대해 알아보자. 간단한 이해를 위해 단위 원 $x^2+y^2=1$을 살펴보자. 아래는 단위 원을 그리는 Python 코드다.theta = np.linspace(0, 2 * np.pi, 400)x, y = np.cos(theta), np.sin(theta)fig, ax = plt.subplots(figsize=(6, 6))ax.plot(x, y)단위 원은 $[0, 2\\pi]$ 범위의 $\\theta$에 대한 $P(cos\\theta , sin\\theta )$의 집합이다. 즉, $cos\\theta$는 x축, $sin\\theta$는 y축과 관계가 있다.구체적으로 $P(cos30, sin30)$를 찍어보면 $P(1,0)$를 반시계 방향으로 회전한 모습이다. 시계 방향으로 회전한 파란 점은 빨간 점에 대해 x축 대칭이므로 $P(cos30,-sin30)$이다. 구체적인 유도 과정은 gaussian37님의 블로그에 잘 정리되어 있다.결론적으로 시계 방향 회전에 대한 회전 행렬은 아래와 같다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} cos(\\theta ) &amp; -sin(\\theta) &amp; 0 \\\\ sin(\\theta) &amp; cos(\\theta) &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]반시계 방향에 대한 회전 행렬은 다음과 같다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} cos(\\theta ) &amp; sin(\\theta) &amp; 0 \\\\ -sin(\\theta) &amp; cos(\\theta) &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]시계 방향 회전에 대한 코드는 다음과 같다.double angle = 30;double radian = angle * CV_PI / 180;// (0, 0)를 기준으로한 시계 방향 회전Mat rotation_matrix = Mat_&lt;double&gt;(\t{2, 3}, {cos(radian), -sin(radian), 0, sin(radian), cos(radian), 0});warpAffine(img, dst, rotation_matrix, Size());하지만 OpenCV는 $\\theta$에 대한 회전 행렬을 생성하는 getRotationMatrix2D 함수를 지원한다.Point2f center(img.cols / 2.f, img.rows / 2.f); // 이미지 중심double angle = 30;Mat rotation_matrix = getRotationMatrix2D(center, angle, 1);warpAffine(img, dst, rotation_matrix, Size());또는 rotate를 통해 쉽게 처리할 수 있다. 하지만 90도 단위로 회전한다는 한계가 있다.rotate(img, dst, ROTATE_90_CLOCKWISE);대칭 변환대칭(reflection) 변환은 축을 기준으로 이미지를 뒤집는 변환이다. 먼저 y축을 기준으로 대칭인 이미지를 만들어보자.수평 대칭인 이미지의 y 좌표는 같고, x 좌표의 부호만 변한다.\\[x'=-x\\]\\[y'=y\\]하지만 이미지 좌표를 음수로 표현할 수 없다. 이미지 넓이를 $w$라할 때, x 좌표는 $[0, w)$ 범위를 가진다. 따라서 $w$만큼 평행이동 시켜 범위를 맞출 수 있다.\\[x'=-x+(w-1)\\]-1이 붙은 이유는 프로그래밍 언어에서 좌표가 0부터 시작하기 때문이다. 넓이가 300이라면 실제로는 [0, 299] 범위의 인덱스를 가진다. C++에서 변환 행렬을 사용하기 위해 $w$가 아닌 $w-1$만큼 이동해야 범위를 넘지 않는다. 이를 행렬로 정리하면 다음과 같다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} -1 &amp; 0 &amp; w-1 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]double w = img.cols - 1;Mat affine_matrix = Mat_&lt;double&gt;(\t{ 2, 3 }, { -1, 0, w, 0, 1, 0 });warpAffine(img, dst, affine_matrix, Size());같은 맥락에서 x축 대칭은 y 좌표의 부호를 바뀐 뒤 범위를 조정해 주면 된다. 높이가 $h$일 때, y 좌표의 범위는 $[0,h)$이다. 따라서 변환 행렬은 아래와 같이 표현된다.\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; h-1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]double h = img.rows - 1;Mat affine_matrix = Mat_&lt;double&gt;(\t{ 2, 3 }, { 1, 0, 0, 0, -1, h });warpAffine(img, dst, affine_matrix, Size());OpenCV는 flip을 통해 쉽게 이미지를 뒤집을 수 있다.filp(img, dst, flipCode=1);3번째 파라미터는 flipCode로 회전축을 지정한다.  flipCode == 0: 상하 대칭  flipCode &gt; 0: 좌우 대칭  flipCode &lt; 0: 상하 대칭 + 좌우 대칭투시 변환투시(perspective) 변환은 네 점을 기준으로 임의의 사각형을 직사각형 형태로 변환한다. 먼저, 변환을 위해 네 점의 좌표가 필요하다. 왼쪽 카드의 네 꼭짓점 좌표를 $p=(x,y)$라고 정의하겠다. 그리고 좌표 $p$가 이동할 최종 좌표도 필요하다. 오른쪽 이미지의 네 꼭짓점 좌표를 $q=(x’,y’)$라고 하겠다. 결론부터 이야기하면 변환 과정은 아래와 같다.\\[\\begin{bmatrix} x' \\\\ y' \\\\ w \\end{bmatrix} = M_{trans} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\]\\[M_{q}=M_{coef}\\cdot M_{trans}'\\]$M_{q}$ and $M_{coef}$ are given.$M_{trans}$는 3 x 3 변환 행렬이다. $w$는 이미지를 조정하기 위한 scale factor이다. $M_{q}$는 최종 목표 좌표인 $q$를 담고 있는 행렬이다. $M_{q}$는 변환 행렬 벡터를 담고 있는 8 x 1 크기의 $M_{trans}’$과 8 x 8 행렬 $M_{coef}$로 나타낸다. 여기서 LU-decomposition 등 방법으로 $M_{coef}$를 분해한 뒤, $M_{trans}’$를 구한다. $M_{trans}’$를 3 x 3 행렬로 매핑하면 변환 행렬 $M_{trans}$를 얻을 수 있다. 참고로 $M_{trans}$ 내 마지막 값은 1로 고정이기 때문에 8 x 1 행렬을 3 x 3으로 매핑하는 것이 가능하다.위 과정은 OpenCV 기본값으로 지정된 DECOMP_LU를 기준으로 한 설명이다. 세부적인 과정은 분해 방법에 따라 달라지기 때문에 큰 흐름만 읽고 넘어가자.다행히 OpenCV의 getPerspectiveTransformation을 통해 쉽게 변환 행렬을 얻을 수 있다.// 카드의 꼭짓점. 순서대로:// top-left &gt; top-right &gt; bottom-right &gt; bottom-leftPoint2f objectPoint[4] = {\tPoint2f(10, 141),\tPoint2f(212, 29),\tPoint2f(486, 273),\tPoint2f(268, 477)};int dst_w = 150;int dst_h = 200;Point2f dstPoint[4] = {\tPoint2f(0, 0),\tPoint2f(dst_w - 1, 0),\tPoint2f(dst_w - 1, dst_h - 1),\tPoint2f(0, dst_h - 1)};Mat transform_matrix = getPerspectiveTransform(objectPoint, dstPoint, DECOMP_LU);warpPerspective(img, dst, transform_matrix, Size(dst_w, dst_h));"
  },
  
  {
    "title": "Attention is all you need",
    "url": "/paper-review/2024/04/10/transformer.html",
    "categories": "Paper-Review",
    "tags": "",
    "date": "2024-04-10 00:00:00 +0900",
    





    
    "snippet": "Attention Is All You Need본 글은 “Attention is All You Need” 논문을 번역 및 분석했다. 일부 문장은 맥락에 따라 의역되었으며, 명확한 이해를 위해 부분적으로 설명을 추가했다. 주요 용어는 정확한 의미 전달을 위해 영문 그대로 작성했다. (예: recurrent, convolutional 등)Abstract 기...",
    "content": "Attention Is All You Need본 글은 “Attention is All You Need” 논문을 번역 및 분석했다. 일부 문장은 맥락에 따라 의역되었으며, 명확한 이해를 위해 부분적으로 설명을 추가했다. 주요 용어는 정확한 의미 전달을 위해 영문 그대로 작성했다. (예: recurrent, convolutional 등)Abstract 기존 시퀀스 모델은 encoder-decoder가 포함된 복잡한 recurrent나 convolutional 신경망을 기반으로 한다. 본 논문은 recurrence와 convolution 없이 attention mechanisms을 기반으로 하는 간단한 Transformer 구조를 제안한다. 2종류의 기계번역 문제에서 좋은 성과를 보였고, 병렬화를 통해 학습 시간을 단축했다. 본 모델은 WMT 2014 영어-독일어 번역에서 28.4 BLEU를 달성했다. WMT 2014 영어-프랑스어 번역은 41.8 BLEU로 단일 모델 SOTA를 달성했다. 8개 GPU로 3.5일을 학습했다. Transformer를 영어 문장 성분 파싱에 적용했고, 다른 문제에도 적용 가능하다는 사실을 확인했다. 학습 데이터가 큰 상황과 제한된 상황에서 모두 잘 학습되었다.IntroductionRNN, LSTM, GRU는 기계 번역이나 언어 모델 분야에서 준수한 성능으로 입지를 확고히 해왔다. Recurrent 모델은 $t$ 시점 hidden state인 $h_t$를 학습하기 위해 $h_{t-1}$를 사용한다. 이러한 순차적인 구조는 병렬 연산을 활용할 수 없어 긴 시퀀스에 치명적이다. 최근 factorization tricks나 conditional computation을 이용해 연산 효율과 앞서 말한 문제를 개선했다. 하지만 여전히 모델 구조에 따른 근본적인 제약이 있다.Attention mechanisms는 시퀀스 길이에 관계없이 의존성 모델링이 가능하며, 다양한 문제에서 좋은 모습을 보여준다. 하지만 대부분 Attention은 recurrent 구조와 함께 사용된다. 본 논문은 Transformer를 제안하고, recurrence를 피하는 대신 완전히 attention 구조에 의존하는 방식으로 입출력 사이 global dependency를 도출한다. Transformer는 병렬 처리를 통해 변역 문제에서 SOTA를 달성했고, 8개의 P100 GPU로 12시간을 학습했다.Model Architecturefig1*그림에서 좌측이 Encoder, 우측이 Decoder 구조다.Encoder-Decoder stacksEncoder는 N=6개의 동일한 층이 연결된 모습이다. 각 층은 multi-head self-attention과 간단한 position-wise fully connected feed-forward network로 구성된다. 각 sub-layer에 대해 residual connection을 적용하고, 뒤이어 정규화를 진행한다. * 그림에서  residual connection은 multi-head attention 입력을 출력과 합치는 부분을 말한다. (Add)$LayerNorm(x+Sublayer(x))$residual connection을 쉽게 처리하기 위해 embedding을 포함한 모든 출력은 $d_{model}=512$ 차원으로 고정한다.Decoder도 N=6개의 동일한 층으로 구성된다. 내부는 Encoder에 한 개 sub-layer을 추가한 형태로, 총 3개 층으로 구성된다. 추가된 층은 Encoder 출력을 받아 multi-head attention을 수행한다. Decoder도 Encoder와 마찬가지로 residual connection과 정규화를 적용한다. 또한 첫 self-attention에 masking을 적용해 output embedding을 상쇄한다. 이를 통해 i번째 위치의 값은 i 이전 값에만 영향을 받도록 한다. *masking에 대해 다음 챕터(Applications of Attention in our Model)에서 자세히 설명한다.Attentionfig2Scaled Dot-Product Attention입력은 $d_k$ 차원의 query, key와 $d_v$ 차원의 value이다. Query와 Key를 점곱한 뒤 $\\sqrt{d_k}$로 나누고 softmax를 통해 각 value에 대한 가중치를 얻는다.$Attention(Q,K,V)=softmax(\\cfrac{QK^T}{\\sqrt{d_k}})V$Dot-product attention은 최적화된 행렬 연산 코드를 이용하기 때문에 다른 attetion에 비해 빠르고 공간 효율성이 좋다. 그리고 $d_k$가 큰 값이면 점곱의 결과가 커진다. 이는 softmax 연산 시 매우 작은 gradient로 이어질 수 있다. 문제를 해결하기 위해 $\\cfrac{1}{\\sqrt{d_k}}$로 스케일링했다.Multi-Head Attention각 query, key, value에 대해 attention 연산하는 것보다 각각 $d_k,d_k,d_v$ 차원으로 h번 linearly project 하는 것이 효율적이다 (Fig2 참고). 그리고 각 query, key, value에 대해 병렬로 attention을 수행해 $d_v$ 차원의 출력을 계산한다. 이 값은 다시 concat &amp; project 되어 출력이 된다. Mutil-head attention은 다른 위치에 다른 영역에서 온 정보를 한 번에 확인할 수 있다.$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O$where $head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$$W_i^Q,W_i^K,W_i^V$과 $W^O\\in\\mathbb{R}^{hd_v\\times d_{model}}$는 project 될 때 사용하는 parameter다.본 연구는 h=8개의 병렬된 attention 층을 사용한다. 각 차원은 $d_k=d_v=d_{model}/h=64$이다. 줄어든 차원 덕분에 전체 연산 비용은 singe-head attention과 유사하다.*여기서 차원이 줄었다는 표현은 병렬 연산을 하며 나타난 효과다. 위 설명에 따르면 $d_k=64$차원의 모델 8개를 병렬로 처리한다. 이는 $d_{model}=512$차원의 모델 하나를 처리하는 것과 같다 (512 = 64 x 8). Applications of Attention in our ModelTransformer는 multi-head attention을 3가지 방식으로 활용한다. encoder-decoder attention 층에서 query는 이전 decoder에서 오고, key와 value는 encoder 출력에서 나온다. 따라서 decoder가 모든 입력 시퀀스 위치에 적용된다. 이는 seq2seq에서 전형적인 encoder-decoder attention 구조와 동일하다.encoder는 self-attention 층을 가지고 있다. self-attention에서 key, value, query는 같은 곳에서 나오며, 본 연구에서는 encoder 이전 층의 출력을 말한다. 따라서 encoder 위치가 이전 encoder의 모든 위치를 참고하게 된다. *자세히 말하면, embedding 된 단어를 key, value, query로 사용한다. 이를 통해 각 벡터 간 거리를 계산한다.decoder도 마찬가지로 self-attention을 통해 모든 위치를 참조한다. 하지만 auto-regressive 속성을 유지하기 위해 다음 출력의 영향을 받으면 안 된다. 따라서 softmax 입력을 모두 masking(-∞로 설정)하는 방식을 scaled dot-production attention에 적용했다.*Transformer는 순차적으로 정보를 입력하는 encoder-decoder와 달리 모든 값을 한 번에 입력한다. 따라서 미래 정보를 확인할 수 있다. 예를 들어, “the song Attention by Newjeans“라는 문장이 있다고 하자. Newjeans는 Attention 뒤에 위치한다. 따라서 시간 상 Attention → Newjeans 관계를 파악하는 것은 바람직하다. 하지만 Newjeans → Attention 순서로 맥락을 파악하는 것은 바람직하지 않은(illegal) 연결이다. 이러한 문제를 해결하기 위해 masking을 사용한다. masking 된 정보를 -∞로 설정하는 이유는 softmax를 거쳤을 때 0이 되도록 하기 위함이다.Position-wise Feed-Forward Networks각 sub-layer는 fully connected feed-forward network를 가진다. 모두 동일한 형태로 각 위치에 적용된다. 2개의 linear 층이며 ReLU를 활성화 함수로 사용한다.$FFN(x)=ReLU(xW_1+b_1)W_2+b_2$선형 변환에서 각 층마다 다른 파라미터를 가진다. 입출력은 $d_{model}=512$ 차원으로 내부 층은 $d_{ff}=2048$ 차원이다. (2048 =512 x 4. W1, W2, b1, b2에 대해)Embeddings and Softmax$d_{model}$ 차원의 벡터로 입출력을 변환하기 위해 학습된 embedding을 사용한다. decoder 출력을 확률로 변환하기 위해 선형 변환과 softmax를 사용했다. 본 연구는 두 embedding 층과 softmax 이전 선형 변환에서 같은 가중치 행렬을 사용했다. 그리고 embedding 층에서는 가중치에 $\\sqrt{d_{model}}$을 곱한다. Positional Encoding본 모델은 순환 구조가 없기 때문에 시퀀스 순서를 이해하기 위해 토큰에 위치 정보가 필요하다. 이를 위해 positional encoding을 encoder와 decoder의 input embedding 밑부분에 추가했다. positional encoding은 embedding과 더해질 수 있도록 같은 $d_{model}=512$ 차원을 가진다. 본 연구는 여러 방법 중 다른 주기를 가지는 sine과 cosine 함수를 이용한다. $PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$pos는 위치이며, i는 차원이다. 각 차원은 정현파(sinusoid)에 대응된다. 주기는 $2\\pi$에서 $10000\\cdot 2\\pi$가 된다. *positional encoding 추가 설명말 그대로 embedding 된 단어에 위치 정보를 추가해 주는 역할이다. 위치를 표기하는 방법은 다양하다. 예를 들어 첫 번째 단어는 1 … i번째 단어는 i로 나타낼 수 있다. 그런데 i 값이 너무 커지면 더했을 때 임베딩된 벡터와 관계없이 아주 큰 값이 된다. 임베딩 벡터는 단어 정보를 담고 있기 때문에 중요하다. 따라서 항상 -1 ~ 1 사이 범위를 가지는 sine, cosine 함수를 선택했다. 하지만 sine, cosine은 일정한 주기를 가지기 때문에 i가 커지면 중복 값이 발생할 수 있다. 따라서 논문에서는 i마다 다른 주기를 가지도록 PE 함수를 정의했다. 물론 i 값이 매우 커지면 겹치는 경우가 발생할 수 있지만, 현재 연구에서는 i에 비해 주기가 충분히 크기 때문에 문제가 되지 않는다.이를 통해 값이 너무 작거나 크지 않으면서 값이 중복되지 않도록 positional encoding을 수행했다.Why Self-Attention?self-attention을 사용한 이유는 크게 3가지이다.첫 번째는 연산 복잡도가 작다. 다른 하나는 연산을 병렬로 처리할 수 있다. 세 번째는 장거리 의존성(long-range dependencies)이다. 장거리 의존성을 가지는 학습에서 중요한 요인은 앞뒤로 정보를 주고받을 수 있는 경로의 거리다. 이 거리가 짧을수록 장거리 의존성을 학습하기 쉽다. 그래서 layer 종류에 따라 입력과 출력 간 경로 최대 길이를 비교했다.table1표에서 볼 수 있듯이 self-attention은 상수 시간으로 모든 위치를 연결한다. 반면 recurrent 모델은 O(n)이 걸린다. 단일 convolution에서 kernel 크기 k가 n보다 작으면 모든 입출력 위치를 연결할 수 없다. 따라서 contigious kernel에 대해 $O(n/k)$개의 convolutional 층이 필요하고, dilated convolution에 대해 $O(\\log_k(n))$가 들어 오히려 최대 길이가 증가한다. convolutional 층은 k 때문에 일반적으로 recurrent 층보다 비용이 많이 든다. seperable convolutional 층은 복잡도를 $O(knd+nd^2)$으로 매우 크게 줄여주지만 k = n이더라도 self-attetion + feed-forward layer와 동일하다.추가로 self-attention은 더 많은 해석 가능한(interpretable) 모델을 생산해 낼 수 있다. attention distribution을 살펴보면 아래 그림과 같다.fig5다양한 문제를 잘 해결할 뿐만 아니라 문장 의미와 문법을 잘 나타낸다.TrainingTraining Data and Batching450만 개 문장 쌍으로 구성된 stardard WMT 2014 영어-독일어 데이터를 학습했다. 영어-프랑스 번역 문제에서 3600만 개 WMT 영어-프랑스어 데이터를 사용했고, 토큰을 32000 word-piece 단어로 나눴다. 문장 쌍은 시퀀스 길이 정도로 batch 했다. 각 training batch는 대략 25000 source token과 25000개 target token을 담고 있는 문장 쌍이 들어있다. Hardware and Schedule8개의 NVIDIA P100 GPU로 학습했다. 논문에서 설명한 base model은 각 step이 0.4초 정도로 총 100,000 step, 12시간을 학습했다. big model은 각 step 당 1.0초로 300,000 step, 3.5일을 학습했다.OptimizerAdam optimizer를 사용했고, $\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-9}$이다. 아래 수식을 이용해 learning rate를 변화해 가며 학습했다.$lrate=d^{-0.5}_{model}\\cdot min(step\\_num^{-0.5},step\\_num\\cdot warmup\\_steps^{-1.5})$RegularizationResidual dropout: 각 sub-layer가 입력과 더해지고 정규화되기 전에 dropout 시킨다. 추가로 embedding과 positional encoding 합에도 dropout을 적용한다. base model은 $P_{drop}=0.1$을 적용한다.Label smoothing: label smoothing factor로 $\\epsilon_{ls}=0.1$을 사용한다. 모델을 모호하게 학습해 perplexity를 해치지만 accuracy와 BLEU 점수를 높여준다. ResultMachine TranslationWMT 2014 영어-독일어 번역 문제(task)에서 big transformer가 28.4 BLEU로 이전에 나온 모델을 능가하는 성능을 보였다. 모델 설정은 Table3에 기록했다. 학습은 8개 P100 GPU로 3.5일이 걸렸다. 심지어 base 모델도 학습 비용 측면에서 이전 모델 성능을 뛰어넘었다.WMT 2014 영어-프랑스어 번역 문제에서 big model은 41.0 BLEU로 이전에 발표된 single model을 뛰어넘었다. 이는 이전 SOTA 모델 학습 비용의 1/4로 달성했다. 영어-프랑스어 번역에 사용한 big model은 dropout rate를 0.1 대신 0.3으로 사용했다.base model은 마지막 5개 체크 포인트의 평균으로 구했으며, 각 체크 포인트는 10분 간격으로 나왔다. big model은 마지막 20개 체크 포인트를 평균 내 사용했다. beam search를 사용했고 beam size는 4, length penalty $\\alpha$는 0.6이다. 하이퍼파라미터는 validation set을 통해 나온 결과로 결정했다. inference에서 최대 출력 길이를 입력 길이 + 50으로 뒀지만, 가능한 빨리 끝내는 게 좋다.table2위 표는 결과를 요약하며 번역 성능과 학습 비용을 비교한다. 학습 시간, 사용한 GPU 개수, GPU의 single-precision floating-point 성능을 고려해 floating point operations를 예측했다.Model VariationsTransformer의 component 별 중요도를 평가하기 위해 base model을 다양하게 변형해 영어-독일어 번역 성능을 측정했다. 앞서 설명했듯 beam search를 사용했고, 대신 체크포인트를 평균 내는 방식은 사용하지 않았다. 결과는 아래 표에서 볼 수 있다. table3Table3 (A) 열에서 attention head 개수, key-value 차원을 다르게 하되 연산 일관성을 유지했다. single-head attention은 0.9 BLEU로 성능이 하락했고, 너무 많은 head는 성능을 떨어뜨린다.Table3 (B) 열에서 attention key 차원을 줄이니 성능에 문제가 발생했다. (C)와 (D) 열은 큰 모델일수록 성능이 좋고, dropout이 over-fitting을 막는데 도움이 된다는 사실을 보여준다. (E) 열은 sinusiudal positional encoding 대신 학습된 positional embeddings을 사용했고 base model과 거의 비슷한 결과를 보였다. ConclusionTransformer는 오직 attention만을 사용한 첫 sequence transduction model이며, 가장 흔하게 사용되는 recurrent 층을 multi-headed self-attention으로 대체했다. 변역 문제에서 recurrent나 convolutional 모델에 비해 훨씬 빠르게 학습한다. WMT 2014 영어-독일어와 영어-프랑스어 문제에서 SOTA를 달성했다.학습과 평가에 사용한 코드는 https://github.com/tensorflow/tensor2tensor에서 확인할 수 있다."
  },
  
  {
    "title": "Chripy 블로그 만들기",
    "url": "/playground/2023/10/15/init-chripy.html",
    "categories": "Playground",
    "tags": "blog",
    "date": "2023-10-15 00:00:00 +0900",
    





    
    "snippet": "만약 본인이 Jekyll을 사용해본 경험이 있다면 공식 문서를 참고하는 게 더 빠를 수 있다. 그런데 Jekyll도 잘 모르고 FrontEnd도 잘 모르겠다하는 사람은 이 글을 잘 찾아왔다.지금 보고 있듯이 Chirpy은 정말 깔끔한 Jekyll 테마이다. 하지만 막상 시작하려니 계속 문제가 생겨서 5시간 정도를 허무하게 날렸다. 모드 변경도 안 되고...",
    "content": "만약 본인이 Jekyll을 사용해본 경험이 있다면 공식 문서를 참고하는 게 더 빠를 수 있다. 그런데 Jekyll도 잘 모르고 FrontEnd도 잘 모르겠다하는 사람은 이 글을 잘 찾아왔다.지금 보고 있듯이 Chirpy은 정말 깔끔한 Jekyll 테마이다. 하지만 막상 시작하려니 계속 문제가 생겨서 5시간 정도를 허무하게 날렸다. 모드 변경도 안 되고 난리도 아니였다. 심지어 로컬 PC를 사용할 수 없는 상황이라 Ruby를 설치하고 build를 할 수 없어 더 골치 아팠다. 같은 문제를 겪지 않을 수 있게 과정을 기록해봤다.  처음 글을 작성할 당시는 군 복무 중이었기 때문에 Github 웹 에디터에서 글을 작성하고 바로 build, deploy하는 묘기를 선보였다. 하지만 현재(2025)는 로컬 환경에서 작업하고 있다. 따라서 글도 로컬 환경에서 확인 후 빌드하는 방법으로 수정하였다.세팅된 Repo Fork하기Repo로 가면 이 블로그가 돌아가고 있는 Repo(저장소)가 보인다.여기서 상단의 Fork를 클릭하자. 그럼 해당 Repo가 자신의 Repo로 복사된다. 복사된 곳에서 본인의 블로그로 수정해 쓸 수 있다.중간에 생략한 부분은 나중에 수정할 수 있으니 중요하지 않다. 중요한 건 딱 하나다.  Repo 이름은 {유저 ID}.github.io로 만들어야 한다.  Create Fork를 누른다.앞에 보면 자신의 Github ID가 있다. 그걸 똑같이 적고 + .github.io를 적으면 된다. 그리고 기다리면 Repo가 그대로 복사된다.Github Pages아직은 Repo만 복사된 상태이다. Github Pages를 해줘야 서버에 올릴 수 있다.  Settings &gt; Pages &gt; Build and deployment &gt; SourceRepo에서 Github Actions를 선택하면 GitHub Pages Jekyll라는 옵션이 생긴다. 그럼 Configure 버튼을 누른다. 그리고 아무런 수정 없이 계속 Commit Changes를 누르면 된다. 그런 뒤 유저.github.io로 블로그에 들어가보자.  유저의 이름이 Denev6라면 블로그 주소는 https://denev6.github.io로 생성된다. 이름에 대문자가 있어도 소문자로 입력해야 한다.주소로 들어갔을 때 블로그가 보이면 반은 성공이다. deploy가 완료되면 Repo에 ✅초록 체크가 생긴다. 안 보인다면 조금 기다렸다 들어가거나 새로고침을 하면 보인다. 완료되었다면 git clone으로 로컬 환경으로 저장소를 불러온다.git clone https://github.com/이름/이름.github.io.git초기화 세팅지금 이 블로그를 복사해 가져간 것이기 때문에 글도 적혀있고 불필요한 세팅도 되어 있을거다. 일단은 초기화를 해보자. 아래 내용은 반드시 해야하는 작업이다.  /assets/posts 모두 삭제  /assets/img 모두 삭제  /_posts 모두 삭제  /_sites 모두 삭제  /scripts 모두 삭제  /_config.yml 내용 삭제 후, /tools/_config_init.yml 내용 복사/ 붙여넣기  _config.yml을 삭제하지 않으면 블로그 세팅이 초기화되지 않는다. 이 작업은 필수다. tools/_config.yml에 미리 초기화 시켜둔 파일이 있으니 그대로 복붙해서 사용하자._config.yml_config.yml은 블로그의 전반적인 세팅을 담당하는 파일이다. _config.yml을 초기화했다면 잘 읽어보고 입력하자. 주요 설정 값들은 아래와 같다.lang: ko  # 블로그 주요 언어 (영어: en)timezone: Asia/Seoul  # 사용할 시간대# 검색: http://www.timezoneconverter.com/cgi-bin/findzone/findzonetitle: 제목 # 블로그 이름tagline: 부제목 # 블로그 부제목description: &gt;- # 블로그 설명  \"설명\"url: \"https://이름.github.io\" # 프로필 클릭 시 이동할 주소github:  username: github_username  # Github IDtwitter:  username: twitter_username  # 트위터 IDsocial:  name: 이름  # 본인 이름  email: ID@mail.com  # 메일 주소  links:    # 외부 링크. 처음에 적히는 링크는 글 저작권자의 링크로 사용    - https://github.com/이름     # - https://이름.tistory.com/     # - https://www.instagram.com/이름    # - https://www.linkedin.com/in/usernametheme_mode: # light 또는 dark. 비워두면 자동으로 설정img_cdn: \"https://이름.github.io\"  # 이미지 기본 경로# 현재 블로그 주소로 작성하면 편하다. avatar: /assets/img/avatar.jpg  # 프로필 이미지 주소comments:  # 댓글 관련 설정  active: # 사용할 서비스 작성  ...paginate: 10  # 한 페이지에 보여줄 글 수 사용하지 않을 정보는 비워두면 된다.FaviconFavicon은 반드시 assets/img/favicons/에 위치해야 한다. real-favicon-generator에서 파비콘을 만든 뒤, 생성된 파일들을 그대로 assets/img/favicons/에 넣어주면 끝난다. 자세한 내용은 공식 문서에도 나와 있다.AboutAbout으로 들어오면 내용이 그대로 남아있을 거다. _tabs/about.md에서 내용을 수정할 수 있다. 설정만 남겨두고 다 지워도 된다.---icon: fas fa-info-circleorder: 1---Contact사이드바 하단에 보면 Github부터 여러 아이콘이 있다. 이곳은 _data/contact.yml에서 수정할 수 있다.- type: 링크 종류  # 아이콘은 fontawesome에서 찾을 수 있다.  icon: \"fa-solid fa-pen-to-square\"  # 해당 링크를 현재 탭에서 보여줄지 여부  # false면 새로운 탭에서 보여준다.  noblank: false    # 이동할 주소  url: \"\"Authors닉네임:  name: 이름  twitter: 트위터 ID  url: 대표 URL 주소authors는 글을 쓸 때 글의 저자를 입력할 수 있게 해준다. 지금은 필자의 아이디로 되어 있으니 본인 걸로 수정하자.여기까지 했다면 웬만한 초기화는 완료됐다. 사실상 초기화를 하며 동시에 간단한 커스텀까지 했다. 만약 문제 없이 따라왔다면 local server를 띄워보거나 git commit해서 잘 돌아가는지 확인해자.글쓰기기본적으로 마크다운으로 글을 작성한다. 기본적인 마크다운 기능 외에 Chirpy에서 사용할 수 있는 기능들을 알아보자.파일명글은 _posts/YYYY-MM-DD-NAME.md에 작성된다. 파일명 양식은 반드시 지켜야 한다. 만약 _post 폴더가 없다면 만들면 된다.  ie. 2000-01-01-test-post.md설정본문을 설정하기 전 상단에 설정 값을 적어야 한다.기본---title: 제목category: 카테고리 # [c1, c2...]tags: 태그 # [t1, t2...]---추가---date: YYYY-MM-DD HH:MM:SS +/-TTTTauthor: 글쓴이 IDtoc: true # 우측에 인덱스 생성comments: falsepin: true # Home 상단에 글 고정---링크[텍스트](/ai/2023/01/01-title.html)만약 블로그 내의 글을 연결하고 싶다면 /부터 작성한다. 만약 연결하고 싶은 글의 파일명이 2023-01-01-title.md이고, ai라는 파일에 들어가 있다고 하자. 그럼 (/ai/2023/01/01/title.html)로 작성한다.[텍스트](#제목)동일한 글 내의 제목으로 이동하고 싶다면 #을 사용한다. 예를 들어 제목이 모델 소개이라면 모델-소개로 작성한다.수식---math: true ---\\[\\cfrac{1}{N}\\sum_{i=1}^{k}x_i\\]$$수식$$으로 Latex 문법의 수식을 작성한다.만약 인라인으로 $\\cfrac{1}{N}$ 이렇게 수식을 작성할 때는 $수식$으로 쓴다. 공식문서는 inline도 $$수식$$으로 작성하라고 하지만 실제 작성해보니 깨지는 경우가 대부분이었다.이미지---img_path: /assets/img/---이미지 경로를 요약해 쓸 수 있다. 만약 img_path가 /assets/img이고, 본문에서 flower.png를 사용했다면 assets/img/flower.png를 불러온다.---image:  path: /path/to/image  alt: image alternative text---썸네일 이미지를 지정할 수 있다.아바타 이미지![alt](/path/to/image)_Caption_이미지 바로 밑에 _캡션_을 작성할 수 있다.![alt](img.png){: w=\"700\" h=\"400\" }이미지 크기를 지정할 수 있다.![alt](img.png){: .normal }![alt](img.png){: .left }![alt](img.png){: .right }이미지를 정렬할 수 있다.![Light mode only](/path/to/light-mode.png){: .light }![Dark mode only](/path/to/dark-mode.png){: .dark }라이트모드와 다크모드에 이미지를 따로 적용할 수 있다.![alt](img.png){: .shadow }이미지에 그림자 효과를 줄 수 있다. (다크모드에서는 티가 안 날 수도 있다.)프롬프트&gt; 내용{: .prompt-info }  prompt-info  prompt-tip  prompt-warning  prompt-danger코드 블럭{: file=\"path/to/file\" }코드 블록 바로 밑에 작성하면 코드의 파일명을 지정할 수 있다.fmt.Print(\"Hello World!\"){: .nolineno }줄번호를 생략한다.for i, name := range names {    b.WriteString(name)}확인 및 빌드먼저 PC에 Ruby와 jekyll, bundler가 설치되어 있어야 한다. 자세한 설치법은 이미 많은 블로그에서 다루고 있기 때문에 넘어가겠다. “ruby jekyll bundler 설치”로 검색하면 많이 나온다.설치가 완료되면 아래 명령어로 로컬 서버를 실행한다.bundle exec jekyll serve이제 브라우저를 열고 “http://127.0.0.1:4000/”로 접속하면 작성한 파일이 보인다. 종료하고 싶을 땐 터미널에서 Ctrl+C를 누른 뒤, y를 입력하고 엔터하면 된다.문제가 없다면 github에 올리기 전, build를 해주어야 한다. build 하지 않고 올리면 블로그가 보이지 않는다.bundle exec jekyll b -d \"_site\" --incremental_site 폴더 아래에 뭔가가 많이 생겼다면 성공이다. 이제 github에 올리면 나머지는 github actions이 자동으로 처리한다.git add .git commit -m \" \"git push  만약 build하는 과정을 자동화하고 싶다면 .github/workflows/pages-deploy.yml에 주석 처리된 부분을 수정하면 된다."
  },
  
  {
    "title": "PySet을 Go답게",
    "url": "/playground/2023/10/14/goset.html",
    "categories": "Playground",
    "tags": "Go, Python",
    "date": "2023-10-14 00:00:00 +0900",
    





    
    "snippet": "PySet은 아주 유용한 자료구조이다. 이를 Go로 Go스럽게 구현하기 위해 CPython의 소스코드와 golang 소스코드를 살펴보았다. set과 map이 뒤에서 어떻게 작동하는지를 살펴보고 가장 합리적인 방법으로 집합을 구현해보려 한다.문제Python에는 집합이라는 아주 유용한 구조가 있다. set 객체는 크게 2가지 역할이 있는데, 중복 값을 제...",
    "content": "PySet은 아주 유용한 자료구조이다. 이를 Go로 Go스럽게 구현하기 위해 CPython의 소스코드와 golang 소스코드를 살펴보았다. set과 map이 뒤에서 어떻게 작동하는지를 살펴보고 가장 합리적인 방법으로 집합을 구현해보려 한다.문제Python에는 집합이라는 아주 유용한 구조가 있다. set 객체는 크게 2가지 역할이 있는데, 중복 값을 제거하는 것과 빠르게 값을 탐색하는 것이다. n = [1, 3, 3, 5, 6, 3, 8]n = set(n)print(n)# {1, 3, 5, 6, 8}has_three = (3 in n)print(has_three)# True하지만 Go는 set을 제공하지 않는다. 따라서 set과 유사하게 작동하는 객체를 만들어보려 한다. 그런데 Go를 곁들인.CPython의 Set 분석Python의 구현체인 C코드를 뜯어보자. 코드는 Github-python에 공개되어 있다.typedef struct {    PyObject *key;    Py_hash_t hash;             /* Cached hash code of the key */} setentry;typedef struct {    PyObject_HEAD    Py_ssize_t fill;            /* Number active and dummy entries*/    Py_ssize_t used;            /* Number active entries */    Py_ssize_t mask;    setentry *table;    Py_hash_t hash;             /* Only used by frozenset objects */    Py_ssize_t finger;          /* Search finger for pop() */    setentry smalltable[PySet_MINSIZE];    PyObject *weakreflist;      /* List of weak references */} PySetObject;핵심은 setentry이다. setentry를 보면 key를 가지고 있고, 주석을 통해 key를 hash한다는 걸 알 수 있다. 즉, PySet은 Hash Table의 구조를 가지고 있다고 추측해 볼 수 있다. /* set object implementation   Written and maintained by Raymond D. Hettinger &lt;python@rcn.com&gt;   Derived from Lib/sets.py and Objects/dictobject.c.   The basic lookup function used by all operations.   This is based on Algorithm D from Knuth Vol. 3, Sec. 6.4.   ...*/set을 구현한 파일의 첫 주석이다. set은 dict 객체에서 파생되었다고 언급하고 있다. 이 주석을 통해 set이 Hash Table 구조를 사용한다는 것을 알 수 있다.쉽게 말해, PySet은 key만 있는 PyDict이다.Go Map 분석Go는 map이라는 자료구조를 제공한다. 이는 {key:value}로 매핑되는 Hash table이다. 이를 통해 set을 구현할 수 있을 것 같다. 그런데 value는 필요하지 않다. 따라서 value 자리에 zero-value를 넣어 마치 값이 없는 것과 같은 효과를 낼 수 있다.// nilmap[T]interface{}{}map[i] = nil// structmap[T]struct{}{}map[i] = struct{}{}대표적으로 nil과 빈 struct가 있다. 따라서 둘 중 어떤 값이 더 효과적일지 판단해야 한다.interface와 메모리 자원import (\t\"fmt\"\t\"unsafe\")func main() {\tfmt.Println(unsafe.Sizeof(struct{}{}))      // 0\tvar nilInterface interface{} = nil\tfmt.Println(unsafe.Sizeof(nilInterface))      // 16}빈 struct는 메모리 자체를 할당받지 않는다. 즉 메모리에 없는 값이다. 반면 nil interface는 16Byte를 할당받는다.그럼 빈 struct를 사용하면 메모리 할당이 적은가? 그건 또 아니다.Bucket의 작동원리map이 어떻게 구현되어 있는지 살펴보자.// Map contains Type fields specific to maps.type Map struct {\tKey  *Type // Key type\tElem *Type // Val (elem) type\tBucket *Type // internal struct type representing a hash bucket}Map은 key:value 쌍과 Bucket을 갖는다. 특징적인 점은 Bucket을 가진다는 것이다. 조금 더 깊게 들어가보자.// A map is just a hash table. The data is arranged// into an array of buckets. Each bucket contains up to// 8 key/elem pairs. The low-order bits of the hash are// used to select a bucket. Each bucket contains a few// high-order bits of each hash to distinguish the entries// within a single bucket.//// If more than 8 keys hash to a bucket, we chain on// extra buckets.//// When the hashtable grows, we allocate a new array// of buckets twice as big. Buckets are incrementally// copied from the old bucket array to the new bucket array.// mapextra holds fields that are not present on all maps.type mapextra struct {\t// If both key and elem do not contain pointers and are inline, then we mark bucket\t// type as containing no pointers. This avoids scanning such maps.\t// However, bmap.overflow is a pointer. In order to keep overflow buckets\t// alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow.\t// overflow and oldoverflow are only used if key and elem do not contain pointers.\t// overflow contains overflow buckets for hmap.buckets.\t// oldoverflow contains overflow buckets for hmap.oldbuckets.\t// The indirection allows to store a pointer to the slice in hiter.\toverflow    *[]*bmap\toldoverflow *[]*bmap\t// nextOverflow holds a pointer to a free overflow bucket.\tnextOverflow *bmap}앞으로 key:value 쌍을 entry라고 부르겠다. bucket은 8쌍의 entry를 담고 있다. 그리고 map은 bucket의 배열로 데이터를 저장한다. 만약 bucket 내의 entry가 유효한 포인터를 가지고 있지 않으면 overflow bucket을 생성해 값을 할당한다. overflow bucket에 할당된 entry는 더 이상 GC(Garbage collector)에 스캔되지 않는다.빈 struct는 포인터를 가질 수 없다. 따라서 overflow bucket으로 분류될 것이다. 따라서 GC에 의해 스캔되지 않는다. 하지만 nil interface는 포인터가 될 수 있기 때문에 계속해서 GC에 스캔된다. 이는 속도를 느리게 만든다.반면 overflow bucket을 계속해서 생성하면서 지속적인 메모리 할당이 발생한다는 단점이 있다. 이를 해결하기 위해서는 map을 초기화할 때 capacity를 크게 잡으면 된다. map은 메모리 공간이 부족할 때 2배씩 늘려가며 메모리를 확보한다. 따라서 처음부터 넉넉하게 메모리를 할당해두면 bucket을 생성하는 빈도가 줄어든다.결론결론적으로 빈 struct를 사용하는 것이 성능(속도/ 메모리) 면에서 더 뛰어나다. 만약 map의 capacity까지 넉넉하게 초기화해주면 더욱 좋은 성능을 보인다.참고: memory-allocation-and-performance-in-golang-mapsGo로 구현PySet의 주요 아이디어만 빌려와 GoSet을 만들어보자. map[T]struct{}Go에서는 map을 통해 Hash Table을 만들 수 있다. key만 가지는 map이 간단하게 구현되었다. 간단한 코드로 GoSet이 잘 작동하는지 확인해보자. import (\t\"fmt\")func main() {\tdata := []int{7, 3, 3, 5, 6, 1, 5}    \tset := make(map[int]struct{})\t// add\tfor _, n := range data {\t\tset[n] = struct{}{}\t}\t// print\tfor key, _ := range set {\t\tfmt.Printf(\"%d \", key)\t}\t// 3 5 6 1 7 }예상대로 값이 출력되었다. 주의할 점은 map의 key는 순서를 유지하지 않는다. 이 부분은 PySet, PyDict도 동일하다.type으로 구현추상화를 통해 set을 일반화 시켜보자. Generic을 사용해 다양한 타입의 데이터를 담을 수 있도록 작성했다.  Generic은 Go v1.18에 처음 추가된 기능이다. 따라서 1.17이하의 버전에서는 정확히 Key의 데이터 타입을 명시한 후 사용해야 한다.// Set 구현type Set[T comparable] struct {\ttable map[T]struct{}}// 빈 Set 초기화func NewSet[T comparable]() *Set[T] {\temptySet := make(map[T]struct{})\treturn &amp;Set[T]{emptySet}}// Set에 값 추가func (s *Set[T]) Add(key T) {\ts.table[key] = struct{}{}}// Set에 값이 있는지 확인func (s *Set[T]) Has(key T) bool {\t_, ok := s.table[key]\treturn ok}// Set에서 값 삭제func (s *Set[T]) Pop(key T) bool {\tif s.Has(key) {\t\tdelete(s.table, key)\t\treturn true\t}\treturn false}// Set에 모든 값 출력func (s *Set[T]) PrintAll() {\tfor key, _ := range s.table {\t\tfmt.Printf(\"%d \", key)\t}\tfmt.Print(\"\\n\")}이제 Set을 사용해보자. package mainimport \"fmt\"func main() {\tdata := []int{3, 5, 5, 6, 7, 7}\tset := NewSet[int]()\tfor _, n := range data {\t\tset.Add(n)\t}\tset.PrintAll() // 3 5 6 7 \tfmt.Println(set.Has(5)) // true\tok := set.Pop(7)\tif ok {\t\tset.PrintAll() // 5 6 3 \t}}예상한 대로 잘 작동하는 것을 볼 수 있다. GoSet이 slice보다 빠를까?중복을 제거할 때는 Set이 분명한 장점을 갖는다. 하지만 Set이 정말 탐색에서도 빠를까?코드의 흐름은 다음과 같다.   9999999개의 정수를 각각 slice와 Set에 추가한다.   이때 추가되는 값은 100 이하의 랜덤한 정수이다.   마지막에 slice와 Set에 101을 각각 추가한다. (slice의 마지막에 101이 위치한다.)  slice와 Set에서 101을 탐색한다. import (\t\"fmt\"\t\"math/rand\"\t\"time\")func main() {\tlenData := 9999999\tmaxRandInt := 100\ttarget := maxRandInt + 1\ttestSlice(lenData, maxRandInt, target)\ttestSet(lenData, maxRandInt, target)}func testSlice(lenData, maxRandInt, target) {\t// Slice 초기화 + 탐색 (최악의 경우)\tstart := time.Now()\tdata := make([]int, 0) // Set과 같은 조건에서 시작\tfor i := 0; i &lt; lenData; i++ {\t\tdata = append(data, rand.Intn(maxRandInt))\t}\tdata = append(data, target)\tfor _, n := range data {\t\tif n == target {\t\t\tbreak\t\t}\t}\tduration := time.Since(start)\tfmt.Println(\"Slice으로 탐색\", duration)}func testSet(lenData, maxRandInt, target) {\t// Set 초기화 + 탐색\tstart := time.Now()\tset := NewSet[int]()\tfor i := 0; i &lt; lenData; i++ {\t\tset.Add(rand.Intn(maxRandInt))\t}\tset.Add(target)\tset.Has(target)\tduration := time.Since(start)\tfmt.Println(\"Set으로 탐색\", duration)}// Slice으로 탐색 1.75740032s// Set으로 탐색 1.343899904s예상대로 Set이 빠르다. 하지만 slice의 크기가 작다면 어떨까? lenData를 99999로 줄이고 실행해봤다. lenData := 99999// Slice으로 탐색 9.400064ms// Set으로 탐색 37.700096msSet이 훨씬 유리한 조건이었음에도 slice가 더 빠르다. 정확히는 map의 초기화가 느린 것이다.같은 조건에서 초기화 시간을 제외하고 순수 탐색에 사용한 시간을 측정해 봤다. lenData := 99999// Slice으로 탐색 99.84µs// Set으로 탐색 0sSet이 더 빠르다. 따라서 탐색할 상황이 많다면 Set이 유리하다. 하지만 탐색이 많이 일어나지 않고, 데이터가 충분히 크지 않다면 오히려 무식하게 slice로 탐색하는 게 더 빠를 수 있다."
  },
  
  {
    "title": "Support Vector Machine",
    "url": "/machine-learning/2023/10/10/svm.html",
    "categories": "Machine-Learning",
    "tags": "Python, AI",
    "date": "2023-10-10 00:00:00 +0900",
    





    
    "snippet": "Linear SVMSupport Vector Machine은 분류 문제를 해결하는 머신러닝 기법이다. 대체적으로 준수한 성능을 보이며 SVM 또는 SVC(Support Vector Classifier)라고 부른다.아이디어직선으로 두 종류의 클래스를 분류하는 문제는 어렵지 않다. 그런데 과연 “어떤 경계가 가장 잘 분류했다고 할 수 있을까?”, “새로운...",
    "content": "Linear SVMSupport Vector Machine은 분류 문제를 해결하는 머신러닝 기법이다. 대체적으로 준수한 성능을 보이며 SVM 또는 SVC(Support Vector Classifier)라고 부른다.아이디어직선으로 두 종류의 클래스를 분류하는 문제는 어렵지 않다. 그런데 과연 “어떤 경계가 가장 잘 분류했다고 할 수 있을까?”, “새로운 데이터가 추가되었을 때, 어떤 경계가 가장 잘 분류해낼까?” 이 질문의 답으로 Margin을 제안한다.먼저 Support Vector를 알아보자.경계(초평면)와 가장 인접한 양쪽의 데이터들을 Support Vector라고 한다. 그리고 경계(초평면)와 수직인 데이터 간 거리를 Margin이라고 한다.이 때 Margin이 최대가 되어야 한다. 만약 새로운 데이터가 비슷한 양상으로 들어온다면 어떨까? (위 이미지)오른쪽 분류기는 극단적으로 값을 나눈다. 반면, 왼쪽 경계는 상대적으로 완만하게 범위를 잘 나눈다. 따라서, Margin을 최대화했을 때 경계가 합리적이라고 할 수 있다.문제 정의  Margin이 최대가 되는 경계를 찾아라Margin을 정의하고 Margin을 최대화하는 문제를 풀면 된다.분류를 수행하는 경계를 $wx+b=0$이라고 하자.양성 쪽 초평면은 $wx+b=1$이며, 그 위에 있는 support vector를 $x^{+}$라고 하자.음성 초평면은 $wx+b=-1$, 그 위의 support vector를 $x^{-}$라고 하겠다.Margin은 두 support vector 사이의 거리이므로 $ \\parallel x^{+}-x^{-}\\parallel$로 정의할 수 있다.그리고 각 support vector를 식에 대입하면,$wx^{+}+b=1 \\ wx^{-}+b=-1$두 식을 빼면,$w(x^{+}-x^{-})=2 \\ x^{+}-x^{-}=\\cfrac{2}{w}$따라서, Margin을 w에 대해 정의할 수 있다.$Margin=\\parallel x^{+}-x^{-}\\parallel =\\parallel \\cfrac{2}{w}\\parallel$이제 풀어야할 문제는 $max\\cfrac{2}{\\parallel w\\parallel }$이다.  아래 풀이는 안 봐도 관계없다. 조금 더 수학적으로 접근한 내용일 뿐이다. 선형(Linear) 분류에서 중요한 개념은 이게 전부다.풀이 (참고)$max\\cfrac{2}{\\parallel w\\parallel}$를 풀기 위해 필요한 조건이 있다.$y_{i}\\in{-1, +1}$를 클래스라고 하자. (ie. 동그라미/ 네모)$y_{i}=1$일 때, $wx_{i}+b\\geq1$이고$y_{i}=-1$일 때, $wx_{i}+b\\leq-1$이다.따라서, $y_{i}(wx_{i}+b)\\geq1$이 된다.이제 문제를 다시 정리해보자.$max\\cfrac{2}{\\parallel w\\parallel} \\ y_{i}(wx_{i}+b)\\geq1$다시 표현하면,$min\\cfrac{1}{2}\\parallel w\\parallel \\ y_{i}(wx_{i}+b)\\geq1$식을 뒤집게 되면서 최대화(max) 문제가 최소화(min) 문제로 바뀐다. 한 단계 더 나가면,$min\\cfrac{1}{2}w^{2} \\ y_{i}(wx_{i}+b)\\geq1$절댓값은 부호를 없애기 위해 사용했으므로 제곱을 해도 동일한 식이 성립한다. 이렇게 돌아온 이유는 Quadratic programming 또는 Lagrange multiplier를 사용하기 위해서이다. 이 글에서 다루기에는 너무 길어진다. SVM의 핵심은 아니니 생략하겠다.오차 허용 (C)실제 데이터는 이상치(outlier)가 존재한다.정상적인 데이터라면 파란 세모는 음성 쪽 초평면 아래에 위치해야 한다. 하지만 분류에 어긋나는 데이터들이 존재할 수 있다. 이 때 데이터와 초평면 사이의 거리를 $\\varepsilon$이라고 하자. (분류하는 경계와의 거리가 아니다.)SVM 식은 아래와 같이 재정의된다.\\[min(\\cfrac{1}{2}w^{2}+C\\sum_{k=1}^{R}\\varepsilon_{k})\\]C는 오차를 얼마나 반영할지를 나타낸다. 이 값은 우리가 정해줘야 한다.  C = 0: 오차를 무시한다. 오차를 최대로 허용한다.  C = ∞: 오차를 최대로 반영한다. 오차를 허용하지 않는다.C 값이 클수록 오차에 민감해지고, 작을수록 오차에 관대해진다.Non-Linear (Kernel)때로는 선형 경계로는 분류할 수 없는 데이터도 있다. 이럴 때는 데이터 차원을 높여 문제를 해결한다.위 예시의 경우, 1차원 데이터를 2차원으로 맵핑했다.\\[\\Phi(x): x \\to (x, x^{2})\\]이처럼 차원을 변환하면 해결할 수 있다. 하지만 모든 데이터를 특정 차원으로 맵핑하려면 많은 비용이 든다. 따라서 연산을 줄이는 꼼수를 사용한다.Kernel TrickLinear-SVM/풀이에서 생략했지만, SVM을 푸는 과정에서 두 벡터 간의 점곱을 계산하게 된다. 이때 Kernel Trick을 사용하면 연산을 줄일 수 있다.예를 들어, $x$를 변환하는 $\\Phi(x)$를 아래와 같이 정의해보자.\\[\\Phi(x_{1}, x_{2}):(x_{1}, x_{2})\\to(1,\\sqrt{3}x_{1},\\sqrt{3}x_{2},\\sqrt{3}x_{1}^{2},\\sqrt{3}x_{2}^{2},x_{1}^{3},x_{2}^{3},\\sqrt{6}x_{1}x_{2},\\sqrt{3}x_{1}x_{2}^{2},\\sqrt{3}x_{1}^{2}x_{2})\\]그리고 $\\Phi(x_{1})\\cdot\\Phi(x_{2})$를 풀어보자.\\[\\Phi(x_{1})\\cdot\\Phi(x_{2})=(x_{1}\\cdot x_{2}+1)^{3}\\]결과는 간단하게 정리된다. 따라서 실제로 데이터의 차원을 변환하는 것이 아니라 점곱의 계산 결과를 이용해 빠르게 처리할 수 있다. 위 커널 함수를 일반화하면 아래와 같다.\\[K(x_{1},x_{2})=\\Phi(x_{1})\\cdot\\Phi(x_{2})=(x_{1}\\cdot x_{2}+1)^{k}\\]여러 커널 함수가 있지만 가장 많이 사용되는 건 RBF(Radial Basis Function)이다.\\[K(x_{1},x_{2})=exp(-\\cfrac{\\parallel x_{1}-x_{2}\\parallel^2}{2\\sigma^{2}})\\]\\[=exp(-\\cfrac{\\parallel x_{1}\\parallel^2}{2})exp(-\\cfrac{\\parallel x_{2}\\parallel^2}{2})\\sum_{n=0}^{\\infty}\\cfrac{(x_{1}\\cdot x_{2})^n}{n!}\\]식을 풀어보면 데이터를 무한차원으로 맵핑하는 모습을 볼 수 있다.Python 코드from sklearn.svm import SVCfrom sklearn.metrics import accuracy_score# SVC 설정model = SVC(C=1, kernel=\"rbf\")# SVC 학습model.fit(X_train, y_train)# SVC로 예측y_pred = model.predict(X_test)# 정확도 출력accuracy_score(y_test, y_pred)sklearn에서 SVC를 제공한다. SVC의 파라미터를 보면 kernel과 C 값을 받는다. 자세한 내용은 공식 문서를 참고하자.kenrel은 비선형 문제를 해결할 때 사용했던 커널 함수다. 앞에서 소개했던 RBF를 적용한 모습이다. 이외에도 linear, poly, sigmoid, precomputed가 있다.C도 오차 허용에서 봤던 그 값이다. 기본값은 1로 설정되어 있다.실행sklearn의 iris 데이터를 이용해 실제 SVM을 테스트 해봤다. iris는 붓꽃 데이터로 0, 1, 2 3종류의 붓꽃 클래스를 가지고 있다.import pandas as pdimport numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitiris = load_iris()df = pd.DataFrame(    data=np.c_[iris.data, iris.target],    columns=[\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"target\"],)FEATURES = [\"sepal width\", \"petal length\"]df = df[[*FEATURES, \"target\"]]df = df[df[\"target\"] != 1]X = df[FEATURES]y = df[\"target\"]X_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.3, random_state=30)데이터를 불러오는 코드이다. (잘 몰라도 된다.)df.head()            index      sepal width      petal length      target                  0      3.5      1.4      0.0              1      3.0      1.4      0.0              2      3.2      1.3      0.0              3      3.1      1.5      0.0              4      3.6      1.4      0.0      데이터를 처리한 결과를 보자. sepal width, petal length를 특징으로 가진다. 참고로 데이터 클래스는 0과 2를 사용했다.from sklearn.svm import SVCfrom sklearn.metrics import accuracy_scoremodel = SVC(kernel=\"linear\", C=1)model.fit(X_train, y_train)y_pred = model.predict(X_test)accuracy_score(y_test, y_pred)1.0시각화를 위해 linear하게 분류해 보았다. 정확도 100%가 나온다.시각화결과에 집중하자. (시각화 코드 몰라도 된다.)import matplotlib.pyplot as pltt0 = df[df[\"target\"] == 0]t2 = df[df[\"target\"] == 2]w = model.coef_[0]b = model.intercept_support_vectors = model.support_vectors_plt.scatter(x=FEATURES[0], y=FEATURES[1], data=t0, c=\"r\", label=\"0\")plt.scatter(x=FEATURES[0], y=FEATURES[1], data=t2, c=\"b\", label=\"2\")plt.scatter(    support_vectors[:, 0], support_vectors[:, 1], c=\"y\", label=\"support vectors\")line = np.linspace(2, 5)plt.plot(line, -(w[0] * line + b) / w[1], c=\"b\")plt.legend()plt.show()파란점과 빨간점은 원래 데이터다. 노란점은 support vector이고, 검정 선은 SVM이 분류에 사용한 경계다. 시각화 해봤더니 잘 계산된 모습을 볼 수 있다."
  },
  
  {
    "title": "Go를 빠르게 굴리기",
    "url": "/playground/2023/10/08/faster-go.html",
    "categories": "Playground",
    "tags": "Go",
    "date": "2023-10-08 00:00:00 +0900",
    





    
    "snippet": "Go를 시작한 나는 Go뽕을 느끼기 위해 백준 문제를 Go로 풀어봤다. 그런데 일부 문제는 Python 풀이보다 더 느린 결과를 보였다. 뭔가 잘못됐음을 직감했고, 백준에 제출된 고인물들의 코드를 살펴봤다. 그렇게 삽질이 시작됐다.삽질 결과: 속도 향상  공간 확보: 1.2x  빠른 입력: 17x  문자열 합치기: 71x  정규 표현식: 2.6x메모리...",
    "content": "Go를 시작한 나는 Go뽕을 느끼기 위해 백준 문제를 Go로 풀어봤다. 그런데 일부 문제는 Python 풀이보다 더 느린 결과를 보였다. 뭔가 잘못됐음을 직감했고, 백준에 제출된 고인물들의 코드를 살펴봤다. 그렇게 삽질이 시작됐다.삽질 결과: 속도 향상  공간 확보: 1.2x  빠른 입력: 17x  문자열 합치기: 71x  정규 표현식: 2.6x메모리 공간 확보s := make([]int, 100)  // 슬라이스, len: 100, cap: 100s[i] = val  // 값 대입s := make([]int, 0, 100)  // 슬라이스, len: 0, cap: 100s = append(s, val)  // 값 대입slice를 생성할 때, 해당 slice에 저장될 값의 범위를 알 수 있다면 미리 메모리에 공간을 확보하는 것이 유리하다. make는 slice의 형식을 입력받은 후, len과 cap을 입력받는다. len은 슬라이스의 길이로 []int를 3으로 선언하면 [0 0 0]이 만들어진다.중요한 점은 Capacity이다. 만약 길이가 3인 슬라이스에 값을 추가해 길이가 4인 슬라이스를 만든다고 하자. 그럼 Go는 새로운 슬라이스를 만들게 된다. 이 때 성능 저하가 발생한다. 불필요한 슬라이스의 재생성을 막기 위해서는 cap을 지정하면 된다. cap은 미리 메모리 공간을 얼마나 확보해둘지에 대한 값이다. len이 3, cap이 5라면 [0 0 0]을 생성하지만 5개의 값이 들어갈 수 있는 메모리 공간을 확보해둔 상태이다. 따라서 값을 하나 추가해도 새로운 슬라이스를 생성하지 않는다. 이는 map에도 동일하게 적용된다. 다른 점이라면 map은 make를 할 때 바로 cap을 받는다.make(map[int]bool, 100) // len: 0, cap: 100결과백준 10815를 풀이한 결과이다. 문제에서 최대 입력은 500,000개이다.            cap      풀이 시간      메모리                  500,000      684ms      32576KB              100,000      644ms      48360KB              0      784ms      50356KB      다른 문제에서도 slice와 map 모두 공간을 미리 확보한 풀이가 빠른 모습을 보여줬다.참고배열을 사용해 정적으로 공간을 확보하는 방법도 있지만 Go에서는 유연하게 길이를 조정할 수 있는 slice를 선호한다. 예를 들어, 함수에 값을 넘길 때 배열보다 slice가 더 범용적으로 사용될 수 있다. // 모든 int Slice를 받음func GetSlice(s []int) []int {\treturn s}// 길이가 3인 int 배열만 받음func GetArray(a [3]int) [3]int {\treturn a}func main() {\t// 정상 실행\ta := make([]int, 3)\ta = GetSlice(a)\t// 에러 발생\ts := [5]int{}\ts = GetArray(s) // 배열의 길이가 다름}빠른 입력입력이 많은 문제의 경우, fmt 입력 구문은 느리다. bufio 패키지를 사용하면 시간을 단축할 수 있다.결과백준 14425번 문제를 4가지 입력 방식을 이용해 풀이해 보았다.             입력 방식      풀이 시간                  scanner.Scan      116ms              reader.ReadString      124ms              fmt.Fscan(reader, …)      268ms              fmt.Scan      시간 초과 (2초 이상)      어떤 입력을 사용하는가에 따라 시간을 2배 이상 단축하기도 하고, 시간 초과가 발생하기도 한다. // 가장 빠른 풀이var sc = bufio.NewScanner(os.Stdin)func main() {\tsc.Scan()       \tsentence := sc.Text()}문제백준 27649을 풀어보니 분명 Python에서 문제가 없었는데 Go로 작성하니 계속 ❗틀렸습니다❗가 나왔다. 그런데 Scanner를 Reader로 교체하니 문제가 해결되었다.const (\t// MaxScanTokenSize is the maximum size used to buffer a token\t// unless the user provides an explicit buffer with Scanner.Buffer.\t// The actual maximum token size may be smaller as the buffer\t// may need to include, for instance, a newline.\tMaxScanTokenSize = 64 * 1024\tstartBufSize = 4096 // Size of initial allocation for buffer.)scanner.Scan은 큰 입력을 받지 못한다. Scanner를 구현한 소스코드를 살펴보면 MaxScanTokenSize라는 값이 정의되어 있다. 만약 입력의 크기가 64KB보다 크면 문제가 발생할 수 있다. 따라서 값이 클 것으로 예상되면 차선책인 reader.ReadString을 사용하는 것이 안전하다.해결책import (\t\"bufio\"\t\"os\"\t\"strings\")var reader = bufio.NewReader(os.Stdin)func main() {\tsentence, _ := reader.ReadString('\\n')\tsentence = strings.TrimSpace(sentence)}NewScanner 대신 NewReader를 사용한다. 참고로 ReadString은 마지막 \\n까지 읽어온다. 따라서 TrimSpace를 통해 줄바꿈 문자를 제거해줘야 한다. 이거 놓쳐서 많이 틀렸다.문자열 합치기 (출력)res := []string{\"a\", \"b\", \"c\"}fmt.Println(strings.Join(res, \"-\"))// a-b-c문자열을 연결할 때 + 연산자를 사용할 수도 있지만 느리다. 따라서 strings.Join을 사용하면 빠르게 문자열을 이어붙일 수 있다. 첫 인자로 string Slice를 입력받고, 두 번째 인자로 문자열 사이에 삽입할 문자열을 건네준다. Join 메서드가 문자열을 합치는 과정을 보면 내부적으로 Builder를 사용하고 있다. (strings.go;line456)words := []string{\"a\", \"b\", \"c\"}var b strings.Builderb.WriteString(words[0])for _, s := range words[1:] {    b.WriteString(\"-\")    b.WriteString(words)}fmt.Print(b.String())// a-b-c  A Builder is used to efficiently build a string using Write methods. It minimizes memory copying. The zero value is ready to use. Do not copy a non-zero Builder.      pkg.go.dev  Builder에 대해 알아두면 시간 단축에 많은 도움이 된다. 대표적인 메서드는 아래와 같다.  Len: 축적된 문자열의 길이  Reset: 초기화  String: 축적된 문자를 문자열로 반환  WriteRune: 문자 입력  WriteString: 문자열 입력결과백준 1181번 문제를 풀이한 결과를 보자.            문자열 결합      풀이 시간                  builder.WriteString(“\\n”)      28ms              Println      884ms              += “\\n”      시간 초과 (2초 이상)      WriteString이 압도적으로 빠른 것을 볼 수 있다. 심지어는 Python을 이용한 동일한 풀이도 200ms가 나왔다. 그에 비해 Go가 884ms나 시간 초과를 내는 것을 보면 잘못된 문자열 조작이 얼마나 치명적인가를 알 수 있다.정규 표현식Go의 정규 표현식인 regexp는 비교적 느리다고 알려져 있다. 따라서 직접 Go 레벨에서 처리해주는 것이 속도 향상에 도움이 될 수 있다.// regexp 예시re := regexp.MustCompile(`[&lt;&gt;\\(\\)]|&amp;&amp;|\\|\\|`)sentence = re.ReplaceAllString(sentence, ` $0 `)결과            풀이 방식      풀이 시간                  for { switch }      168ms              regexp.ReplaceAllString      444ms      백준 27649을 풀어본 결과, 복잡한 regexp를 사용하는 것보다 반복문+조건문으로 직접 구현하는 것이 더 빠른 것을 볼 수 있다. 다만 ‘백준 2870’, ‘백준 1264’와 같이 간단한 문제는 정규 표현식을 사용해도 성능에 큰 영향은 없었다. 함수 인라인만약 사용 중인 Go의 버전이 1.16 이하라면 함수의 인자와 반환값을 스택에 전달하는 방식을 사용한다. 이로 인해 약간의 성능 저하가 발생할 수 있으므로 간단한 함수라면 인라인 처리하는 것이 유리하다.  Go 1.17 implements a new way of passing function arguments and results using registers instead of the stack. Benchmarks for a representative set of Go packages and programs show performance improvements of about 5%, and a typical reduction in binary size of about 2%.- Go 1.17 Release Notes그리고 이 부분은 Go 1.17에서 해결되었다.백준과 leetcode에서 Go 1.18을 사용하고 있기 때문에 큰 문제가 되지 않는다. 굳이 인라인 처리해서 코드를 더럽게 만들지 말자. (23.10.09)체크리스트  사전에 메모리를 충분히 확보했는가?  fmt로 입력을 받고 있지 않은가?  fmt나 +로 문자열을 적고 있지 않은가?  복잡한 정규표현식을 사용하고 있지 않은가?  Go가 최신 버전인가?이 글은 코딩테스트 한정 Go를 빠르게 만드는 방법이다.성능이 크게 중요하지 않다면 가독성 좋은 코드, 안정적인 코드, 수정/확장이 용이한 코드가 우선이라는 걸 잊지 말자. 불쌍한 Gopher를 위해서라도.  이미지 출처: tottie000/GopherIllustrations  The Go gopher was designed by Renée French. Illustrations by tottie."
  },
  
  {
    "title": "영상을 통한 우울증 예측 모델 분석",
    "url": "/paper-review/2023/02/06/multimodal-depression.html",
    "categories": "Paper-Review",
    "tags": "AI, CV, Python",
    "date": "2023-02-06 00:00:00 +0900",
    





    
    "snippet": "동기성균관대 우수학부생 프로그램을 통해 우울증 챗봇 개발에 참여하는 기회를 얻었다. 이전에 실시간 얼굴 인식 프로젝트를 진행한 경험이 있어 얼굴 이미지를 통해 우울증을 탐지하는 멀티모달 구현에 도전했다.논문 요약 (번역/ 정리)이미지를 중심으로한 우울증 연구 중 논문: Automatic Depression Detection via Learning an...",
    "content": "동기성균관대 우수학부생 프로그램을 통해 우울증 챗봇 개발에 참여하는 기회를 얻었다. 이전에 실시간 얼굴 인식 프로젝트를 진행한 경험이 있어 얼굴 이미지를 통해 우울증을 탐지하는 멀티모달 구현에 도전했다.논문 요약 (번역/ 정리)이미지를 중심으로한 우울증 연구 중 논문: Automatic Depression Detection via Learning and Fusing Features from Visual Cues을 찾게 되었다.문맥을 위해 일부 표현이 의역됐으며, 혼동의 여지가 있을 경우 영문과 번역을 함께 작성했다. 기술 용어는 혼동이 없도록 원문으로 작성했다.( + 오역이 있다면 댓글로 알려주세요!)Abstract  In this paper, we propose a novel Automatic Depression Detection (ADD) method via learning and fusing features from visual cues. Specifically, we firstly construct Temporal Dilated Convolutional Network (TDCN), in which multiple Dilated Convolution Blocks (DCB) are designed and stacked, to learn the long-range temporal information from sequences. Then, the Feature-Wise Attention (FWA) module is adopted to fuse different features extracted from TDCNs.이 논문에서 우리는 학습을 통한 자동 우울증 탐지(ADD)와 시각 정보를 융합하는 방법을 제안한다. 먼저, Temporal Dilated Convolutional Network(TDCN)이 있다. TDCN은 여러 Dilated Convolution Blocks(DCB)이 겹쳐 있는 형태로 연속된 정보로부터 맥락을 학습할 수 있다. 그리고 Feature-Wise Attention(FWA)이 적용되어 여러 TDCN에서 추출된 특징을 연결할 수 있다.Introduction과거 우울증 진단은 Eight-item Patient Health Questionnaire depression scale(PHQ-8) 같은 방식을 사용했으며, 전문가의 주관적인 견해가 반영된다. 하지만 ADD는 언어/ 시각 정보를 바탕으로 보다 객관적인 판단을 내릴 수 있다.우울증 환자는 멍한(glazed) 표정이나 특징적인(abnormal) 얼굴 움직임을 가지고 있다. 하지만 즉시 이러한 특징을 보이지는 않는다. 대신 비교적 긴 시간을 관찰해야 알아챌 수 있다. 따라서 시각 정보로 우울증을 탐지하기 위해서는 시간(temporal) 정보를 다루는 과정이 필요하다. 선행 연구에서 LSTM과 TCN을 활용했지만 여전히 장기(overlong sequences) 정보를 충분히 고려하지 못했고, 단일 시각 정보를 활용함으로서 복합적인 시각 단서를 반영하지 못했다.이 연구는 크게 Temporal Dilated Convolutional Network(TDCN)과 Feature-Wise Attention(FWA) 두 종류의 모듈로 구성되어 있다. TDCN은 dilated convolution 연산을 통해 우울증 정보를 추출한다. FWA는 각 특징 채널(feature channels)에 다른 가중치를 부여해 탐지된 특징을 강화한다.  TDCN은 긴 영상에서 시간(temporal) 정보를 효과적으로 추출해낸다. TDCN 내에는 두 개의 평행한 dilated convolution 모듈이 적용되어 우울증 탐지에 필요한 유용한 정보를 학습하도록 했다.  FWA 모듈은 TDCN branch로부터 학습된 정보를 융합하기 위해 설계했다. Attention 모듈은 더 중요한 정보를 강조해 ADD의 정확도를 높인다.Fig. 1Temporal Dilated Convolution NetworkTDCN은 일반적으로 multi-layer로, 하나의 layer는 5개의 Dilated Convolutional Blocks(DCB)과 4개의 Max-Pooling layers로 구성된다. 각 TDCN 층의 DCB는 다른 범위의 지각 정보(perceptive ranges)를 탐색한다. 그리고 TDCN 파이프라인에서 Max-Pooling 층은 계속해서 특징의 크기(resolution)를 줄여나가며 중요 반응을 점진적으로 추출한다.Fig. 2Fig 2는 평행한 두 dilated convolution이 어떻게 구성되어 있는지 볼 수 있다.  입력: $ X=[x_1;x_2;…;x_T]\\in \\mathbb{R}^{T\\times D} $  $ T $: 시간(time step)  $ D $: 특징의 차원dilated convolution은 아래와 같이 표현된다.\\[F(t)=\\sum_{i=0}^{k-1}filter(i)\\cdot x_{t+d\\cdot (i-1)}+b\\]$ d $는 dilation factor, $ k $는 kernel 크기, $ b $는 편향(bias)이다. 입력과 출력의 크기를 맞추기 위해 Zero-Padding이 적용됐다. dilation factor는 2배씩 증가하며 다른 범위(time spans)에서 시간(temporal) 정보를 얻는다. 다른 dilaton 인자 사이에는 합의 평균과 ELU가 적용된다.\\[f_{ELU}(x)=\\left \\lbrace\\begin{matrix}x &amp; \\text{if } x\\geq 0 \\newlinee^x-1 &amp; \\text{if } x&lt;0\\end{matrix}\\right.\\]네트워크가 깊어지며 발생하는 degradation 문제를 피하기 위해 추가(residual) 블록을 추가했다. 다음 단계에서 요소별(element-wise) 덧셈을 수행하기 위해 kernel 크기가 1인 1D convolution 층을 모든 DCB에 추가했다. DCB의 마지막에는 batch 정규화를 통해 학습을 가속화했지만, gradient vanishing 문제가 발생할 수 있다. 따라서 다른 분포의 특징을 남겨두기 위해 TDCN의 마지막 DCB에서는 정규화 층을 제거했다.마지막 TDCN을 제외한 모든 DCB 뒤에 max-pooling 층을 추가했다. 이는 출력 tensor가 더 넓은 범위를 수용해 중요한 장기(long sequence) 정보를 모은다. 또한 sequence의 길이를 줄여 모델의 복잡도를 줄이는 역할도 한다.Feature-Wise AttentionFig. 4FWA는 다른 종류의 시각 정보를 효과적으로 합치기 위해 설계됐다. 먼저 다른 TDCN branch에서 학습된 특징을 직접적으로 연결(concatenate)해 $ X\\in \\mathbb{R}^{T\\times kD} $를 도출한다. 여기서 $ D $는 특징의 차원, $ k $는 TDCN brach의 개수다. 본 연구에 $ k $는 2이다. 그 다음, global average pooling이 적용돼 특징별 벡터 $ s\\in \\mathbb{R}^{kD} $를 얻는다. global average pooling은 아래와 같이 정의된다.\\[s_j=\\cfrac{1}{T}\\sum_{i=0}^{T-1}x_{i,j}\\]여기서 $ x_{i,j} $는 i번째 time step &amp; j번째 특징 차원의 $ X $을 나타내는 단위이다. 그 후 2개의 Linear 층과 ReLU가 $ s $에 적용된다. 최종적으로 sigmoid가 적용되며 결과인 $ h\\in \\mathbb{R}^{kD} $는 특징 채널의 중요도를 나타낸다.\\[h=\\sigma_{sigmoid}(W_2(f_{ReLU}(W_1s)))\\]\\[\\tilde{X}=F_{scale}(x,h) =X \\odot \\tilde{H}\\]$ h $를 $ X $와 같은 크기로 broadcast 시킨 다음 요소별 곱(element-wise product)을 통해 결과를 도출한다.데이터데이터는 Distress Analysis Interview Corpus Wizard-of-Oz dataset (DAIC WOZ)를 사용했다. DAIC는 오디오, 영상 그리고 오디오를 받아쓴 필기본(transcript)을 가지고 있다. training/validation/testing 크기는 각각 107/35/47이다. 본 연구는 모든 샘플의 길이를 5000으로 다듬어 사용했다. 시각 정보는 OpenFace toolkit으로 추출된 68개의 2D/3D 얼굴 랜드마크, Action Units(AUs), 주시(gaze) 정보, 얼굴 방향(head-pose) 그리고 Histogram of Oriented Gradients(HOG) 특징이다. 본 연구는 2가지 특징을 사용해 모델의 성능을 측정했다. 참고로 3개 또는 그 이상의 정보를 사용해 봤지만 탐지 성능이 크게 향상되지 않았다. (뒤에서 자세한 설명이 나온다.)학습 정보  우울증 데이터는 1(positive), 비우울증 데이터는 0(negative)으로 레이블을 매겼다.  DCB의 특징(feature) 차원은 2차원 이미지에 대해 256, 256, 128, 64, 64로, 얼굴 방향에 대해 128, 64, 256, 128, 64로 사용했다.  optimizer는 SGD, 학습률은 2e-5, momentum은 0.9이다.  mini-batch 크기는 8이다.다른 모델과의 비교(표의 일부 내용은 생략했다. 원본은 논문을 참고하자.)선행 연구와 비교            Method      Feature      Accuracy      F1-score                  SVM      V      -      0.500              CNN      AUs+Gaze+Pose      -      0.530              SGD-SVM      3D Landmarks+Gaze+Pose      -      0.63              C-CNN      A+L+3D Landmarks      -      0.769              SS-LSTM-MIL      2D Landmarks      -      0.783              본 연구      2D Landmakrs+Pose      0.857      0.800      A는 오디오, V는 시각 정보, L은 텍스트 정보를 뜻한다. 본 연구는 다른 single-modal과 multi-modal 모델에 비해 높은 점수를 보였다. 이를 통해 본 연구의 모델이 시각 정보를 종합적으로 잘 판단했다고 평가할 수 있다.Single Modal과 비교초기에는 하나의 특징만으로 이용해, 하나의 TDCN branch로 학습했다.            Feature      Accuracy      Recall      F1-score                  AUs      0.638      0.357      0.370              Gaze      0.596      0.214      0.240              Pose      0.660      0.214      0.273              2D Landmarks      0.596      0.214      0.240              Now      0.660      0.643      0.530      두 종류의 특징을 결합할 때 가장 높은 점수를 기록했다. Recall이 크게 향상된 것을 통해 우울증 환자를 더 잘 찾아낸 것을 알 수 있다. 이는 ADD 문제를 효과적으로 해결한다는 사실을 입증한다.Multi Modal과 비교            Features      Accuracy      F1-Score                  Pose+AUs      0.800      0.720              Landmarks+AUs      0.829      0.786              Landmarks+Pose      0.857      0.815              Landmarks+Pose+Gaze      0.743      0.609              Landmarks+AUs+Pose+Gaze      0.686      0.421      다른 특징으로 학습한 결과 2D Landmark + Pose가 가장 높은 성능을 기록했다. 단일 모델보다 여러 특징을 조합한 멀티 모달의 성능이 전반적으로 더 우수했다. 여러 조합을 시도한 결과 랜드마크를 사용했을 때 대체적으로 좋은 성능을 보였다. 랜드마크가 얼굴 특징에 대한 정교한 정보를 제공하기 때문으로 분석된다. 3개 이상의 특징을 결합할 경우 분명한 성능 감소가 나타났다. 특징이 많아지면 모델의 크기가 커지며 over-fitting 문제가 발생하기 때문으로 보인다.데이터 전처리에 따른 비교            Method      Accuracy      F1-score                  Head-first      0.857      0.815              Average      0.629      0.519      데이터 전처리 방법에 따라서 성능 차이가 있었다. Head-first는 본 연구에서 사용한 방식으로 데이터 처음부터 5000씩 잘라 사용한 방법이다. Average는 데이터를 여러 조각으로 나눈 뒤 soft predicting 점수의 평균을 이용해 선택하는 방식이다. 표에서 볼 수 있듯이 Head-first 방식이 가장 좋은 성능을 보였다. 분할된 데이터 조각(sub-sequences)은 우울증 특징을 담고 있지 않을 수 있기 때문에 average 방식이 낮은 점수를 기록했다고 볼 수 있다.일부 모듈 제거            backbone      Accuracy      F1-score                  TCN      0.686      0.522              TDCN      0.857      0.815      TDCN 대신 TCN을 사용할 경우 성능 저하가 발생했다. 뿐만 아니라 낮은 FLOPs(FLoating point Operations Per Second)를 보여 TDCN의 연산 효율이 좋다는 점도 알 수 있었다.(결과 표 생략)FWA를 제거한 모델, Max-Pooling 대신 Average-Pooling을 사용한 모델을 학습해봤지만 성능 향상은 없었다.구현 계획논문에서 제안한 TDCN을 단순화한 모델을 목표로 했다.변경된 내용  FWA 대신 Classifier 층 사용  DCB 내 Dilation이 2인 DCN과 1CNN만 사용  Github 코드: archive/tdcn_demo.ipynb  batch size: 8  learning rate: 1e-5  optimizer: Adam  loss function: CrossEntropy  모델 구조에서 Detector는 얼굴 랜드마크 정보를 추출하는 모듈로 Github: archive에서 확인할 수 있다.학습 결과 분석Accuracy는 0.702, F1-score는 0.000이다. 결과값을 살펴보면 레이블과 관계없이 모든 데이터에 대해 [1.000  0.000]을 뱉어낸다. 비우울증(0) 데이터가 훨씬 많기 때문에 이런 편향된 결과를 도출했다고 생각한다.박사과정의 연구원분께 조원을 구하니 간단한 CNN 기반의 Base 모델을 만들어 학습하라고 말해주셨다. 만약 Base 모델에서 유의미한 학습이 진행되면 우리의 모델이 잘못 설계되었다고 생각할 수 있다. 반면, Base 모델에서도 같은 현상이 일어난다면 데이터 전처리에 문제가 있을 가능성이 높다. CNN 기반의 Base model에서도 유사한 현상이 발견됐고 데이터에 문제가 있는 것으로 판단했다.프로젝트 마무리하며여러 팀원분의 도움을 받아 문제를 해결하려 했지만 결국 답을 찾지 못했다. 2월에 군입대가 예정되어 있었기 때문에 나는 프로젝트를 그만둘 수 밖에 없었다. 답을 찾지 못하고 마무리하니 찝찝했다. 그래도 논문 하나를 깊게 분석하고, 논문 저자와 컨택하며 문제를 해결하려는 시도는 값진 경험이라고 생각한다. 또 NIPA나 CLOVA 같이 외부 서버에 접속해 학습하는 기회도 얻을 수 있었다. 비록 분명한 결과물은 없지만 탐구하고 고민하는 과정에서 그 어느 때보다 많이 배울 수 있는 프로젝트였다."
  },
  
  {
    "title": "Optimizer 살펴보기",
    "url": "/machine-learning/2023/01/05/optimizer.html",
    "categories": "Machine-Learning",
    "tags": "Pytorch, AI",
    "date": "2023-01-05 00:00:00 +0900",
    





    
    "snippet": "Jaewan-Yun: optimizer-visualizationOptimizer는 모델의 Training Loss를 최소화하는 방향으로 파라미터를 업데이트하는 중요한 역할을 한다. 쉽게 말해 모델을 어떤 방향으로 얼마만큼 업데이트할 것인지를 결정하는 역할을 한다. Optimizer는 Gradient Descent(경사하강법)를 기반으로 한다. 기울기(...",
    "content": "Jaewan-Yun: optimizer-visualizationOptimizer는 모델의 Training Loss를 최소화하는 방향으로 파라미터를 업데이트하는 중요한 역할을 한다. 쉽게 말해 모델을 어떤 방향으로 얼마만큼 업데이트할 것인지를 결정하는 역할을 한다. Optimizer는 Gradient Descent(경사하강법)를 기반으로 한다. 기울기(Gradient)는 학습 방향을 결정하고, 학습률은 학습할 정도를 결정한다. 여기까지 내용을 모른다면 “경사하강법과 학습률“을 먼저 이해해야 한다. 아래 내용은 Gradient Descent를 기반으로 한 여러 optimizer의 개념들을 설명한다.기호 정리  $w$: 가중치  $t$: 시점(step)  $\\mu$: 학습률  $L$: Loss 값SGDSGD는 Stochastic Gradient Descent의 약자로 각각의 데이터를 반영해 가중치를 업데이트한다. 기본적인 경사 하강법은 전체 데이터를 살펴 본 후, 한 번에 가중치를 업데이트한다. 그에 반해 SGD는 각각의 데이터를 보고 학습을 진행하기 때문에 더 빠른 학습이 가능하다. 하지만 그만큼 노이즈에 민감할 수 있다는 단점도 있다. \\[w_t=w_{t-1}-\\mu\\cfrac{dL}{dw}\\]주의할 점은 일반적으로 SGD라고 하면 Mini-batch Gradient Descent를 뜻한다. 이름 그대로 미니 배치의 데이터를 확인한 후, 가중치를 업데이트하는 방식이다. Pytorch의 SGD를 이용해 학습할 때는 Mini-batch Gradient Descent의 개념을 가지고 있으며, 아래에서 소개할 momentum도 적용할 수 있다. momentum경사 하강법의 saddle point(안장점) 문제를 가지고 있다. 안장점 문제란 최솟값은 아니지만 기울기가 0에 가까워서 업데이트되지 않는 상황을 뜻한다. 이러한 문제를 해결하기 위해 도입된 개념으로, momentum(관성)은 물체가 운동하는 추세를 뜻한다. Gradient의 위치를 보면 기울기가 거의 0에 가깝다. 따라서 0에 가까운 아주 작은 값이 업데이트되면서 마치 학습이 멈춘 것처럼 보이게 된다. 이런 상황을 안장에 안착하였다고 해서 안장점 문제라고 한다. 본론으로 돌아가서 momentum은 마치 공에 관성이 있는 것처럼 값이 움직이는 것을 말한다. \\[v_t=\\gamma v_{t-1}-\\mu\\cfrac{dL}{dw}\\]\\[w_t=w_{t-1}+v_t\\]미분 값을 활용하여 업데이트하는 것이 아니라 누적된 $v$값도 함께 적용된다. 따라서 만약 이전의 기울기가 현재의 기울기와 같은 방향이라면 관성이 적용된다. $\\gamma$는 관성 계수로 0 ~ 1 사이의 값을 가지며 $v_0$는 0으로 초기화된다. 일반적으로 관성 계수로 0.9가 많이 사용한다. import torch.optim as optimoptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)for epoch in range(n_epochs):    for x, y in dataloader:        # 생략...            optimizer.step()문제는 가파른 구간에서 over shooting 문제 발생한다. 이전 기울기가 가파르다면 현재의 기울기가 작아져도 관성이 적용되어 원래보다 크게 움직이게 된다. 그러면 아래처럼 최솟값을 지나치는 문제가 발생할 수도 있다. Adagradmomentum에서 over-shooting 문제가 발생한 것은 현재의 기울기 상태를 적게 반영했기 때문이다. Adagrad는 현재 기울기가 커지면 학습률을 ‘상대적으로’ 줄여주고, 반대로 현재 기울기가 커지면 학습률을 상대적으로 늘려주는 기능을 가진다. \\[h_t=h_{t-1}+\\cfrac{dL}{dw}\\odot \\cfrac{dL}{dw}\\]\\[w_t=w_{t-1}-\\cfrac{\\mu}{\\sqrt{h_t}+\\epsilon}\\times\\cfrac{dL}{dw}\\]$\\odot$은 행렬의 원소별 곱을 뜻한다. 즉, $h$는 가중치 변화율의 제곱을 더해 계산된다. 기울기에 제곱을 하게 되면서 방향이 아닌 기울기의 크기에 집중하였다. 기울기의 크기가 커지면 $h$가 증가하면서 최종적으로 학습률은 감소한다. 반대로 기울기의 크기가 작아지는 구간에서는 $h$가 감소하며 상대적으로 학습률이 증가하는 효과를 얻을 수 있다. 하지만 학습이 진행됨에 따라 $h$값이 계속 누적하여 증가하게 되고, 최종적으로 학습률이 0에 가까워진다. 이렇게 되면 더 이상 학습이 진행되지 않는 문제가 발생할 수 있다.RMSPropRoot Mean Sqaure Propagation, 줄여서 RMSProp이라고 부르는 기법은 Adagrad에 지수가중이동평균을 적용하였다. 쉽게 말해 $h$에 무작정 값을 누적하는 것이 아니라 이전 상태와 현재 상태에 반영 비율을 적용해 주는 것이다. \\[h_t=\\gamma h_{t-1}+(1-\\gamma)\\cfrac{dL}{dw}\\odot \\cfrac{dL}{dw}\\]\\[w_t=w_{t-1}-\\cfrac{\\mu}{\\sqrt{h_t}+\\epsilon}\\times\\cfrac{dL}{dw}\\]이전 상태의 정보를 담고 있는 $h_{t-1}$은 $\\gamma$만큼 반영하고, 현재 미분 값의 제곱은 $(1-\\gamma)$만큼 반영하여 새로운 $h$를 구한다. 그리고 Adagrad와 같은 원리로 계산한다. 이를 통해 Adagrad처럼 $h$가 커지는 현상을 완화했다. 하지만 그만큼 이전 상태를 덜 반영하게 된다. 눈치챘겠지만 RMSProp을 개선한 버전이 다음에 소개할 Adam이다. Adammomentum + RMSProp의 개념을 적용한 것이 Adam이다. \\[m_t=\\beta_1 m_{t-1}+(1-\\beta_1)\\cfrac{dL}{dw}\\]\\[\\hat{m_t}=\\cfrac{m_t}{1-\\beta_1}\\]\\[v_t=\\beta_2 v_{t-1}+(1-\\beta_2)\\cfrac{dL}{dw}\\odot\\cfrac{dL}{dw}\\]\\[\\hat{v_t}=\\cfrac{v_t}{1-\\beta_2}\\]\\[w_t=w_{t-1}-\\mu\\cfrac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon}\\]\\[m_0=0, v_0=0\\]위 식에서 $m$을 계산한 과정은 momentum을 계산하는 식과 동일하다. $v$를 구하는 과정은 RMSProp과 동일하다. $\\hat{m}$과 $\\hat{v}$를 계산하는 과정은 bias correction이라고 한다. bias correction과 관련된 자료는 아래 링크를 참조하자.  bias correction 설명: youtu.be/lWzo8CajF5s  Adam에서 correction을 하는 이유: stats.stackexchange.com/questions가장 많이 사용되는 값은 $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$이다. import torch.optim as optimoptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)추가로 Adam의 변형된 버전인 AdamW, NAdam 등도 있다. 가장 널리 사용되는 optimizer는 Adam이다. Adam을 사용하면 보편적으로 나쁘지 않은 성능을 보인다. 교수님이 안 되면 일단 Adam 써보라고 하셨을 정도. 클래식한 SGD + momentum도 종종 사용된다. Pytorch에서는 optim을 통해 사용할 수 있다."
  },
  
  {
    "title": "역방향으로 미분값 계산하기",
    "url": "/machine-learning/2022/12/31/backpropagation.html",
    "categories": "Machine-Learning",
    "tags": "Pytorch, AI",
    "date": "2022-12-31 00:00:00 +0900",
    





    
    "snippet": "딥러닝으로 모델을 학습시키기 위해 미분 값을 구하는 과정이 필요하다. 만약 왜 미분이 필요한지 모른다면 ‘경사하강법과 학습률‘을 참고하면 된다. 해당 내용을 몰라도 이번 글을 이해하는 데는 문제는 없다. 배경 지식  미분의 정의 (+극한)  도함수  합성 함수 표현문제점일반적으로 미분 값을 구할 때, 도함수를 구한 후 값을 대입해 계산한다.\\[f(x)...",
    "content": "딥러닝으로 모델을 학습시키기 위해 미분 값을 구하는 과정이 필요하다. 만약 왜 미분이 필요한지 모른다면 ‘경사하강법과 학습률‘을 참고하면 된다. 해당 내용을 몰라도 이번 글을 이해하는 데는 문제는 없다. 배경 지식  미분의 정의 (+극한)  도함수  합성 함수 표현문제점일반적으로 미분 값을 구할 때, 도함수를 구한 후 값을 대입해 계산한다.\\[f(x)=ax^3+bx^2+c \\\\ \\cfrac{d}{dx}f(x)=3ax^2+2bx\\]하지만 문제는 모델의 연산 과정이 너무 복잡하다. \\[f(x)=Linear(Droupout( ... (maxpool(relu(conv(...))))))\\]위 예시는 아주 기본적인 CNN 모델의 일부이다. 그리고 가장 많이 사용되는 손실 함수인 Cross Entropy with Softmax는 아래와 같이 정의된다. \\[H(x,y)=-\\cfrac{1}{N}\\sum_c^Nlog(\\cfrac{exp(f(x_c))}{\\sum_i^N exp(f(x_i))})y_c\\]위 모델을 $f(x)$라고 할 때, $H(x, y)$에 $f(x)$를 대입하고 도함수를 구한다고 생각하면 막막하다. 나는 할 자신이 없다. 도함수를 구하는 것이 매우 복잡하고 비효율적이다. 이러한 문제를 해결할 수 있는 아이디어로 Chain-Rule이 있다.Chain-Rule미분 값을 구하는 과정을 이해하기 위해서는 연쇄 법칙이라고 부르는 Chain-Rule에 대한 배경 지식이 필요하다.\\[\\cfrac{dL}{da}=\\cfrac{dL}{dc}*\\cfrac{dc}{db}*\\cfrac{db}{da}\\]결론부터 이야기하면 연쇄 법칙은 분수를 약분하듯이 분자, 분모가 연쇄적으로 계산된다는 법칙이다. 여기까지만 알아도 미분값을 구하는 데는 문제가 없다. 그래도 확실히 하기 위해 조금 더 자세히 이야기해 보겠다. 합성 함수를 미분하기 위해 연쇄 법칙을 적용해 보자. \\[y=f(g(x))=f\\circ g(x) \\\\ g(x)=u\\to f(u)=y\\]미분 가능한 함수 $f$와 $g$에 대해 위와 같은 관계가 성립한다고 하자. \\[\\cfrac{dy}{du}=\\displaystyle\\lim_{\\triangle u\\to0}\\cfrac{\\triangle y}{\\triangle u}\\]미분의 정의를 활용해 작성한 식이다. 여기서 $u$와 $x$의 관계를 살펴보자.\\[\\triangle u=g(x+\\triangle x)-g(x)\\]\\[\\triangle x\\to0\\ \\ \\text{then}\\ \\ \\triangle u \\to0\\]$u$의 변화량은 $g(x)$의 변화량을 뜻한다. $x$의 변화량이 0에 가까워지면 $u$의 변화량도 0에 가까워진다. 따라서 $\\cfrac{dy}{du}$를 다시 정의하자. \\[\\cfrac{dy}{du}=\\displaystyle\\lim_{\\triangle u\\to0}\\cfrac{\\triangle y}{\\triangle u}=\\displaystyle\\lim_{\\triangle x\\to0}\\cfrac{\\triangle y}{\\triangle u}\\]\\[\\cfrac{du}{dx}=\\displaystyle\\lim_{\\triangle x\\to 0}\\cfrac{\\triangle u}{\\triangle x}\\]이제 구해둔 단서를 연결해 보자. \\[\\cfrac{dy}{du}*\\cfrac{du}{dx}=\\displaystyle\\lim_{\\triangle x\\to0}\\cfrac{\\triangle y}{\\triangle u}*\\displaystyle\\lim_{\\triangle x\\to0}\\cfrac{\\triangle u}{\\triangle x}\\\\=\\displaystyle\\lim_{\\triangle x\\to0}(\\cfrac{\\triangle y}{\\triangle u}*\\cfrac{\\triangle u}{\\triangle x})\\\\=\\displaystyle\\lim_{\\triangle x\\to0}\\cfrac{\\triangle y}{\\triangle x}\\\\=\\cfrac{dy}{dx}\\]극한 값의 특징을 이용해 연쇄 법칙을 확인할 수 있다. 그리고 합성 함수의 미분 값을 여러 미분 값으로 쪼갤 수 있다는 것도 알 수 있다.그렇다면 앞에서 봤던 모델의 미분도 연쇄 법칙을 이용해 쪼개볼 수 있다. $y=maxpool(relu(conv(x)))=maxpool\\circ relu\\circ conv(x)$위와 같은 합성 함수의 연산을 아래와 같이 표현할 수 있다. $conv(x)=z_0\\to relu(z_0)=z_1\\to maxpool(z_1)=y$$\\cfrac{dy}{dx}=\\cfrac{dy}{dz_1}\\cfrac{dz_1}{dz_0}\\cfrac{dz_0}{dx}$그리고 각각의 함수는 덧셈, 곱셈, 제곱, log, sin, cos 등등 작은 단위의 연산으로 쪼갤 수 있다. 따라서 우리는 작은 단위의 연산에 대해 미분 값을 정의하면, 커다란 함수의 미분값도 계산할 수 있게 된다. 연산자와 미분 결과다양한 연산이 있지만 이번 글에서는 가장 기본적인 덧셈(+), 뺄셈(-), 곱셉(*), 제곱(^2)의 미분 값만 살펴보겠다.덧셈, 뺄셈$\\cfrac{d}{da}(a+b)=1$, $\\cfrac{d}{da}(a-b)=1$덧셈과 뺄셈은 어떤 값이 더해지던 항상 미분 값은 1이다. 곱셈\\[\\cfrac{d}{da}(a*b)=b\\]a에 대한 미분값으로 b가 나왔다. 즉, 곱해진 값을 미분 값으로 갖는다는 것을 알 수 있다. 제곱\\[\\cfrac{d}{da}a^2=2a\\]제곱은 알다시피 2를 곱한 값을 미분 값으로 갖는다. 순방향 계산 (forward)계산되는 과정을 확인하기 위해 간단한 식을 하나 만들어보자. \\(L=(wx+b-y)^2\\) $wx+b$라는 일차 함수와 $y$의 차이를 제곱한 값을 $L$이라고 두었다. 이제 위 식의 계산 과정을 단계별로 생각해 보자. 먼저 $w$와 $x$를 곱하고, $b$를 더한 후, $y$를 뺀다. 마지막으로 제곱을 한다. {x=4, w=1, b=-3, y=3}일 때, 과정을 그림으로 표현하면 아래와 같다. 왼쪽부터 순서대로 값과 연산자를 거쳐 최종적으로 $L$이 계산된다. 그리고 계산된 각각의 결괏값을 저장해뒀다. 역방향 계산 (backward)\\[\\cfrac{dL}{dw}=\\cfrac{dL}{dz_2}*\\cfrac{dz_2}{dz_1}*\\cfrac{dz_1}{dz_0}*\\cfrac{dz_0}{dw}\\]앞에서 봤던 연쇄 법칙을 이용해 \\(\\cfrac{dL}{dw}\\)를 정의했다.그럼 이제 하나씩 찾아가 보자. \\[\\cfrac{dL}{dz_2}=2z_2\\]$L$은 제곱(^2)으로 계산되었다. 위에서 제곱은 2를 곱한 값을 미분 값으로 갖는 것을 확인했었다. \\[\\cfrac{dz_2}{dz_1}=1\\]$z_2$는 뺄셈(-)으로 계산되었다. 뺄셈은 항상 1을 미분 값으로 갖는다. \\[\\cfrac{dz_1}{dz_0}=1\\]$z_1$은 덧셈(+)으로 계산되었고 덧셈도 항상 미분 값으로 1을 갖는다. \\[\\cfrac{dz_0}{dw}=x\\]$z_0$는 곱셈(*)으로 계산되었다. 따라서 곱해진 값인 $x$를 미분값으로 갖는다. ($z_0 = wx$)\\[\\cfrac{dL}{dw}=\\cfrac{dL}{dz_2}*\\cfrac{dz_2}{dz_1}*\\cfrac{dz_1}{dz_0}*\\cfrac{dz_0}{dw}\\\\=2z_2*1*1*x\\\\=2*(-2)*4\\\\=-16\\]결과적으로 $w$에 대한 $L$의 미분 값은 -16으로 나온다. 순방향으로 계산하는 과정에서 $z_2$의 값을 저장해 두었기 때문에 별다른 연산 없이 바로 결과를 구할 수 있었다. 따라서 순방향으로 한 번, 역방향으로 한 번 계산하고 나면 미분값을 구할 수 있다. 직접 손으로 도함수를 계산해 미분 값을 구해도 동일한 결과가 나오는 것을 확인할 수 있다.역전파 (BackPropagation)우리가 했던 거꾸로 계산했던 과정이 역전파의 아이디어이다. 역전파 알고리즘은 모든 신경망 학습에 사용되는 핵심적인 개념이다.# Pytorchloss.backward()  # 역방향으로 계산자세한 내용이 궁금하면 Pytorch: autograd에 대해 찾아보면 알 수 있다."
  },
  
  {
    "title": "경사하강법과 학습률",
    "url": "/machine-learning/2022/12/29/gradient-descent.html",
    "categories": "Machine-Learning",
    "tags": "Python, AI",
    "date": "2022-12-29 00:00:00 +0900",
    





    
    "snippet": "사전 지식  평균 ( $\\cfrac{1}{n} \\sum_{i=1}^{n}x_i$ )  이차함수의 미분과 접선의 기울기  편미분전체 개념 살펴보기여기서 이해 못 해도 괜찮다. 일단 읽고 넘어가자.딥러닝에서 모델을 학습한다는 것은 실제 값과 예측 값 오차를 최소화하는 가중치를 찾는 과정이다. 여기서 ‘오차’를 정의하는 함수를 비용 함수(Cost funct...",
    "content": "사전 지식  평균 ( $\\cfrac{1}{n} \\sum_{i=1}^{n}x_i$ )  이차함수의 미분과 접선의 기울기  편미분전체 개념 살펴보기여기서 이해 못 해도 괜찮다. 일단 읽고 넘어가자.딥러닝에서 모델을 학습한다는 것은 실제 값과 예측 값 오차를 최소화하는 가중치를 찾는 과정이다. 여기서 ‘오차’를 정의하는 함수를 비용 함수(Cost function)라고 한다. 즉, 비용 함수(오차)가 최솟값을 갖는 방향으로 가중치를 업데이트해야 한다.경사 하강법이라고 불리는 Gradient Descent는 최솟값을 찾을 때 사용할 수 있는 최적화 알고리즘이다. 먼저, 최솟값을 찾을 함수를 설정한 후, 임의의 값으로 초기화한다. 그리고 기울기를 빼면서 최솟값에 가까워질 때까지 반복하는 방법이다.\\[w^{t+1}=w^t-\\mu\\cfrac{dE(w)}{dw}\\]$E(w)$는 오차를 계산하는 비용 함수(목적 함수)이며, $w$는 오차를 구하는 과정에서 사용된 가중치이다. $t$ 시점의 $w$에 대해 편미분한 값을 빼서 $t+1$ 시점의 $w$를 구하는 과정을 식으로 나타낸 것이다. 미분 값 앞에 곱해져 있는 상수는 학습률(learning rate)이다. 학습률이 작으면 왼쪽 그래프처럼 값이 천천히 변하고, 학습률이 크면 오른쪽 그래프처럼 큰 보폭으로 움직인다.   용어 정리: 오차를 구하는 함수를 비용 함수라고 소개했다. 이 비용 함수를 최적화하는 것이 목적으로 설정되었다. 따라서 최적화 단계에서는 비용 함수라는 용어 대신 목적 함수라는 표현을 사용한다. 위에서는 이해를 위해 비용 함수로 작성했지만 엄밀히 말하면 목적 함수가 정확한 표현이다. 자세한 내용은 뒤에서 계산하며 확인할 수 있다.경사하강법을 쉽고 자세하게아래 그림처럼 점 하나(파란점)를 그래프 위에 찍어보자. 최솟값은 그래프에서 가장 작은 값(검정점)이다. 미분 값이란 현재 점에 대한 그래프의 접선의 기울기이다. (파란선의 기울기)  만약 현재 값이 최솟값의 왼쪽에 있다면 기울기(미분 값)가 음수이기 때문에 현재 값 - 미분 값(음수)를 하면 커지게 된다. 즉, 오른쪽으로 점이 이동한다.  만약 현재 값이 최솟값의 오른쪽에 있다면 기울기가 양수이기 때문에 현재 값 - 미분 값(양수)를 하면 현재 값은 작아지게 된다. 즉, 왼쪽으로 이동하게 된다.  그러다가 최솟값에 도달하면 미분값이 0이 되어 현재 값 - 0으로 더 이상 변화하지 않고 멈춘다. 따라서 경사 하강법을 적용하면 최솟값 왼쪽의 값은 점점 오른쪽으로 이동하고, 최솟값 오른쪽의 값은 왼쪽으로 이동하게 된다. 이 과정을 계속 반복하면 결국 최솟값에 도달하게 되는 원리이다.그런데 단순히 미분값만 뺀다고 최솟값에 도달하지는 않는다. 간단하게 그림으로 그려보면 좌우로 움직였지만 오히려 최솟값에서 더 멀어지는 경우다. 이 과정을 반복하면 값이 저 멀리로 날아가 버린다. 따라서 미분 값은 오직 방향만 결정한다. 왼쪽으로 갈지 오른쪽으로 갈지만 결정한 후, 얼마나 움직일지는 학습률이 결정한다. 학습률을 쉽고 자세하게학습률인 learning rate는 학습을 진행할 강도를 결정한다. 여기서 학습한다라는 것은 오차의 최솟값을 찾는다는 의미임을 잊지 않아야 한다. 이 개념을 가지고 아래 예시를 천천히 읽어보자.위 그림처럼 생긴 경사면에서 공을 굴려 검은색 포인트에서 멈추도록 하는 게임을 해보자. 여기서 공을 강하게 밀면 좌우로 요동치다가 어느 순간 점점 목적지에 가까워지며 멈출 것이다. 약하게 밀면 주르륵 흘러서 목적지에 도착할 것이다. 하지만 너무 강하게 밀면 공이 튕겨져 나가서 목적지에 도달하지 못할 것이다. 따라서 공을 어느 정도로 강하게 밀지가 관건이다. 아까 위에서 수식과 함께 봤던 그래프다. 공이 굴러가는 모습을 순간순간 사진으로 찍었다고 생각하면 위와 같이 공이 움직이는 모습을 상상해볼 수 있다. 학습률은 공을 얼마나 세게 밀 것인가와 비슷한 개념이다. 학습률이 너무 크면 값이 튀어 최솟값을 찾지 못한다. 반면 학습률이 너무 작으면 최솟값에 도달하기까지 너무 오래 걸릴 수 있다. 그리고 학습률이 너무 작아서 최솟값을 찾지 못하는 상황도 존재한다. 똑같은 게임이지만 더 복잡한 맵을 가지고 왔다. 우리가 찾고 싶은 최솟값은 오른쪽 끝에 위치한다. 만약 공을 아주 약하게 민다면 첫 번째 골짜기에 빠져서 멈춰버린다. 공을 애매하게 밀면 두번째 골짜기에서 멈춰버릴 수도 있다. 이걸 조금 더 있어 보이게 표현하면 Local optimum에 빠진다고 한다. 첫번째 골짜기에 빠져도 부분적으로는 최솟값을 찾은 게 맞다. 하지만 전체에서 최적의 값인 Global optimum을 찾고 싶다. 전역 최솟값이라는 뜻에서 Global minimum이라고도 한다. Global optimum을 찾는 방법은 적당히 세게 미는 것이다.  ‘적당한’ 학습률은 정해져 있지 않다. 여러 번 밀어 보면서 값을 키우거나 줄여서 적당한 값을 찾아가는 수밖에 없다. 그래서 어떤 논문에서는 학습률이 0.1일 때, 0.01일 때, 0.001일 때… 이런 식으로 기록해 둔 것도 본 적이 있다. 대중적인 모델의 경우, ‘대충 어느 정도의 학습률이 좋더라’와 같은 정보를 논문 또는 구글링으로 찾아볼 수 있다. 잘 모르겠다면 0.01 정도로 한 번 밀어보고 결과에 따라 조정하는 것도 방법이다. 학습률이 큰지/ 작은지는 Training 중 Loss를 보면 알 수 있다. 이것까지 다루기에는 글이 길어지니 넘어가겠다.경사하강법으로 계산해 보기이제 구체적으로 계산 방법에 대해 이야기할 차례다. 편미분에 대해서는 미리 알고 있어야 계산 과정을 이해할 수 있다.$f(x)=w_1x+w_0$이라는 일차 함수가 있다. 일차 함수를 완성하기 위해 우리는 $W {w_1, w_0}$를 업데이트한다.앞에서 정의했듯이 모델을 거쳐 나온 예측 값(출력값)은 $f(x)$이다. 그리고 실제 값은 $y$라고 하겠다. 모델의 오차란 예측 값과 실제 값 사이의 차이를 뜻한다. 기호로 표현하면 $f(x)-y$이다. 우리는 오차의 크기가 궁금하지 부호는 관심 없기 때문에 제곱을 통해 부호를 없애주겠다. $E(x)=(f(x)-y)^2$이제 오차 값을 구하는 $E(x)$가 대충 정의되었다. 그런데 실제 학습에서는 데이터를 하나만 사용할 수 없다. 1차 함수가 (0, -1)를 지난다는 데이터 하나만 가지고는 $2x-1$인지 $1234x-1$인지 특정할 수 없다. 따라서 모든 X, Y에  대해 오차를 구해서 평균을 낸다.$E(x)=\\cfrac{1}{N}\\sum_i^N (f(x_i)-y_i)^2$이제야 비용 함수가 완전히 정의되었다. 참고로 이렇게 빼고 제곱해서 오차를 구하는 방식을 MSE (Mean Square Loss)라고 한다.위 내용이 정리되었다면 $f(x)$에 원래 식을 대입해 보자.$E(x)=\\cfrac{1}{N}\\sum_i^N (w_1x_i+w_0-y_i)^2$식이 복잡해 보여도 과정만 이해했다면 전혀 걱정할 필요 없다. 어차피 계산은 컴퓨터가 할 거다.이제 비용 함수 $E(x)$에 가중치 값인 $W$가 포함되어 있는 것을 볼 수 있다. 우리의 목적은 W값을 업데이트하는 것이기 때문에 $W$를 중심으로 정의한 $E(W)$를 목적 함수로 둔다. 식이 달라진 것은 없다.\\[E(W)=\\cfrac{1}{N}\\sum_i^N (w_1x_i+w_0-y_i)^2\\]이제 경사 하강법으로 계산할 준비가 끝났다. \\[w^{t+1}=w^t-\\mu\\cfrac{dE(W)}{dw}\\]위에서 봤던 경사 하강법 식이다. 우리는 $w_1$과 $w_0$라는 2개의 가중치가 있기 때문에 각각에 대해 편미분을 수행하고 업데이트해주어야 한다. $E(W)$ 자리에 목적 함수를 대입하고 편미분을 수행한다. 어차피 $\\mu$라는 상수가 곱해져 있기 때문에 간단하게 다른 상수 값은 생략하고 계산하겠다. $w_1\\rightarrow w_1-\\mu\\cfrac{dE(W)}{dw_1} \\ =w_1-\\mu\\cfrac{1}{N}\\sum_i^N 2(w_1x_i+w_0-y_i)x_i \\ =w_1-\\mu\\sum_i^N(w_1x_i+w_0-y_i)x_i$$w_0\\rightarrow w_0-\\mu\\cfrac{dE(W)}{dw_0} \\ =w_0-\\mu\\cfrac{1}{N}\\sum_i^N2(w_1x_i+w_0-y_i) \\ =w_0-\\mu\\sum_i^N(w_1x_i+w_0-y_i)$이렇게 $w_1, w_0$값을 업데이트하는 과정을 계속 반복하면 최적의 값을 찾을 수 있다.   이쯤 되면 ‘이거 더 쉽게 풀 수 있을 거 같은데’하는 생각이 들 수 있다. 아래에서 보여줄 예시는 연립 방정식으로 푸는 게 더 쉽고 빠르다. 하지만 실생활에서 발생하는 문제들은 훨씬 복잡하다. 예를 들어, 쓰레기 이미지 1000장을 가지고 ‘종이와 플라스틱을 분류하는 모델을 만들고 학습해라’라고 한다면 머리가 하얘질 거다. 이런 상황에서도 오차를 최소화할 수 있도록 일반화한 것이 경사하강법이라고 보면 된다. Python으로 테스트아래 예제는 Linear Regression이라는 문제를 경사하강법을 활용해 해결하는 과정이다. $X={0, 1, 3}$$Y={-1, 1, 5}$우리는 빨간 직선과 같이 임의의 일차 함수를 만들고 경사 하강법을 이용해 데이터 간 오차를 줄여나갈 것이다. 최종적으로 검정 점선과 같이 데이터에 대해 오차가 작아지도록 만드는 것이 목표이다. $f(x)=w_1x+w_0=1x+0$직선을 만들고 가중치를 $w_1=1$, $w_0=0$으로 랜덤하게 정했다. 이 함수에서 업데이트해야 하는 가중치는 $w_1, w_0$이다.그리고 위에서 열심히 풀었던 과정을 코드로 옮겨보았다. 손으로 계산하면 오래 걸리지만 컴퓨터로 계산하면 0.01초 정도 걸린다. Python을 잘 모른다면 그냥 #으로 적힌 부분을 따라가면서 어떤 과정으로 코드가 실행되었는지만 봐도 충분하다.   전체 코드: github.com/Denev6/gradient.ipynb# 사용할 데이터: X, YX_ = [0, 1, 3]Y_ = [-1, 1, 5]# 데이터의 크기: nn = len(X_)# 가중치 초기화: f(x) = 1x + 0w_ = [0, 1]  # [w0, w1]# 함수 정의: f(x) = w1 * x + w0def f(x):    pred = w_[1] * x + w_[0]    return pred# 비용 함수: E(x) = 1/N * Sum(f(xi) - yi)^2def cost():    error = 1 / n * sum((f(X_[i]) - Y_[i])**2 for i in range(n))    return error# 학습률lr = 0.1# 업데이트를 몇 번 반복할지 결정n_iter = 20for iter in range(1, n_iter+1):    # 경사하강법으로 반복    print(f\"[{iter}번째 업데이트]\")    # 가중치 업데이트    w_[1] = w_[1] - lr * sum((f(X_[i]) - Y_[i]) * X_[i] for i in range(n))    w_[0] = w_[0] - lr * sum([(f(X_[i]) - Y_[i]) for i in range(n)])    # 업데이트된 가중치 보기    print(f\" f(x) = {w_[1]:.2f}x {w_[0]:.2f}\")    # 오차 확인하기     error = cost()    print(f\" 오차: {error:.5f}\\n\")[1번째 업데이트] f(x) = 1.60x -0.14 오차: 0.35560[2번째 업데이트] f(x) = 1.66x -0.26 오차: 0.26300[3번째 업데이트] f(x) = 1.70x -0.36 오차: 0.19452  ...[20번째 업데이트] f(x) = 1.98x -0.95 오차: 0.00115값이 점점 업데이트되면서 오차가 감소하는 것을 볼 수 있다. 20번 정도 반복하고 확인해보니 오차가 아주 작아졌다. 30번 반복하면 오차가 0.00006까지 감소한다. 학습되는 과정을 영상으로 확인하면 아래와 같다. 이걸로 경사 하강법이 잘 작동한다는 것을 확인해 보았다. 물론 실제로는 손으로 하나하나 미분하지 않고 자동으로 계산한다. 역방향으로 미분값 계산 참고.마지막으로 테스트해볼 것은 학습률이다. 학습률이 커지면 값이 튄다고 했는데 어떤 의미인지 직접 확인해 보겠다. 위 예시는 학습률을 0.1로 두고 학습했다. 이번에는 같은 문제를 학습률 0.5로 학습해 보았다. 학습률을 제외하고 다른 요소는 모두 동일하다. [1번째 업데이트] 오차: 9.58333[2번째 업데이트] 오차: 4.06250[3번째 업데이트] 오차: 39.76562    ...  [29번째 업데이트] 오차: 537407805.94885[30번째 업데이트] 오차: 5223270089.75743값을 보면 자기 멋대로 커졌다 작아졌다를 반복한다. 결국 30번 정도 돌렸을 때 오차가 처음보다 더 커졌다. 이럴 거면 차라리 눈 가리고 아무 값이나 찍는 게 더 정확하다. 그렇기 때문에 오차 값을 보고 뭔가 이상하다 싶으면 학습률을 줄여가며 ‘적당한’ 학습률을 찾아야 한다.PytorchPytorch는 미분값을 자동으로 계산한다. torch를 잘 모른다면 #으로된 주석만 봐도 괜찮다.import torch# 학습할 X, y를 tensor로 정의x_train = torch.as_tensor(x_train)y_train = torch.as_tensor(y_train)# 학습률lr = 0.2# 가중치를 랜덤하게 초기화w_1 = torch.rand(1, requires_grad=True)w_0 = torch.rand(1, requires_grad=True)for iter in range(1, 31):    # 예측값    pred = w_1 * x_train + w_0    # 손실 함수    loss = ((pred - y_train)**2).mean()    # 가중치의 미분값 기억하기    w_1.retain_grad()    w_0.retain_grad()    # 역방향으로 미분값 계산    loss.backward()    # 가중치 업데이트    w_1 = w_1 - lr * w_1.grad    w_0 = w_0 - lr * w_0.grad    if iter % 5 == 0:        print(f\"[{iter}번째 업데이트]\")        print(f\" - 오차: {loss.item():.5f}\\n\")    print(f\"w_1: {w_1.item():.3f}, w_0: {w_0.item():.3f}\")"
  },
  
  {
    "title": "선형 모델과 활성화 함수",
    "url": "/machine-learning/2022/12/28/activation.html",
    "categories": "Machine-Learning",
    "tags": "Pytorch, AI",
    "date": "2022-12-28 00:00:00 +0900",
    





    
    "snippet": "선형 구조활성화 함수는 알지만 왜 써야하는지 모를 수 있다. 그럼 아래와 같이 은닉층이 있는 신경망은 어떻게 결과값을 계산하는지 확인해보자.\\[z_0=w_0x_0+w_1x_1\\]\\[z_1=w_2x_0+w_3x_1\\]\\[y=w_5z_0+w_6z_1\\]확인을 위해 식을 직접 대입해 보면,\\[y=w_5(w_0x_0+w_1x_1)+w_6(w_2x_0+w_3x...",
    "content": "선형 구조활성화 함수는 알지만 왜 써야하는지 모를 수 있다. 그럼 아래와 같이 은닉층이 있는 신경망은 어떻게 결과값을 계산하는지 확인해보자.\\[z_0=w_0x_0+w_1x_1\\]\\[z_1=w_2x_0+w_3x_1\\]\\[y=w_5z_0+w_6z_1\\]확인을 위해 식을 직접 대입해 보면,\\[y=w_5(w_0x_0+w_1x_1)+w_6(w_2x_0+w_3x_1)\\]\\[=(w_5w_0+w_6w_2)x_0+(w_5w_1+w_6w_3)x_1\\]\\[=w_{t0}x_0+w_{t1}x_1\\]$w$값들은 그냥 ‘어떤 값’에 불과하다. 따라서, 또 다른 상수로 치환할 수 있다. 결과적으로 레이어를 추가했지만 또 다른 선형 함수가 만들어졌다. 그림으로 나타내면 아래와 같다. 즉, 은닉층을 선형적으로 쌓아도 결국 하나의 선형 함수로 정의된다는 것이다. 따라서 위와 같은 문제를 해결하기 위해 활성화 함수가 필요하다. 선형 모델은 사실상 아래와 같이 활성화 함수(파란점)와 결합된 형태로 사용한다.활성화 함수Sigmoidtorch.nn.functional.sigmoid()\\[f(x)=\\cfrac{1}{1+exp(-x)}\\]sigmoid는 0 ~ 1 사이의 범위를 가진다는 특징이 있다. 이러한 특징 때문에 이진 분류 문제에서 최종적으로 sigmoid를 거쳐 결괏값이 나오도록 만든다. 하지만 sigmoid를 잘 사용하지 않는 이유는 기울기 소실(Vanishing Gradient) 현상 때문이다. (여기서부터는 역전파에 대한 이해가 필요하다.) 모델을 학습하는 과정에서 sigmoid의 미분값을 곱하게 되는데, sigmoid를 미분한 형태는 아래와 같다. \\[\\cfrac{d}{dx}sigmoid(x)=sigmoid(x)(1-sigmoid(x))\\]최댓값이 $x$가 0일 때 0.25로 매우 작은 값을 가지게 된다. 여러 층을 거쳐 학습을 반복하게 되면 결과적으로 미분 값이 아주 작아진다. 이러한 현상을 Gradient가 소실된다고 표현한 것이다. tanhtorch.nn.functional.tanh()\\[f(x)=\\cfrac{exp(x)-exp(-x)}{exp(x)+exp(-x)}\\]tanh(Hyperbolic Tangent) 함수는 sigmoid에 비해 더 급격한 기울기를 가진다. tanh에서 주목할 부분은 미분값이다. \\[\\cfrac{d}{dx}tanh(x)=(1-tanh(x))(1+tanh(x))\\]sigmoid와 달리 0일 때 최댓값으로 1을 갖는다. 하지만 여전히 0에서 멀어지면서 값이 0에 가까워지기 때문에 신경망이 깊다면 기울기 소실 문제가 발생할 수 있다. ReLUtorch.nn.functional.relu()\\[ReLU(x)=max(0, x)\\]ReLU(Rectified Linear Unit)는 0보다 작은 값을 모두 0으로 처리한다는 특징이 있다. ReLU는 일반적으로 기울기 소실 문제의 대안으로 언급된다. ReLU는 sigmoid나 tanh의 미분값에서 보였던 문제를 해결하였다. 또 계산이 단순하고 음수를 모두 0으로 처리하기 때문에 연산 효율이 뛰어나다. 문제는 음수값들이 모두 0으로 사라진다는 것이다. 이러한 문제를 Dying ReLU라고 부른다. ReLU 변형앞에서 봤듯이 ReLU는 장점이 많지만 음수가 0으로 사라지는 문제가 있었다. 이 문제를 해결하기 위한 대안으로 여러 종류의 ReLU가 파생되어 사용되고 있다. 대표적으로 Leaky ReLU와 ELU가 있다. torch.nn.functional.leaky_relu(x, negative_slope=0.01)torch.nn.functional.elu(x)Leaky ReLU는 0 이하의 값에 대해 작은 기울기(0.01)를 가진다. ELU는 0 이하의 값에 대해 exp함수를 적용해 기울기가 변하도록 조정해 주었다. 하지만 이 함수들도 단점은 있다. Leaky ReLU의 경우, 여전히 선형을 띄고 있고 음수값에 큰 변화가 없기 때문에 성능을 보장하지 않는다. ELU의 경우, 비선형의 형태를 띠지만 exp 함수로 인해 연산 효율이 상대적으로 좋지 않다. Leaky ReLU와 ELU 외에도 PReLU와 같이 변형된 함수들이 존재한다."
  },
  
  {
    "title": "월간 DACON 발화자의 감정 인식 AI",
    "url": "/projects/2022/12/17/dacon.html",
    "categories": "Projects",
    "tags": "AI, NLP, Python",
    "date": "2022-12-17 00:00:00 +0900",
    





    
    "snippet": "  대회: 월간 데이콘 발화자의 감정인식 AI 경진대회제출 코드: dacon.io/codeshare인터뷰: 우승자 인터뷰: 219동기자연어처리 대회를 소개받아 DACON 대회에 참가하게 되었다. 자연어처리 과목을 수강하고 있었는데 교수님께서 대회를 소개해주셨다. 당시 멀티모달 우울증 탐지 연구를 하고 있었기 때문에 감정 분석 모델에 대해 공부도 할 겸...",
    "content": "  대회: 월간 데이콘 발화자의 감정인식 AI 경진대회제출 코드: dacon.io/codeshare인터뷰: 우승자 인터뷰: 219동기자연어처리 대회를 소개받아 DACON 대회에 참가하게 되었다. 자연어처리 과목을 수강하고 있었는데 교수님께서 대회를 소개해주셨다. 당시 멀티모달 우울증 탐지 연구를 하고 있었기 때문에 감정 분석 모델에 대해 공부도 할 겸 참가하게 되었다. 그래도 가장 큰 목표는 대회 우승이었다.데이터 전처리발화문 데이터의 대다수가 구어체여서 정규화를 진행했다.  Github: 전처리 코드대부분 데이터는 20 단어 이내 문장이고, 2~5개 단어로 구성된 문장도 포함되어 있었다. 발화 문장은 특수 문자가 많이 포함되어 있는 구어체 문장이다. didn’t 같은 축약형에서 사용하는 apostrophe(‘)도 두 종류가 섞여 있는 등 불균일한 모습이다. Aaaaaaawwwww나 Oh-oh-oh-oh-oh처럼 같은 패턴의 문자가 반복되는 경우도 볼 수 있다.  시도한 전처리는 아래와 같다.   유사한 특수문자 통일 (i.e. “와 “)  소문자로 통일  TweetTokenizer 활용  불용어(stopwords) 제거  반복 표현 제거 (i.e. Oh-oh-oh-oh-oh  →  Oh)  축약 표현 복원 (i.e. didn’t  →  did not)  의미 없는 특수 문자 제거 (i.e. ‘ : Ok’  →  ‘Ok’)  표제어 추출(lemmatization)표제어 추출이나 불용어 제거 같이 정보 손실이 많은 경우 성능이 크게 떨어진다. \"\"\"      원문: I didn't break the cup!!!축약어 복원: I did not break the cup!!!불용어 제거: I break cup !!!\"\"\"&gt;&gt;&gt; from transformers import AutoTokenizer&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")&gt;&gt;&gt; tokenizer.tokenize(\"I didn't break the cup!!!\")['I', 'Ġdidn', \"'t\", 'Ġbreak', 'Ġthe', 'Ġcup', '!!!']&gt;&gt;&gt; tokenizer.tokenize(\"I break cup !!!\")['I', 'Ġbreak', 'Ġcup', 'Ġ', '!!!']축약어 복원 + 불용어 제거가 만나며 문장 의미가 바뀌는 상황도 발생한다. \"\"\"      원문: I did not break the cup!!!표제어 추출: I do not break the cup!!!\"\"\"&gt;&gt;&gt; tokenizer.tokenize(\"I do not break the cup!!!\")['I', 'Ġdo', 'Ġnot', 'Ġbreak', 'Ġthe', 'Ġcup', '!!!']표제어 추출도 마찬가지다. “제가 컵 안 깼어요!!!”와 “저는 컵 안 깹니다!!!”는 다른 의미라고 생각한다. 이러한 전처리를 거쳐 학습한 모델은 좋지 않은 성능을 보였다. 그 외 전처리도 유의미한 차이는 없었다. 다만, TweetTokenizer는 약간의 성능 향상을 보였다. 결론적으로 원본 데이터를 최대한 유지해야 했다. 모델 선택 및 구현사전학습된 파라미터 활용을 위해 Emoberta를 선택하였다.  문제 데이터와 같은 레이블을 가진다.  사전학습된 모델이다.tae898/utils.py에서 레이블을 확인할 수 있었고, 동일하게 학습하도록 LabelEncoder를 생성했다.class LabelEncoder(object):    \"\"\"EmoBERTa에 맞게 직접 생성한 인코더\"\"\"    def __init__(self):        self._targets = [            \"neutral\",            \"joy\",            \"surprise\",            \"anger\",            \"sadness\",            \"disgust\",            \"fear\",        ]        self.target_size = len(self._targets)    def encode(self, labels):        labels = [self._targets.index(lb) for lb in labels]        return labels    def decode(self, labels):        labels = [self._targets[lb] for lb in labels]        return labels비교를 위해 sklearn의 sklearn.preprocessing.LabelEncoder로 랜덤하게 레이블을 지정하고, 직접 만든 Encoder와 비교해 보았다. 당연한 결과지만, EmoBERTa에 맞게 직접 만든 Label-Encoder가 확연히 더 좋은 성능을 보였다. 모델 변형모델의 성능을 높이기 위해 발화문의 문맥을 해석할 수 있는 RNN 기반의 구조를 결합하였다.  모델 전체 Fine-tuning  Classifier 층 (Linear~)만 학습  Classifier 층 대신 GRU 결합 후 학습모델 기본 구조분류 모델은 RoBERTa+Classifier 형태를 가진다. 따라서 RoBERTa는 학습되지 않도록 하고, Classifier 가중치만 학습시켰다. 학습된 모델과 해결하려는 문제가 동일하기 때문에 효과가 있을 수 있다.for name, param in emoberta.named_parameters():    if not str(name).startswith(\"classifier\"):        param.requires_grad = False문맥을 파악하기 위해 Classifier 대신 GRU를 사용했다. 유사한 구조를 사용한 논문: “Sentiment Analysis With Ensemble Hybrid Deep Learning Model”에서 제시한 값과 optimizer를 참고했다. 대신 데이터를 랜덤하게 섞지 않고 발화 순서를 유지하며 입력했다. 결국은 RoBERTa의 전체 구조를 유지하며 Fine-tuning하는 경우 가장 좋은 성능을 보였다. 모델 구조의 차이보다 데이터 양의 문제라고 생각한다. EmoBERTa는 학습된 파라미터를 가지고 있다. 하지만 학습된 값을 덜어내고 적은 데이터로 학습하면 학습량이 차이날 수 밖에 없다.메모리 부족 문제큰 Batch size가 중요했지만 메모리가 부족해 Gradient Accumulation을 적용했다. Batch size를 8, 16, 32…로 테스트 했지만 메모리 에러가 발생했다. 메모리 문제로 인해 Gradient Accumulation을 적용해 batch를 8*8, 16*8, 16*16로 키우며 테스트 했다.model.zero_grad()for epoch in epoch_progress:    model.train()    for batch_id, data in enumerate(train_loader, start=1):                # 학습 과정 생략...        batch_loss = criterion(output.logits, train_label.long())        batch_loss /= grad_step        batch_loss.backward()        if batch_id % grad_step == 0:            # Gradient Accumulation            optimizer.step()            model.zero_grad()Batch size는 32가 가장 좋은 결과를 보였다. Gradient Accumulation을 적용한 경우 16*16과 32*8에서 더 좋은 성능을 보였다. (Colab Pro에서 실행했고, Batch size로 32가 한계였다.)결과모델 Accuracy는 0.76877, F1-macro는 0.66016가 나왔다. test set에 대해서는 F1-macro가 0.56172로 대회 2위를 수상했다.느낀점전공으로 자연어 처리를 수강하며 NLP를 처음 접했고, transformer 모델을 처음 다루며 하루종일 삽질만 하기도 했다. 생각없이 만져보다 결국 기억이 안 나서 처음부터 시작하기도 했다. 다행히  정신차리고 변경한 내용을 기록하며 어떤 요소가 얼마나 영향을 주었는지 비교했다. Base 모델부터 차근차근 기록해야 한다는 사실을 뼈저리게 느꼈다. 너무 Private score(DACON 대회 중 공개되는 점수)에 집착하다보니 큰 그림을 그리지 못했다는 아쉬움도 있다. 그래도 문제들을 해결하기 위해 Label Smootiong이나 Gradient Accumulation 등 새로운 개념도 알게 되었고, 배운 게 많은 프로젝트가 되었다.   다양한 모델 학습 기록: deep-learning-codes/roberta"
  },
  
  {
    "title": "CNN 개념과 MNIST 분류",
    "url": "/machine-learning/2022/12/15/cnn.html",
    "categories": "Machine-Learning",
    "tags": "Python, CV, AI",
    "date": "2022-12-15 00:00:00 +0900",
    





    
    "snippet": "기본적인 CNN 모델을 만들기 위해 필요한 개념들을 정리하였다. 결과: Github: cnn_mnistCNN 모델 구조2D ConvolutionConvolution은 합성곱 연산이다. CNN 모델에서 이미지 특징을 추출하는 과정이 바로 합성곱 연산이다.   Input: 입력은 (h, w) 크기를 가지는 2차원 이미지.  kernel: 이미지의 특징을 ...",
    "content": "기본적인 CNN 모델을 만들기 위해 필요한 개념들을 정리하였다. 결과: Github: cnn_mnistCNN 모델 구조2D ConvolutionConvolution은 합성곱 연산이다. CNN 모델에서 이미지 특징을 추출하는 과정이 바로 합성곱 연산이다.   Input: 입력은 (h, w) 크기를 가지는 2차원 이미지.  kernel: 이미지의 특징을 추출하기 위해 사용되는 필터.  Feature map: Kernel을 거쳐 연산된 결과로, 추출된 이미지의 특징을 가짐.2D Convolution 연산은 아래와 같이 수행된다. (“다음 단계” 클릭)                                                                                                                                                                                                Image (input)                                            1                0                1                1                        Kernel                                                                                                                    Feature (output)                Image * Kernel = Output        0/4 단계    다음 단계Kernel은 계속 순회하며 이미지와 합성곱 연산을 수행한다. 그 결과로 추출된 값이 Feature map이다. 따라서, Feature map은 이미지로부터 추출된 특징이다. 위 예시는 (3 x 3) 크기의 이미지와 (2, 2) 크기의 Kernel을 사용했다. Kernel이 우측으로 한 칸씩 그리고 아래로 한 칸씩 움직였다. 이 경우 stride는 (1, 1)이다. stride란 kernel이 몇 칸씩 움직이며 합성곱 연산을 수행할 것인지를 뜻한다. stride가 (2, 1)이라면 우측으로 2칸, 아래로 1칸씩 움직인다. Conv2dtorch.nn.Conv2d(    in_channels,     out_channels,     kernel_size,     stride=1, )Pytorch에서는 nn.Conv2d라는 이름으로 Convolution 객체를 제공한다. kernel_size와 stride는 위에서 살펴봤던 값이다. 중요한 것은 입력 채널과 출력 채널을 반드시 입력해 주여야 한다. in_channels는 입력 이미지 차원을 의미한다. 일반적으로 흑백 이미지는 1, 색상(RGB) 이미지는 3이 된다. out_channels은 다음 은닉층으로 전달할 출력 크기이다. 위 예시는 흑백 이미지를 입력으로 받으므로 in_channels는 1이며, 출력으로 4개의 Feature map이 만들어지므로 out_channels가 4인 예시다. Conv2d에서는 (out_channels, in_channels) 크기의 Kernel을 만들어 out_channels개의 출력을 만들어낸다. CNN을 만들기 위해 크기가 어떻게 변환되는지 반드시 이해해야 한다.Dilated Convolutiontorch.nn.Conv2d(    in_channels,     out_channels,     kernel_size,     dilation=1)dilation은 Convolution 연산에서 Kernel의 간격을 조정할 때 사용한다. dilation 값을 늘리면, 이미지를 탐색할 때 Kernel 값들의 사이 간격이 커진다. 기본값은 1로 설정되어 여백 없는 Kernel 형태로 탐색된다. dilation이 2인 예시를 보면 (3 x 3) Kernel을 이용해 (5 x 5) Kernel이 커버하는 범위를 탐색하고 있다. Dilational Conv는 이미지를 넓은 범위로 탐색해야 하거나 큰 Kernel을 사용할 여유가 안 될 때 연산 효율을 높여준다. ReLUReLU는 활성화 함수 중 하나로, Conv2d를 거쳐 나온 특징을 조정해준다. torch.nn.ReLU(inplace=False)\\[ReLU(x) = max(0, x)\\]ReLU는 0 이하의 특징 값을 모두 0으로 만든다. 자세한 내용은 활성화 함수에서 다룬다.2D MaxPoolingMax Pooling이란 특정 범위에서 가장 큰 값을 추출해내는 연산이다. 아래에서 “다음 단계” 버튼을 눌러 MaxPooling이 수행되는 과정을 볼 수 있다.                                                                                                                                                                                                 Input                                                                                                                                    MaxPooling                0/4 단계    다음 단계이렇게 지역별 특징 값만 추출하여 모델이 과적합되는 현상을 방지한다. torch.nn.MaxPool2d(    kernel_size,     stride=None,     padding=0,     dilation=1)kernel_size, stride, dilation은 위에서 봤던 값과 동일한 개념이다.padding이란 MaxPooling을 수행하기 전 가장자리에 0 값을 추가하는 과정을 뜻한다. Pytorch의 경우, 0 값을 채우는 zero-padding을 수행한다.분류분류 모델은 최종 분류 층을 거쳐 레이블을 예측하게 된다. 이 layer는 주로 Flatten, Linear, Dropout, Softmax로 구성되어 있다. torch.nn.Flatten(start_dim=1, end_dim=-1)Flatten Layer는 추출된 특징 값을 1차원의 데이터로 변환해준다.Batch size를 고려하지 않았을 때, (32, 7, 7)의 크기를 가진 데이터를 (32 x 7 x 7 =)1568의 1차원 데이터로 변환해주는 식이다. 이렇게 변환된 데이터는 Linear Layer에 들어가 분류 문제를 해결하는 데 사용된다. torch.nn.Linear(in_features, out_features, bias=True)Linear는 완전 연결층으로 Fully Connected Layer로 불린다. 이전 은닉층에서 들어온 입력값이 이후 은닉층과 모두 연결될 수 있도록 하는 역할을 한다. 마지막에 사용되는 Linear layer는 레이블 개수에 맞춰 값의 크기를 변환해주는 역할을 한다.  모델 출력값은 각 레이블일 확률(또는 logit) 값이다. 따라서 레이블이 6개인 분류 문제를 푼다면 출력도 6개여야 한다. 모델 내부에서 어떤 과정을 거쳤든 마지막 값은 레이블 개수가 되도록 조정해야 한다. Linear는 자유롭게 출력 크기를 조정할 수 있다. 따라서 마지막에 Linear를 붙여준다.만약 모델의 출력 값이 6개이고 정답 레이블이 2개라면, Linear(6, 2)와 같은 형태로 사용되어 최종적으로 2개의 값을 반환하도록 조정한다. \\[y = W^Tx + b\\]torch.nn.Dropout(p=0.5)Dropout은 p(확률 값)에 따라 입력 값 일부가 랜덤하게 0으로 출력된다. 나머지 값들은 Scale factor를 곱한 결과로 출력된다. 이를 통해 일반화 성능을 높이는 효과를 가진다. 대신 train 모드에서만 해당 층이 활성화되고, eval 모드에서는 Dropout이 적용되지 않는다. (입력값과 동일한 출력 값을 가진다.)\\[Dropout(x)=0 \\text{ or } \\cfrac{1}{1-p}x\\]torch.nn.Softmax(dim=None)Softmax는 모델의 출력 값(logits)을 확률 값으로 변환해 준다. Linear를 거쳐 나온 모델의 최종 결과는 [-∞, +∞] 범위를 가지는 logit 값이다. 따라서 출력 값을 [0, 1]의 범위로 조정해 확률 값을 얻고 싶을 때 softmax를 사용한다. \\[Softmax(x_i)=\\cfrac{exp(x_i)}{\\sum_{j}^{C}exp(x_j)}\\]logits.argmax(dim=-1)argmax는 가장 큰 값의 인덱스를 반환한다. 최종 출력 값 중 가장 큰 값을 가지는 인덱스를 예측 레이블로 간주한다. MNISTMNIST 데이터셋은 손글씨 숫자 이미지로 이미지 분류 연습에 사용되는 대표적인 데이터다. 아래 코드는 CNN 중심의 코드만 기록하였고, 전체 코드는 Github: cnn_mnist에서 확인할 수 있다. Feature map 등 내부 과정이 자세히 기록되어 있으니 확인하는 걸 추천한다.  전체 코드 보기: Github: cnn_mnistMNIST 이미지 데이터는 (28 x 28) 크기의 2차원 이미지다. 0 ~ 9까지의 숫자 이미지를 가지고 있으므로 Label의 개수는 10개다. class CNN(nn.Module):    def __init__(self, num_label=10):        super(CNN, self).__init__()        self.conv1 = nn.Sequential(            nn.Conv2d(1, 16, kernel_size=3),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)        )        self.conv2 = nn.Sequential(            nn.Conv2d(16, 32, kernel_size=3),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2, stride=2, padding=1)        )        self.classifier = nn.Sequential(            nn.Flatten(1, -1),            nn.Linear(7 * 7 * 32, 32),            nn.ReLU(),            nn.Dropout(p=0.3),            nn.Linear(32, num_label)        )    def forward(self, x):        out = self.conv1(x)        out = self.conv2(out)        out = self.classifier(out)        return out위에서 봤던 Conv2d + ReLU + MaxPool2d, Flatten + Linear + Dropout을 모두 적용한 모습이다. 완성된 모델의 구조는 아래와 같다. 학습이 잘 되는지 확인하기 위해 channel과 같은 값들은 임의로 설정하였다. ----------------------------------------------------------------        Layer (type)               Output Shape         Param #================================================================            Conv2d-1           [-1, 16, 26, 26]             160              ReLU-2           [-1, 16, 26, 26]               0         MaxPool2d-3           [-1, 16, 14, 14]               0            Conv2d-4           [-1, 32, 12, 12]           4,640              ReLU-5           [-1, 32, 12, 12]               0         MaxPool2d-6             [-1, 32, 7, 7]               0            Linear-7                   [-1, 32]          50,208              ReLU-8                   [-1, 32]               0           Dropout-9                   [-1, 32]               0           Linear-10                   [-1, 10]             330================================================================Total params: 55,338Trainable params: 55,338Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.27Params size (MB): 0.21Estimated Total Size (MB): 0.49----------------------------------------------------------------  위 결과는 torchsummary를 통해 출력한 결과다. torchsummary는 제작한 모델의 구조를 요약해준다. Output-Shape에서 -1로 표현된 부분은 Batch 크기다.최종적으로 결과 값을 확인해보면 이미지에 대해 잘 예측하는 것을 볼 수 있다. logits = model(images)probs = F.softmax(logits)pred = probs.argmax(-1)Logits: -7.427, 8.698, 0.407, -4.990, -4.264, -3.783, -4.286, -3.159, -2.024, -4.415 Probs:  0.000, 1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000  Pred:  1  True:  1KerasKeras로 2D CNN을 선언하는 방식은 아래와 같다.import tensorflow as tfmodel = tf.keras.Sequential()model.add(    tf.keras.layers.Conv2D(        filters=16,        kernel_size=(3, 3),        strides=(1, 1),        activation=\"relu\",    ))model.add(    tf.keras.layers.MaxPool2D(        pool_size=(2, 2),          strides=(1, 1),    ))Pytorch와 달리 activation 함수는 Conv2D 객체를 선언하며 함께 지정하게 된다."
  },
  
  {
    "title": "실시간 얼굴 인식 프로젝트를 진행하며",
    "url": "/projects/2022/10/07/face-mouse-control.html",
    "categories": "Projects",
    "tags": "AI, CV, Python",
    "date": "2022-10-07 00:00:00 +0900",
    





    
    "snippet": "  Github: /face-mouse-control논문: 얼굴 인식과 Pyautogui 마우스 제어 기반의 비접촉식 입력 기법$ git clone https://github.com/denev6/face-mouse-control.git$ pip install -r requirements.txt$ python main.py실행할 때 프로젝트 최상단에 있...",
    "content": "  Github: /face-mouse-control논문: 얼굴 인식과 Pyautogui 마우스 제어 기반의 비접촉식 입력 기법$ git clone https://github.com/denev6/face-mouse-control.git$ pip install -r requirements.txt$ python main.py실행할 때 프로젝트 최상단에 있는 main.py를 실행한다. settings-gui.py는 사용자 개인 맞춤 설정을 도와준다.동기외할아버지가 손이 불편하셔서 기계 조작에 어려움이 있으셨다. 나는 “만약 마우스와 키보드 없이 조작할 수 있다면 어떨까?”라는 생각이 들었다. 학교에서 교수-학생 협력(Co-Deep learning) 프로젝트를 진행하고 있었고, 팀을 꾸려 비접촉 입력 기법을 연구하기 시작했다.최종 목표처음부터 상용화 가능한 수준이 목표였다. 일상의 불편함을 해결하는 프로젝트이기 때문에 이론에만 머문다면 의미가 없다. 따라서 정확도는 물론 적은 자원으로 빠른 속도를 내야 한다.구체적으로 Intel-i5 CPU 위에서 작동해야 한다. 외장 GPU가 없는 노트북이 많다고 생각해 CPU만으로도 잘 작동하는 서비스를 구현하기로 했다.구현 목표얼굴 주시방향을 따라 마우스가 이동하며, 눈 깜빡임으로 마우스 좌클릭을 실행한다. 작은 목표로 나누면 아래와 같다.  카메라로 얼굴 랜드마크를 인식해야 한다.  랜드마크로 주시방향을 계산해야 한다.  눈 깜빡임을 포착하고 판단해야 한다.  연산된 정보를 이용해 마우스를 조작해야 한다.얼굴 인식 모델FaceMesh가 가장 준수한 성능을 보였다. FaceMesh는 실시간 얼굴 인식에 특화된 모델로 빠른 처리 속도가 특징이다. HOG, SSD 등 다른 모델과 비교했을 때, 속도와 정확도 모두 뛰어난 모습을 보였다.동일한 문제를 계산하는데 소요된 시간이다.            Model      runtime(s)                  FaceMesh      1.12              HOG      13.62              SSD      3.86      자세한 분석은 논문의 3.2.1. 사용한 모델들에 기록했다.  FaceMesh 정보는 아래 문서에서 확인할 수 있다.Docs: MedaiPipe/ Face landmark detection guide논문: Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs얼굴 방향 계산얼굴 방향 계산은 Perspective-n-Point 문제이다. FaceMesh는 3차원 얼굴 랜드마크 좌표를 반환한다. 따라서 카메라 왜곡이 없다면, 이미지 2차원 좌표와 3차원 좌표로 얼굴 회전벡터를 구할 수 있다.  The cv::solvePnP() returns the rotation and the translation vectors that transform a 3D point expressed in the object coordinate frame to the camera coordinate frame, using different methods.- calib3d solvePnP눈 깜빡임 인식눈의 가로, 세로 비율을 측정해 눈 깜빡임을 인식했다. 논문: Real-Time Eye Blink Detection using Facial Landmarks에 따라 가로 비율 대비 세로 비율이 감소하면 눈 깜빡임으로 처리했다. 하지만 눈을 무의식적으로 깜빡이는 경우 클릭으로 처리하면 안 된다. 따라서 일정 프레임 이상 감고 있어야 클릭으로 이어진다.입력 구현Pyautogui를 이용해 모든 조작을 구현했다. Pyautogui 는 마우스 조작, 키보드 조작이 가능하다. 이를 통해 클릭, 이동, 확대/ 축소 기능을 구현했다.Tkinter로 사이드바에 확대/ 축소 등 버튼을 구현했다. 논문: “상지장애인을 위한 시선 인터페이스에서의 객체 확대 및 음성 명령 인터페이스 개발”에 따르면 웹 화면을 140% 확대했을 때 가장 조작하기 편하다고 한다. 따라서 화면 확대/ 축소 기능을 버튼으로 구현했다.구조 설계절차적 프로그래밍으로 제작된 코드들을 OOP로 리팩토링하였다. 팀원들은 각자 파트를 나눠 코드를 구현했다. 완성한 코드를 종합해보니 절차적으로 구성되어 결과 코드가 길어졌다. 따라서 각 기능을 객체로 분리했다. 데이터는 객체 속에서 조작되며 다른 객체의 영향을 받지 않는다. 덕분에 역할과 책임이 분명하고 수정이 편하다.전체 프로세스는 선형으로 설계했다. Python은 뮤텍스인 GIL 때문에 비동기 처리 및 병렬처리에 불리하다. 따라서 각 객체가 순서대로 연산을 진행하는 선형적인 구조가 만들어졌다. (구현 당시 3.8버전을 사용했다.)사용자 피드백다양한 연령대의 사용자 10명을 모집해 간단한 과제를 해결하게 했다. 예를 들어, 구글 검색(클릭), 웹툰 시청(스크롤 및 확대) 등이 있다. 완료한 참가자는 불편하거나 개선할 점을 설문지에 작성했다.주요 내용은 마우스가 바로 정지하지 않는 문제였다. 사용자가 마우스를 움직이기 위해 좌/우측을 주시한다. 이후 정지하기 위해 순간 정면을 바라본다. 이때 고개를 정면으로 돌리는 순간에도 여전히 좌/우측을 바라보기 때문에 마우스가 계속 이동한다. 즉, 마우스가 바로 정지하지 않고 미끄러진다는 것이다.정지하는 상황 재연해결책으로 변화률 계산을 추가했다. 순간 빠르게 정면을 주시할 경우 즉시 마우스를 정지한다. 다시 말해, 각도의 급격한 감소를 발견하면 즉시 정지 명령을 실행한다.그외 수정 사항은 사이드바 버튼 배열 개선, 처리 속도 측정을 이용한 프레임 드랍 감소 등이 있다.전문가 피드백감사하게도 학교 도움으로 현업 전문가분들에게 피드백 받는 기회를 얻었다. 우리 팀이 프로젝트에 대해 발표하고 전문가분과 질의응답하는 시간을 가졌다. 피드백은 아래 사진으로 첨부했다.  Palete팀의 얼굴 인식 활용 대체 입력 프로젝트가 매우 잘 수행되었습니다. 특히, 기존의 연구나 상품들이 제공하지 못했던 여러 기법들을 머신러닝과 딥러닝 기술을 잘 활용해 저렴한 비용으로 사용 가능하도록 새로운 방식을 잘 제안하였다고 판단됩니다. 시간과 비용의 관점에서 기존의 Pre-train된 머신러닝 모델들을 잘 활용하고, dib과 OpenCV 패키지들 잘 활용하였습니다. 한걸음 더 나아가, 이후에는 직접 여러 face landmark recognition 모델을 활용해 인식률을 높이고, 이후 피드백을 받은 부분을 조금 더 보완해 나간다면, 상품화와 실제 서비스로 내 놓아도 손색이 없을 정도로 훌륭한 서비스가 될 것으로 예상됩니다. 끝으로, 이런 과정과 진행을 모두 github repo에 공개해 이후에도 지속적인 발전이 가능한 오픈소스로 꾸준하고 지속적인 관심을 받을 것으로 기대됩니다. - Microsoft 김대우 이사느낀 점막연하게 머리 속으로 생각해왔던 기능을 구현해 보고 동작하는 모습을 보니 뿌듯했다. 실시간 영상 정보 처리 방법이나 얼굴 객체 검출 등 작업을 수행하며 실력이 발전했다. 무엇보다 팀 단위로 작업한 프로젝트이기 때문에 ‘it works on my computer’ 문제가 발생하지 않기 위해 환경 구축으로 버전을 통일하고, Github으로 버전을 관리하는 등 협업 경험을 쌓을 수 있었다. 생각보다 블로그가 도움이 되었다. Conda나 OpenCV 글도 프로젝트 당시 팀원에게 공유하기 위해 작성한 글이다. 알고 있는 내용을 구구절절 설명하는 것보다 블로그 링크를 공유하는 편이 효율적이고 편리했다. 다음에 실시간 이미지 처리를 하게 되면 이번에 배운 내용이 도움이 될 것 같다."
  },
  
  {
    "title": "Python 비동기/병렬 처리는 효율적일까?",
    "url": "/playground/2022/07/19/asyncio.html",
    "categories": "Playground",
    "tags": "Python",
    "date": "2022-07-19 00:00:00 +0900",
    





    
    "snippet": "Python으로 프로젝트를 진행하던 중, Python을 비동기로 처리하면 빠를까?에 대한 궁금증이 생겼다.  우리는 시간이 걸리지만, 우리 코드가 아닌 곳에서 시간이 걸리는 활동을 찾고 싶다. 데이터베이스를 조회할 때나 외부 서비스를 접근할 때, 사용자 입력을 기다릴 때 같이 우리 프로그램이 다른 작업이 끝나기를 기다려야 하는 상황 말이다. 이런 순간...",
    "content": "Python으로 프로젝트를 진행하던 중, Python을 비동기로 처리하면 빠를까?에 대한 궁금증이 생겼다.  우리는 시간이 걸리지만, 우리 코드가 아닌 곳에서 시간이 걸리는 활동을 찾고 싶다. 데이터베이스를 조회할 때나 외부 서비스를 접근할 때, 사용자 입력을 기다릴 때 같이 우리 프로그램이 다른 작업이 끝나기를 기다려야 하는 상황 말이다. 이런 순간이 바로 CPU가 손가락만 빨면서 기다리는 대신 좀 더 생산적인 일을 할 수 있는 기회다. - 실용주의 프로그래머 (Program Programming Programmer)그렇게 비동기 처리에 대한 조사와 실험이 시작되었다. 그리고 비동기를 찾던 중, 병렬 처리에 대한 내용도 알게 되었다.비동기 처리의 종류Sync/ Async두 항목을 나누는 기준은 요청한 작업이 진행되는 순서이다.   Synchronous(동기): 요청된 작업이 순차적으로 진행된다.   Asynchronous(비동기): 요청된 작업 순서가 보장되지 않는다. 예를 들어, 1, 2, 3번 작업을 순서대로 요청했다고 하자. 동기인 경우는 순서대로 1번, 2번, 3번 작업을 수행하고 결과를 반환한다. 반면 비동기 작업은 1번, 3번, 2번과 같이 다른 순서로 결과를 반환할 수 있다. 요청: 1 → 2 → 3[ 결과 ]- 동기: 1, 2, 3 (항상)- 비동기: 1, 3, 2         2, 1, 3         (다양한 경우 가능)Blocking/ Non-Blocking두 항목을 나누는 기준은 함수의 제어권에 있다.   Blocking: 함수가 호출되어 제어권을 받은 후 다시 넘겨주지 않는다.  Non-Blocking: 함수가 호출되어 제어권을 받은 후 즉시 넘겨준다.쉽게 생각해서 자신이 실행되는 동안 다른 함수가 실행되도록 허락하지 않는 상태가 blocking이다. 반면 non-blocking은 호출된 후 제어권을 다시 main 측으로 넘겨준다. 따라서 main 측에서는 다른 작업을 수행할 수 있게 된다. Asynchronous Non-blocking그렇다면 효율적으로 비동기를 실행하기 위해서는 Asynchronous+Non-blocking으로 실행되어야 한다는 것을 알 수 있다. 크롤링을 수행하는 상황을 가정해 보았다.request가 먼저 실행되었다고 해서 결과를 먼저 반환하지 않는다. 따라서 async(비동기)라고 할 수 있다. 또한 main 측에서 제어권을 받아 요청 1을 실행한 후 다른 요청을 보낼 수 있도록 main 측에 제어권을 반납한다. main 측은 요청 2를 실행하고, 위 과정을 계속 반복한다. 제어권을 즉시 주고받으며 main 측에서 다른 작업을 수행할 수 있도록 하는 것으로 보아 request 과정은 non-blocking이다. 만약 별도의 장치 없이 Python을 이용해 request 작업을 수행하면 Synchronous + Blocking 방식으로 작업하게 된다. 이 방식은 request를 실행하고 결과 값을 받기까지 대기 시간이 발생한다. 하지만 blocking 상태이기 때문에 main 측에서도 별다른 작업을 하지 못하고 무작정 기다려야 한다. 이 과정에서 시간이 낭비되는 것이다. 이러한 원리는 I/O 작업에도 동일하게 적용된다. 따라서 Async+Non-blocking을 이용해 I/O 작업을 수행하면 효율적으로 처리할 수 있다. (AIO)  Sync/ Async 처리와 Blocking/ Non-Blocking에 대한 설명을 찾아보면 “Boost application performance using asynchronous I/O”에 작성된 이미지가 가장 많이 보인다. 4가지 상황에 대한 예시가 그림으로 잘 정리되어 있어 한 번 읽어보는 것을 추천한다.Python의 비동기 처리그래서 Python으로 비동기 처리를 할 수 있는가?가 의문이었다.asyncioPython은 asyncio를 활용해 비동기 처리를 지원한다.  import asyncio  asyncio의 경우, 파이썬 버전에 따라 많은 변화가 있었다. 아래 글에 포함된 코드는 Python 3.9를 활용해 코드를 실행해 보았다. 특히 3.7 이전의 버전을 활용한다면 아래 글의 예제 코드가 실행되지 않을 수 있다.아래 함수를 실행해보면 동시에 비동기로 실행되는 것을 볼 수 있다.import asyncioimport timeasync def f(t):    \"\"\" 실행에 약 t초가 소요되는 함수 \"\"\"    await asyncio.sleep(t)async def main():    task1 = asyncio.create_task(f(6))    task2 = asyncio.create_task(f(7))    await task1    await task2start = time.time()ret = asyncio.run(main())end = time.time()print(f\"시간: {round(end - start)}초\")# 시간: 7초동기로 실행되었다면 6초가 걸리는 함수와 7초가 걸리는 함수가 순서대로 실행되어 약 13초가 소요되었을 것이다. 하지만 위 코드는 약 7초가 소요되었다. 이를 통해 우리가 의도한 대로 실행된다는 것을 알 수 있다.   asyncio.sleep는 time.sleep과 같은 역할을 하는 non-blocking 함수이다. time.sleep은 blocking 함수이기 때문에 time.sleep을 활용해 실행해보면 약 13초가 소요된다. 실제 구현Python의 함수는 기본적으로 Blocking 상태이다. 함수가 실행되는 동안 main 함수는 아무것도 하지 못하고 반환 값을 기다려야만 한다. 하지만 event loop에 run_in_executor를 활용하면 Non-blocking으로 동작할 수 있다. # loop = asyncio.get_event_loop()loop.run_in_executor(None, 함수, 인자1, 인자2 ... )예시:import asyncioimport timeimport requestsurls = [\"https://www. ... \", ... ]  # 10개의 url 주소headers = { \"User-Agent\": \"Mozilla/5.0 ... \"}async def get_reqeust(url):    request = await loop.run_in_executor(None, requests.get, url, headers)    return request.status_codeasync def main():    tasks = [asyncio.create_task(get_reqeust(url)) for url in urls]    ret = await asyncio.gather(*tasks)    return retstart = time.time()loop = asyncio.get_event_loop()status = loop.run_until_complete(main())loop.close()end = time.time()print(f\"시간: {round(end - start)}초, 실행 결과: {status}\")시간: 1초, 실행 결과: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]10개의 URL에 접속해 모두 정상적(200)으로 정보를 가져왔으며 총 1초가 소요되었다. 같은 작업을 동기 방식으로 진행해 보았다. # 생략def get_reqeust(url):    request = requests.get(url, headers)    return request.status_codedef main():    ret = [get_reqeust(url) for url in urls]    return retstart = time.time()status = main()end = time.time()print(f\"시간: {round(end - start)}초, 실행 결과: {status}\")시간: 7초, 실행 결과: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]같은 URL에 접속해 정보를 가져왔지만 시간이 7배 정도 더 오래 걸렸다. 접속해야 하는 URL의 수가 많을수록 차이는 더 심해질 것이다. 위 결과를 정리해보면 아래 표와 같다.                    동기      비동기                  접속한 URL      URL 10개      URL 10개              소요 시간      약 7초      약 1초      Python 비동기 장점Request, I/O bound 프로세스와 같이 딜레이가 발생하는 작업에서 뛰어난 효과를 보인다. 쉽게 말해 서버에 정보를 요청하거나 데이터를 읽는 등 작업에 유리하다는 의미이다. 대기 시간이 발생하는데 상황에서 이러한 시간을 다른 작업을 수행하는데 활용함으로써 전체적인 소요 시간이 감소하는 것이다. Python 비동기 단점하지만 CPU bound 작업은 다르다. 파이썬의 비동기는 병렬적(parallel)으로 처리되는 것이 아니라 동시(concurrent)에 처리되는 것이다. 반복적인 연산을 수행하는 코드를 실행해보면 오히려 비동기로 처리했을 때 더 많은 시간이 소요되었다. 일반 이터레이터(동기 방식)를 사용했을 때 21초가 소요되는 작업을 비동기 이터레이터는 29초가 소요되었다.CPU Bound이 부분부터는 이론적인 내용을 중심으로 작성하였다. 현재 로컬에서 멀티 코어 CPU를 사용해 정확히 측정할 수 있는 환경이 안 된다.multi-thread도 느리다CPU 작업에서 비동기 처리는 동기 처리보다 느린 모습을 보였다. 그렇다면 multithread를 이용해 처리하면 빠를까? 결론부터 말하면 그건 또 아니다.이해를 위해 먼저 GIL을 알아야 한다.GILGIL(Global Interpreter Lock)은 한 프로세스 내에서 하나의 쓰레드만 인터프리터에 접근할 수 있도록 하는 뮤텍스(mutual exclusion)의 일종이다. GIL을 사용하는 이유는 경쟁 상태(race condition)의 위험 때문이다. Python에는 Garbage Collector(GC)가 존재한다. Python 객체가 몇 번 참조되는지 객체 참조(reference count)를 할 때 사용한다. import sysclass Obj:    passa = Obj()  # a 참조: 1회sys.getrefcount(a)  # a 참조: 2회 (임시)&gt;&gt;&gt; 2b = a  # a 참조: 2회sys.getrefcount(a)  # a 참조: 3회 (임시)&gt;&gt;&gt; 3b = 0  # a 참조: 1회sys.getrefcount(a)  # a 참조: 2회 (임시)&gt;&gt;&gt; 2(1) a가 생성된 후, (2) getrefcount에서 임시로 참조되며 총 2번의 참조가 발생한다. (3) b가 a를 참조하고 getrefcount에서 임시로 참조되며 총 3번의 참조가 일어난다. b에서 a에 대한 참조가 사라지면 다시 1로 바뀌게 된다. 이렇게 객체를 몇 번 참조했는지 세는 것을 reference count라고 한다. 이때, 참조가 0이 되면 파이썬에서 더 이상 객체를 참조하지 않기 때문에 GC에 의해 메모리에서 삭제된다. 만약 여러 스레드가 접근할 수 있도록 하면 예상치 못한 충돌이 발생할 가능성이 생긴다. 따라서, 락(Lock)을 걸어 하나의 스레드만 접근할 수 있도록 허용한다.이러한 GIL의 특성 때문에 CPU 연산의 경우, 멀티 스레드를 활용하는 것보다 싱글 스레드를 사용하는 것이 오히려 효율적이다. 병렬적인 처리가 안 되고 context switching만 실행하기 때문에, 오히려 시간이 더 많이 드는 경우가 생긴다. multi-process로 해결GIL은 하나의 프로세스 당 하나의 쓰레드만 접근하도록 한다. 그렇다면 여러 개의 프로세서를 생성하면 어떨까? 이 경우는 성능 향상이 보인다. Python은 multiprocessing을 통해 멀티 프로세싱을 지원한다.정리  Async+Non-blocking 작업을 통해 대기 중인 자원을 효율적으로 사용할 수 있다.  asyncio: request 작업에서 성능 향상을 기대할 수 있다.  multithread: I/O 작업에서는 성능 향상을 기대할 수 있지만, GIL로 인해 병렬 처리에 제한이 있다.  multiprocessing: CPU bound에서 병렬 처리로 성능 향상을 기대할 수 있다.Lei Mao님의 글을 보면 아주 적절한 비유가 있다.            개념      비유                  asyncio      한 주방에서 요리사 한 명이 10개의 요리를 한다.              multithread      한 주방에서 요리사 10명이 10개의 요리를 한다.              multiprocessiong      10개의 주방에서 요리사 10명이 10개의 요리를 한다.      호기심에 시작한 삽질치고는 너무 거창해졌다. 직접 테스트를 할 수 없는 환경이라 확실한 결과는 못 찾았다. 하지만 CPU에 대해 더 자세히 찾아보고 공부할 수 있었다. 역시 깊게 팔수록 어려운 게 Python인 거 같다.+ 2023.10며칠전에 릴리즈된 Python 3.12을 보면, Per-Interpreter GIL을 도입했고 asyncio에서 75%의 속도 향상이 있다고 공개했다. 이로 인한 변화가 있는지도 문뜩 궁금하다."
  },
  
  {
    "title": "데코레이터, 컨텍스트 매니저로 성능 측정",
    "url": "/playground/2022/07/11/time-memory.html",
    "categories": "Playground",
    "tags": "Python",
    "date": "2022-07-11 00:00:00 +0900",
    





    
    "snippet": "알고리즘 또는 패키지 성능을 테스트할 때 시간과 메모리를 측정할 일이 정말 많다. 그런데 특히 메모리와 관련해 정리된 글을 못 찾았다.그래서 시간과 메모리 측정을 위해 사용할 수 있는 방법들을 구상해 정리해보았다. 그리고 Decorator+시간측정, Context Manager+메모리 측정를 사용해 파이썬다운 이쁜 코드를 적어보았다.Decorator생...",
    "content": "알고리즘 또는 패키지 성능을 테스트할 때 시간과 메모리를 측정할 일이 정말 많다. 그런데 특히 메모리와 관련해 정리된 글을 못 찾았다.그래서 시간과 메모리 측정을 위해 사용할 수 있는 방법들을 구상해 정리해보았다. 그리고 Decorator+시간측정, Context Manager+메모리 측정를 사용해 파이썬다운 이쁜 코드를 적어보았다.Decorator생성Decotrator를 사용하면 함수의 시작과 끝에 특정 동작을 실행할 수 있다. Decorator를 생성하기 위해서는 wrapper와 inner 2종류의 함수를 정의해야 한다. 아래는 데코레이터 함수의 공식이다.# Decorator 함수def wrapper(func):    def inner(...):        # 시작 코드        func(...)        # 종료 코드        return     return innerwrapper는 파라미터로 반드시 함수를 받는다. 혼동을 피하기 위해 이 함수를 함수'라고 적겠다.그럼 inner에서 파라미터로 받은 함수'를 실행할 수 있다. 이때 함수' 앞뒤로 동작을 정의할 수 있다.wrapper로 함수' 받음 → 함수'를 inner로 전달 → inner에서 파라미터 입력 받음 → inner 함수 실행보다시피 wrapper는 inner를 실행하기 위해 함수'를 받아오는 역할이 전부다. 왜 이렇게 복잡하게 구성하는가 의문이 들 수 있다. 이렇게 하면 @로 파이썬 마법을 부릴 수 있다.실행Decorator 함수를 정의한 뒤에 @wrapper를 쓰면 일반 함수를 데코레이터가 적용된 함수로 변환해준다. @wrapper 밑에 def로 정의된 함수가 위에서 봤던 함수'이다.@wrapperdef f(x): ...이 방법은 함수를 재사용할 수 있게 해준다. 아래 예시를 보자.예시함수 시작과 끝에 [Start Point], [End Point]를 출력해야 하는 상황이다. 여기서 데코레이터를 활용해보자.원본def hi_to(name):    print(\"[Start Point]\")    print(f\" Hi!!! {name}\")    print(\"[End Point]\")def hello_to(name):    print(\"[Start Point]\")    print(f\" Hello!!! {name}\")    print(\"[End Point]\")# 실행hi_to(\"James\")hello_to(\"James\")데코레이터 적용def print_points(func): # wrapper    def inner(name):        print(\"[Start Point]\")        func(name) # 여기에 쓸 함수를 정의        print(\"[End Point]\")    return inner@print_pointsdef hi_to(name):    print(f\" Hi!!! {name}\")@print_pointsdef hello_to(name):    print(f\" Hello!!! {name}\")# 실행hi_to(\"James\")hello_to(\"James\")[Start Point] Hi!!! James[End Point][Start Point] Hello!!! James[End Point]만약 데코레이터를 사용하지 않는다면 매번 print(...)를 해야 한다. 하지만 데코레이터를 활용하면 한 번만 작성해도 된다. 출력 구문이 바뀐다해도 한 번의 수정으로 모두 적용할 수 있다. 코드를 재사용하기 위해 변수나 함수를 사용하는 것과 같은 맥락이다.또 hi_to와 hello_to 함수 정의를 보면 어떤 동작을 하는지 쉽게 이해할 수 있다. 반복되는 동작을 함수 안에 다 정의하지 않으니 코드의 가독성이 높아진다.시간 측정time 모듈시간을 측정하고 싶다면 process_time과 perf_counter가 있다.import time# 순수 연산 시간start = time.process_time()# 코드...end = time.process_time()print(f\"수행 시간: {end - start}초\")# 전체 소요 시간start = time.perf_counter()# 코드...end = time.perf_counter()print(f\"수행 시간: {end - start}초\")  process_time: CPU에서 sleep, io 등 pending 시간을 제외하고 측정한다. 순수 연산 시간만을 측정한다.  perf_counter: 연산에 사용된 모든 시간을 측정한다.둘은 명확한 차이가 있기 때문에 측정하는 목적에 따라 선택해 사용할 수 있다.데코레이터 적용import time# Decorator 함수 정의def with_timer(func):    def timer(*args, **kwargs):        \"\"\"Returns:            - any: func의 반환값            - float: func의 실행 시간 (초)        \"\"\"        start = time.process_time()        # func 실행        retval = func(*args, **kwargs)        end = time.process_time()        duration = end - start        return retval, duration    return timer# Decorator 적용@with_timerdef test():    zeros = [0 for i in range(10**8)]    return len(zeros)#실행ret, sec = test()print(f\"test -&gt; {ret}: {sec:.3f}s\")test -&gt; 100000000: 3.658s시간을 측정할 함수를 정의할 때 @with_timer를 붙여 사용할 수 있다. 함수에 @타이머와 함께라고 써주니 가독성도 좋다.Context ManagerContext manager는 with 구문을 통해 시작과 끝 동작을 정의할 수 있는 기능이다. 파이썬다운 코드를 작성할 수 있는 유용한 기능이다.대표적으로 open 구문이 있다.with open(\"file.txt\", \"r\") as f:    f.read()정의Context manager는 객체로 정의된다.class Context(object):    def __enter__(self):        # 사전 작업        return self        def __exit__(self, exc_type, exc_value, traceback):        # 사후 작업  __enter__: with문 시작 전에 실행할 동작. 반환값은 as를 통해 받을 수 있다.  __exit__: with문을 닫으며 실행할 동작. 파라미터로 오류와 관련된 정보를 받는다.주의할 점은 __exit__가 True를 반환하면 예외가 발생해도 문제 없이 코드를 진행한다.class Context(object):    def __exit__(self, exc_type, exc_value, traceback):        return True실행with Context() as context:    # 필요한 처리 (들여쓰기)with를 통해 정의한 컨텍스트를 사용하고, as를 통해 __enter__의 반환값을 받아온다. 들여쓰기를 통해 컨텍스트 내에서 작동할 코드를 정의한다. 들여쓰기가 끝나는 지점에서 __exit__가 실행된다.풀어쓰면 아래와 같다.class Context(object): ...ct = Context()context = ct.__enter__()# with문에서 들여쓰기한 코드ct.__exit__()변형Context Manager를 정의하는 다양한 방법이 있다. 굳이 알 필요는 없지만 궁금하면 가볍게 살펴보자.ContextDecorator데코레이터를 객체로 정의하는 방법이다.import contextlibclass Context(contextlib.ContextDecorator):    def __enter__(self): ...    def __exit__(            self,             exc_type,             exc_value,             traceback        ): ...contextDecorator를 상속받은 객체는 Decorator로 활용 가능하다.@Contextdef func(): ...위에서 봤던 데코레이터와 동일하게 작동한다. 다만 데코레이터 함수를 정의하지 않고 객체의 형태로 정의한 거다. 이 방법은 with...as와 달리 컨텍스트 객체 자체를 받아와 직접 제어할 수는 없다.contextmanager이번에는 함수로 정의하고, with로 실행하는 방법이다.import contextlib@contextlib.contextmanagerdef context():    # 사전 작업: __enter__과 같은 역할    yield 반환값    # 사후 작업: __exit__과 같은 역할with context() as 반환값:    # 필요한 작업@contextmanager 함수에서 작업을 한 후 yield 키워드로 대기한다. 그 동안 with 구문 내 서브루틴이 실행되는 형태이다. 정의하는 방식이 데코레이터의 inner 함수와 유사하다. 반면, 사용할 때는 with...as 구문을 사용하는 혼종이다.메모리 측정메모리 확인import osimport psutilpid = os.getpid()process = psutil.Process(pid)memory = process.memory_info().rssprint(f\"사용 중인 메모리: {memory / 1024**2}MiB\")현재 할당된 pid(process id)를 찾아 Process 객체를 만든다. 그리고 memory_info를 통해 메모리 사용량을 가져올 수 있다.시간 측정하듯 (종료 시점 메모리) - (시작 시점 메모리)를 하기에는 GC(Garbage Collector)로 정리된 메모리나 함수 호출이 종료되면서 사라진 값 등을 측정할 수 없다. 따라서 계속 추적해가며 메모리를 확인하기로 했다.추적import sys  def tracer(frame, event, arg):    # 필요한 작업 수행    return tracer   sys.settrace(tracer)# 추적할 코드sys.settrace(None)settrace는 tracer 함수를 계속 추적할 수 있도록 해준다. tracer는 3개의 파라미터를 받아야한다.  frame: 현재의 스택 프레임  event: ‘call’, ‘line’, ‘return’, ‘exception’, ‘opcode’ 중 하나이다.  arg: 문서 참고마지막으로 settrace(None)으로 추적을 종료한다.메모리 추적에 필요한 키워드만 뽑아왔으니 예시를 보자.예시import sysdef my_tracer(frame, event, arg):    \"\"\"tracer 함수\"\"\"    # {발생한 이벤트}, {실행된 함수명} 출력    print(f\"{event}\\t{frame.f_code.co_name}\")    return my_tracerdef test():    \"\"\"테스트를 위한 함수\"\"\"    list_ = [i for i in range(2)]    print(\" --출력:\", list_)    return list_print(\"event\\tfunction\")print(\"-----------------\")# 추적 시작sys.settrace(my_tracer)_ = test()# 추적 종료sys.settrace(None)event   function-----------------call    testline    testcall    &lt;listcomp&gt;line    &lt;listcomp&gt;line    &lt;listcomp&gt;line    &lt;listcomp&gt;return  &lt;listcomp&gt;line    test --출력: [0, 1]return  testtest가 호출(call)되고 test 내부에 있던 list-comprehension이 실행되는 과정을 모두 추적했다.컨텍스트 적용import osimport sysimport psutilclass Tracer(object):    \"\"\"Params:        - max_record (int): 예상되는 동작 수        - *to_trace (...str): 추척할 동작 이름        Attr:        - record (list[int]): 기록된 메모리 사용량    \"\"\"        def __init__(self, max_record, *to_trace):        self._to_trace = to_trace        self.__process = psutil.Process(os.getpid())        self.__max_record = max_record        self.__record = [0 for _ in range(self.__max_record)]        self.__count = 0    def __enter__(self):        \"\"\"with문 추적 시작\"\"\"        sys.settrace(self.trace)        return self    def __exit__(self, *args):        \"\"\"with문 추적 종료\"\"\"        sys.settrace(None)    def trace(self, frame, event, arg):        if self.__count &gt;= self.__max_record:            # 예외 처리            messages = [                \"예상된 동작보다 많은 동작이 실행되었습니다.\",                f\"max_record를 {self.__max_record}보다 크게 설정해주세요.\"                \"추적을 종료합니다.\"            ]            print(\"\\n\".join(messages))            self.__exit__()            return                    if (frame.f_code.co_name in self._to_trace) and (            event in (\"call\", \"line\", \"return\")        ):            # 추적한 메모리 기록            self.__record[self.__count] = self.__process.memory_info().rss            self.__count += 1        return self.trace    @property    def record(self):        \"\"\"추적된 메모리 반환\"\"\"        return self.__record[:self.__count]with Tracer(max_record, *to_trace) as tracer:    # 함수 실행    memory = tracer.record  # 메모리 기록 (list)객체는 복잡해 보이지만 사용법은 간단하다. 예상되는 동작의 수와 추적할 함수 이름만 전달해주면 된다.Tracer라는 컨텍스트를 정의해 시작부터 끝까지 sys.settrace로 추적한다. trace를 보면 실행된 함수의 이벤트가 call · line · return일 때 메모리를 저장한다.  이 코드도 불완전하다. 파이썬 리스트는 동적으로 값을 추가한다. 심지어 growth-factor가 1.125로 작다. 따라서 추적 정보 기록 시 리스트로 인한 메모리 증가가 발생할 가능성이 매우 높다. 이렇게 되면 함수 때문에 메모리가 증가하였는지, 리스트가 할당되며 증가하였는지 알 수 없다. 따라서 처음부터 일정 길이의 리스트를 생성한 뒤, 리스트 내 값을 수정하는 방식으로 제작하였다. 지금으로서는 Tracer를 선언할 때, 예상되는 동작보다 넉넉하게 잡는 것이 중요하다.실행import matplotlib.pyplot as pltdef temp():    \"\"\"테스트를 위해 만든 함수\"\"\"    a = [i for i in range(10 ** 5)]    b = [i for i in range(10 ** 2)]    del a    c = [i for i in range(10 ** 4)]    d = [i for i in range(3)]    final = b + d    return finalwith Tracer(10, \"temp\") as tracer:    # tracer 실행    temp()# 메모리 기록 가져오기record = [x / 1024 ** 2 for x in tracer.record]# 시각화plt.figure()plt.plot(record)plt.ylabel(\"Memory (MiB)\")plt.show()     event   내용----------------------------------------------0~1: call    `temp` 호출1~2: line    `a = [i for i in range(10 ** 5)]`2~3: line    `b = [i for i in range(10 ** 2)]`3~4: line    `del a`4~5: line    `c = [i for i in range(10 ** 4)]`5~6: line    `d = [i for i in range(3)]`6~7: line    `final = b + d`7~8: line    `return final`8~ : return  `final` 반환처음 시작점을 기준으로 어디서 메모리가 많이 사용되었는지, 왜 메모리가 튀었는지 등 정보를 볼 수 있다."
  }
  
]

